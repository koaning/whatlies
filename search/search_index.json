{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"WhatLies \u00b6 A library that tries help you to understand. \"What lies in word embeddings?\" Produced \u00b6 This project was initiated at Rasa as a fun side project that supports the research and developer advocacy teams at Rasa. It is maintained by Vincent D. Warmerdam, Research Advocate at Rasa. What it Does \u00b6 This small library offers tools to make visualisation easier of both word embeddings as well as operations on them. This should be considered an experimental project. This library will allow you to make visualisations of transformations of word embeddings. Some of these transformations are linear algebra operators. Note that these charts are fully interactive. Click. Drag. Zoom in. Zoom out. But we also support other operations. Like pca and umap ; Just like before. Click. Drag. Zoom in. Zoom out. Installation \u00b6 You can install the package via pip; pip install whatlies This will install the base dependencies. Depending on the transformers and language backends that you'll be using you may want to install more. Here's all the possible installation settings you could go for. pip install whatlies[base] pip install whatlies[umap] pip install whatlies[spacy] pip install whatlies[tfhub] pip install whatlies[transformers] pip install whatlies[sense2vec] If you want it all you can also install via; pip install whatlies[all] Note that this will install dependencies but it will not install all the language models you might want to visualise. For example, you might still need to manually download spaCy models if you intend to use that backend. Similar Projects \u00b6 There are some similar projects out and we figured it fair to mention and compare them here. Julia Bazi\u0144ska & Piotr Migdal Web App The original inspiration for this project came from this web app and this pydata talk . It is a web app that takes a while to load but it is really fun to play with. The goal of this project is to make it easier to make similar charts from jupyter using different language backends. Tensorflow Projector From google there's the tensorflow projector project . It offers highly interactive 3d visualisations as well as some transformations via tensorboard. The tensorflow projector will create projections in tensorboard, which you can also load into jupyter notebook but whatlies makes visualisations directly. The tensorflow projector supports interactive 3d visuals, which whatlies currently doesn't. Whatlies offers lego bricks that you can chain together to get a visualisation started. This also means that you're more flexible when it comes to transforming data before visualising it. Parallax From Uber AI Labs there's parallax which is described in a paper here . There's a common mindset in the two tools; the goal is to use arbitrary user defined projections to understand embedding spaces better. That said, some differences that are worth to mention. It relies on bokeh as a visualisation backend and offers a lot of visualisation types (like radar plots). Whatlies uses altair and tries to stick to simple scatter charts. Altair can export interactive html/svg but it will not scale as well if you've drawing many points at the same time. Parallax is meant to be run as a stand-alone app from the command line while Whatlies is meant to be run from the jupyter notebook. Parallax gives a full user interface while Whatlies offers lego bricks that you can chain together to get a visualisation started. Whatlies relies on language backends (like spaCy, huggingface) to fetch word embeddings. Parallax allows you to instead fetch raw files on disk. Parallax has been around for a while, Whatlies is more new and therefore more experimental. Citation \u00b6 Please use the following citation when you found whatlies helpful for any of your work (find the whatlies paper here ): @inproceedings{warmerdam-etal-2020-going, title = \"Going Beyond {T}-{SNE}: Exposing whatlies in Text Embeddings\", author = \"Warmerdam, Vincent and Kober, Thomas and Tatman, Rachael\", booktitle = \"Proceedings of Second Workshop for NLP Open Source Software (NLP-OSS)\", month = nov, year = \"2020\", address = \"Online\", publisher = \"Association for Computational Linguistics\", url = \"https://www.aclweb.org/anthology/2020.nlposs-1.8\", doi = \"10.18653/v1/2020.nlposs-1.8\", pages = \"52--60\", abstract = \"We introduce whatlies, an open source toolkit for visually inspecting word and sentence embeddings. The project offers a unified and extensible API with current support for a range of popular embedding backends including spaCy, tfhub, huggingface transformers, gensim, fastText and BytePair embeddings. The package combines a domain specific language for vector arithmetic with visualisation tools that make exploring word embeddings more intuitive and concise. It offers support for many popular dimensionality reduction techniques as well as many interactive visualisations that can either be statically exported or shared via Jupyter notebooks. The project documentation is available from https://rasahq.github.io/whatlies/.\", } Local Development \u00b6 If you want to develop locally you can start by running this command after cloning. make develop","title":"Home"},{"location":"#whatlies","text":"A library that tries help you to understand. \"What lies in word embeddings?\"","title":"WhatLies"},{"location":"#produced","text":"This project was initiated at Rasa as a fun side project that supports the research and developer advocacy teams at Rasa. It is maintained by Vincent D. Warmerdam, Research Advocate at Rasa.","title":"Produced"},{"location":"#what-it-does","text":"This small library offers tools to make visualisation easier of both word embeddings as well as operations on them. This should be considered an experimental project. This library will allow you to make visualisations of transformations of word embeddings. Some of these transformations are linear algebra operators. Note that these charts are fully interactive. Click. Drag. Zoom in. Zoom out. But we also support other operations. Like pca and umap ; Just like before. Click. Drag. Zoom in. Zoom out.","title":"What it Does"},{"location":"#installation","text":"You can install the package via pip; pip install whatlies This will install the base dependencies. Depending on the transformers and language backends that you'll be using you may want to install more. Here's all the possible installation settings you could go for. pip install whatlies[base] pip install whatlies[umap] pip install whatlies[spacy] pip install whatlies[tfhub] pip install whatlies[transformers] pip install whatlies[sense2vec] If you want it all you can also install via; pip install whatlies[all] Note that this will install dependencies but it will not install all the language models you might want to visualise. For example, you might still need to manually download spaCy models if you intend to use that backend.","title":"Installation"},{"location":"#similar-projects","text":"There are some similar projects out and we figured it fair to mention and compare them here. Julia Bazi\u0144ska & Piotr Migdal Web App The original inspiration for this project came from this web app and this pydata talk . It is a web app that takes a while to load but it is really fun to play with. The goal of this project is to make it easier to make similar charts from jupyter using different language backends. Tensorflow Projector From google there's the tensorflow projector project . It offers highly interactive 3d visualisations as well as some transformations via tensorboard. The tensorflow projector will create projections in tensorboard, which you can also load into jupyter notebook but whatlies makes visualisations directly. The tensorflow projector supports interactive 3d visuals, which whatlies currently doesn't. Whatlies offers lego bricks that you can chain together to get a visualisation started. This also means that you're more flexible when it comes to transforming data before visualising it. Parallax From Uber AI Labs there's parallax which is described in a paper here . There's a common mindset in the two tools; the goal is to use arbitrary user defined projections to understand embedding spaces better. That said, some differences that are worth to mention. It relies on bokeh as a visualisation backend and offers a lot of visualisation types (like radar plots). Whatlies uses altair and tries to stick to simple scatter charts. Altair can export interactive html/svg but it will not scale as well if you've drawing many points at the same time. Parallax is meant to be run as a stand-alone app from the command line while Whatlies is meant to be run from the jupyter notebook. Parallax gives a full user interface while Whatlies offers lego bricks that you can chain together to get a visualisation started. Whatlies relies on language backends (like spaCy, huggingface) to fetch word embeddings. Parallax allows you to instead fetch raw files on disk. Parallax has been around for a while, Whatlies is more new and therefore more experimental.","title":"Similar Projects"},{"location":"#citation","text":"Please use the following citation when you found whatlies helpful for any of your work (find the whatlies paper here ): @inproceedings{warmerdam-etal-2020-going, title = \"Going Beyond {T}-{SNE}: Exposing whatlies in Text Embeddings\", author = \"Warmerdam, Vincent and Kober, Thomas and Tatman, Rachael\", booktitle = \"Proceedings of Second Workshop for NLP Open Source Software (NLP-OSS)\", month = nov, year = \"2020\", address = \"Online\", publisher = \"Association for Computational Linguistics\", url = \"https://www.aclweb.org/anthology/2020.nlposs-1.8\", doi = \"10.18653/v1/2020.nlposs-1.8\", pages = \"52--60\", abstract = \"We introduce whatlies, an open source toolkit for visually inspecting word and sentence embeddings. The project offers a unified and extensible API with current support for a range of popular embedding backends including spaCy, tfhub, huggingface transformers, gensim, fastText and BytePair embeddings. The package combines a domain specific language for vector arithmetic with visualisation tools that make exploring word embeddings more intuitive and concise. It offers support for many popular dimensionality reduction techniques as well as many interactive visualisations that can either be statically exported or shared via Jupyter notebooks. The project documentation is available from https://rasahq.github.io/whatlies/.\", }","title":"Citation"},{"location":"#local-development","text":"If you want to develop locally you can start by running this command after cloning. make develop","title":"Local Development"},{"location":"faq/","text":"F.A.Q. \u00b6 Plotting \u00b6 How do I save an interactive chart? \u00b6 The interactive charts that our library produces are made with altair . These charts use javascript for the interactivity and they are based on vega . You can represent the entire chart (including the data) as a json object. This means that you can always save a visluatisation as an html page or as a json file. from whatlies.language import SpacyLanguage words = [\"prince\", \"princess\", \"nurse\", \"doctor\", \"banker\", \"man\", \"woman\", \"cousin\", \"neice\", \"king\", \"queen\", \"dude\", \"guy\", \"gal\", \"fire\", \"dog\", \"cat\", \"mouse\", \"red\", \"blue\", \"green\", \"yellow\", \"water\", \"person\", \"family\", \"brother\", \"sister\"] lang = SpacyLanguage(\"en_core_web_md\") emb = lang[words] p = emb.plot_interactive('man', 'woman') p.to_html(\"plot.html\") p.to_json(\"plot.json\") A tutorial on how this works exactly can be found here . How do I save an interactive chart for publication? \u00b6 You can also choose to save an interactive chart as an svg/png/pdf if you're interested in using an altair visualisation in a publication. More details are listed on their documentation page in short you'll need to install the altair_saver package for this functionality. To get this code to work you may need to install some node dependencies though. To install them locally in your project run; npm install vega-lite vega-cli canvas Once these are all installed, the following code snippet will work; from whatlies.language import SpacyLanguage from altair_saver import save words = [\"prince\", \"princess\", \"nurse\", \"doctor\", \"banker\", \"man\", \"woman\", \"cousin\", \"neice\", \"king\", \"queen\", \"dude\", \"guy\", \"gal\", \"fire\", \"dog\", \"cat\", \"mouse\", \"red\", \"blue\", \"green\", \"yellow\", \"water\", \"person\", \"family\", \"brother\", \"sister\"] lang = SpacyLanguage(\"en_core_web_md\") emb = lang[words] p = emb.plot_interactive('man', 'woman') save(p, \"chart.png\") This saves the following chart on disk; How do I change the title/size of the interactive chart? \u00b6 The interactive charts are Altair charts and that means that you could do something like this: from whatlies.language import SpacyLanguage words = [\"prince\", \"princess\", \"nurse\", \"doctor\", \"banker\", \"man\", \"woman\", \"cousin\", \"neice\", \"king\", \"queen\", \"dude\", \"guy\", \"gal\", \"fire\", \"dog\", \"cat\", \"mouse\", \"red\", \"blue\", \"green\", \"yellow\", \"water\", \"person\", \"family\", \"brother\", \"sister\"] lang = SpacyLanguage(\"en_core_web_md\") emb = lang[words] p = emb.plot_interactive('man', 'woman') p.properties(title=\"spaCy\", height=200, width=200) One common feature is that you might set the width to the container size in order to achieve 100% width. p.properties(title=\"spaCy\", height=200, width=\"container\") Languages \u00b6 How do I access nearest tokens from a language model? \u00b6 This depends on the language model. If a language model isn't based on a vocabulary then we won't be able to retreive embeddings for it. Please check the docs if a language model has a score_similar method attached. For those that do, you can do: from whatlies.language import BytePairLanguage lang = BytePairLanguage(\"en\") lang.score_similar(\"king\") This code snippet will return: [(Emb[king], 1.1102230246251565e-16), (Emb[\u2581king], 0.23501371664985227), (Emb[iv], 0.33016763827104456), (Emb[\u2581throne], 0.3366865106345296), (Emb[iii], 0.33745878416967634), (Emb[lord], 0.37137511153954517), (Emb[\u2581prince], 0.3806569732193965), (Emb[\u2581duke], 0.3889479082730939), (Emb[son], 0.3892961048683081), (Emb[ivals], 0.3904733871620414)] In this case you'll see subword embeddings being return because that is what this language model uses internally. Language models using spaCy would use full tokens. How do I access nearest tokens from a language model using an embedding? \u00b6 You can pass this method a string, but also an embedding object. Ths can contain a custom vector but you can also construct an embedding via operations. This makes the API a lot more flexible. For example, we can construct this embedding; from whatlies.language import BytePairLanguage lang = BytePairLanguage(\"en\") kmw = lang[\"king\"] - lang[\"man\"] + lang[\"woman\"] # Emb[((king - man) + woman)] And use this embedding in our language model to retreive similar items. lang.score_similar(kmw, n=7) This yields. [(Emb[king], 0.2620711370759745), (Emb[mother], 0.36575381150291), (Emb[father], 0.39737356910585997), (Emb[\u2581queen], 0.43554929266740294), (Emb[anne], 0.4583618203004909), (Emb[\u2581throne], 0.47000919280368636), (Emb[mary], 0.4771824121946612)] Note that in general we're using cosine distance here but you can also pass the .score_similar method a metric so select other metrics that are compatible with scikit-learn. How do I retreive an embedding set from language model using similar tokens? \u00b6 You can use the same flow we used in the previous two questions to generate an embedding set that can be used for plotting. from whatlies.language import BytePairLanguage lang = BytePairLanguage(\"en\") kmw = lang[\"king\"] - lang[\"man\"] + lang[\"woman\"] emb_king = lang.embset_similar(kmw, n=20) Compatibility \u00b6 This project depends on a lot of backends so there's a risk of breaking changes whenever we upgrade to a new version of a backend. The goal of this table is to list keep track of compatible versions. whatlies spaCy tensorflow 0.4.5 2.2.4 2.3.0 0.5.0 2.3.2 2.3.0 0.6.0 3.0.0 2.3.0","title":"FAQ"},{"location":"faq/#faq","text":"","title":"F.A.Q."},{"location":"faq/#plotting","text":"","title":"Plotting"},{"location":"faq/#how-do-i-save-an-interactive-chart","text":"The interactive charts that our library produces are made with altair . These charts use javascript for the interactivity and they are based on vega . You can represent the entire chart (including the data) as a json object. This means that you can always save a visluatisation as an html page or as a json file. from whatlies.language import SpacyLanguage words = [\"prince\", \"princess\", \"nurse\", \"doctor\", \"banker\", \"man\", \"woman\", \"cousin\", \"neice\", \"king\", \"queen\", \"dude\", \"guy\", \"gal\", \"fire\", \"dog\", \"cat\", \"mouse\", \"red\", \"blue\", \"green\", \"yellow\", \"water\", \"person\", \"family\", \"brother\", \"sister\"] lang = SpacyLanguage(\"en_core_web_md\") emb = lang[words] p = emb.plot_interactive('man', 'woman') p.to_html(\"plot.html\") p.to_json(\"plot.json\") A tutorial on how this works exactly can be found here .","title":"How do I save an interactive chart?"},{"location":"faq/#how-do-i-save-an-interactive-chart-for-publication","text":"You can also choose to save an interactive chart as an svg/png/pdf if you're interested in using an altair visualisation in a publication. More details are listed on their documentation page in short you'll need to install the altair_saver package for this functionality. To get this code to work you may need to install some node dependencies though. To install them locally in your project run; npm install vega-lite vega-cli canvas Once these are all installed, the following code snippet will work; from whatlies.language import SpacyLanguage from altair_saver import save words = [\"prince\", \"princess\", \"nurse\", \"doctor\", \"banker\", \"man\", \"woman\", \"cousin\", \"neice\", \"king\", \"queen\", \"dude\", \"guy\", \"gal\", \"fire\", \"dog\", \"cat\", \"mouse\", \"red\", \"blue\", \"green\", \"yellow\", \"water\", \"person\", \"family\", \"brother\", \"sister\"] lang = SpacyLanguage(\"en_core_web_md\") emb = lang[words] p = emb.plot_interactive('man', 'woman') save(p, \"chart.png\") This saves the following chart on disk;","title":"How do I save an interactive chart for publication?"},{"location":"faq/#how-do-i-change-the-titlesize-of-the-interactive-chart","text":"The interactive charts are Altair charts and that means that you could do something like this: from whatlies.language import SpacyLanguage words = [\"prince\", \"princess\", \"nurse\", \"doctor\", \"banker\", \"man\", \"woman\", \"cousin\", \"neice\", \"king\", \"queen\", \"dude\", \"guy\", \"gal\", \"fire\", \"dog\", \"cat\", \"mouse\", \"red\", \"blue\", \"green\", \"yellow\", \"water\", \"person\", \"family\", \"brother\", \"sister\"] lang = SpacyLanguage(\"en_core_web_md\") emb = lang[words] p = emb.plot_interactive('man', 'woman') p.properties(title=\"spaCy\", height=200, width=200) One common feature is that you might set the width to the container size in order to achieve 100% width. p.properties(title=\"spaCy\", height=200, width=\"container\")","title":"How do I change the title/size of the interactive chart?"},{"location":"faq/#languages","text":"","title":"Languages"},{"location":"faq/#how-do-i-access-nearest-tokens-from-a-language-model","text":"This depends on the language model. If a language model isn't based on a vocabulary then we won't be able to retreive embeddings for it. Please check the docs if a language model has a score_similar method attached. For those that do, you can do: from whatlies.language import BytePairLanguage lang = BytePairLanguage(\"en\") lang.score_similar(\"king\") This code snippet will return: [(Emb[king], 1.1102230246251565e-16), (Emb[\u2581king], 0.23501371664985227), (Emb[iv], 0.33016763827104456), (Emb[\u2581throne], 0.3366865106345296), (Emb[iii], 0.33745878416967634), (Emb[lord], 0.37137511153954517), (Emb[\u2581prince], 0.3806569732193965), (Emb[\u2581duke], 0.3889479082730939), (Emb[son], 0.3892961048683081), (Emb[ivals], 0.3904733871620414)] In this case you'll see subword embeddings being return because that is what this language model uses internally. Language models using spaCy would use full tokens.","title":"How do I access nearest tokens from a language model?"},{"location":"faq/#how-do-i-access-nearest-tokens-from-a-language-model-using-an-embedding","text":"You can pass this method a string, but also an embedding object. Ths can contain a custom vector but you can also construct an embedding via operations. This makes the API a lot more flexible. For example, we can construct this embedding; from whatlies.language import BytePairLanguage lang = BytePairLanguage(\"en\") kmw = lang[\"king\"] - lang[\"man\"] + lang[\"woman\"] # Emb[((king - man) + woman)] And use this embedding in our language model to retreive similar items. lang.score_similar(kmw, n=7) This yields. [(Emb[king], 0.2620711370759745), (Emb[mother], 0.36575381150291), (Emb[father], 0.39737356910585997), (Emb[\u2581queen], 0.43554929266740294), (Emb[anne], 0.4583618203004909), (Emb[\u2581throne], 0.47000919280368636), (Emb[mary], 0.4771824121946612)] Note that in general we're using cosine distance here but you can also pass the .score_similar method a metric so select other metrics that are compatible with scikit-learn.","title":"How do I access nearest tokens from a language model using an embedding?"},{"location":"faq/#how-do-i-retreive-an-embedding-set-from-language-model-using-similar-tokens","text":"You can use the same flow we used in the previous two questions to generate an embedding set that can be used for plotting. from whatlies.language import BytePairLanguage lang = BytePairLanguage(\"en\") kmw = lang[\"king\"] - lang[\"man\"] + lang[\"woman\"] emb_king = lang.embset_similar(kmw, n=20)","title":"How do I retreive an embedding set from language model using similar tokens?"},{"location":"faq/#compatibility","text":"This project depends on a lot of backends so there's a risk of breaking changes whenever we upgrade to a new version of a backend. The goal of this table is to list keep track of compatible versions. whatlies spaCy tensorflow 0.4.5 2.2.4 2.3.0 0.5.0 2.3.2 2.3.0 0.6.0 3.0.0 2.3.0","title":"Compatibility"},{"location":"releases/","text":"v0.6.3 Removed the deprecated plot_correlation Added the plot_brush tool. v0.6.2 Made each sklearn compatible language also apply .partial_fit v0.6.1 Added support for DIET embeddings Added support for spaCy 3.0 Dropped support for BERT queries v0.6.0 Removed all transformers besides the ones in sklearn and umap v0.5.10 Added a helper to reverse strings for Arabic matplotlib charts. Put back ConveRT. It seems we only need a new hosting link. Added docs example for an Arabic benchmark. Added direct support for LaBSE. v0.5.4 Deprecated the ConveRTLanguage backend. The original authors removed the embeddings. Added the support for the Universal Sentence Encoder. v0.5.3 Fixed the ConveRTLanguage backend. The original source changed their download url. v0.5.2 Added tests for matplotlib and altair . Added plot_3d , allowing you to make some 3d visualisations. Added assign as a nicer alternative for add_property . Added a citation to an research paper on this library. Removed the \"helper vectors\" from our transformers. v0.5.1 Added a guide on debiasing on the docs. You can now specify the plot axes and title. We've added sensible default values to \"plot_interactive\". We now assume that you want to plot the first two elements of a vector. # Before emb.transform(Pca(2)).plot_interactive('pca_0', 'pca_1') # After emb.transform(Pca(2)).plot_interactive() v0.5.0 Added some robustness to the matplotlib based arrow and scatter charts. Started deprecating the plot_correlation method in favor of the new plot_distance and plot_similarity methods. v0.4.7 Fixed bugs relating to conditional imports. Added a new pipe method. v0.4.6 Fixed bugs to become spaCy 2.3 compatible. Added scikit-learn pipeline compatibility for all models. v0.4.5 Created support for tfhub and huggingface backends. Added the ivis transformer. v0.4.4 Added support for ConveRT Embeddings. v0.4.3 Added support for similarity retreival for CountVectorLang Added more methods for Embedding objects: distance , norm v0.4.2 Added support the gensim language backend. v0.4.1 Added support for TSNE v0.4.0 Many small updates to improve documentation Fixed many small bugs for the BytePairLanguage","title":"Releases"},{"location":"roadmap/","text":"There's a few things that would be nice to have. Feel free to start a discussion on these topics in github. Curated Word Lists If people want to investigate bias in word embeddings then it would help if they had word-lists at the ready. But you can also imagine that it would be nice to have a word list of words that have multiple meanings. It'd be especially nice if we can support these word lists in many languages. Multiple Plot Metrics At the moment we only project onto axes to get x/y coordinates. It might make sense to show the cosine distance to these axes instead. And if we're allowing cosine distance ... we might allow for flexible distance metrics in general. Table Summaries We've got a focus on charts now, but one imagines that calculating tables with summary statistics is also relevant. Languages It might be nice if we could help the user automatically download embedding files for supported backends. Testing it would be nice to have a good way of testing the charts it would be nice to be able to test multiple models without having to download gigabytes into github actions","title":"Roadmap"},{"location":"api/embedding/","text":"whatlies.embedding.Embedding \u00b6 This object represents a word embedding. It contains a vector and a name. Parameters Name Type Description Default name the name of this embedding, includes operations required vector the numerical representation of the embedding required orig original name of embedding, is left alone None Usage: from whatlies.embedding import Embedding foo = Embedding ( \"foo\" , [ 0.1 , 0.3 ]) bar = Embedding ( \"bar\" , [ 0.7 , 0.2 ]) foo | bar foo - bar + bar ndim : (property, readonly) \u00b6 Return the dimension of embedding vector. norm : (property, readonly) \u00b6 Gives the norm of the vector of the embedding __add__ ( self , other ) \u00b6 Show source code in whatlies/embedding.py 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 def __add__ ( self , other ) -> \"Embedding\" : \"\"\" Add two embeddings together. Usage: ```python from whatlies.embedding import Embedding foo = Embedding(\"foo\", [0.1, 0.3]) bar = Embedding(\"bar\", [0.7, 0.2]) foo + bar ``` \"\"\" copied = deepcopy ( self ) copied . name = f \"( { self . name } + { other . name } )\" copied . vector = self . vector + other . vector return copied Add two embeddings together. Usage: from whatlies.embedding import Embedding foo = Embedding ( \"foo\" , [ 0.1 , 0.3 ]) bar = Embedding ( \"bar\" , [ 0.7 , 0.2 ]) foo + bar __gt__ ( self , other ) \u00b6 Show source code in whatlies/embedding.py 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 def __gt__ ( self , other ): \"\"\" Measures the size of one embedding to another one. Usage: ```python from whatlies.embedding import Embedding foo = Embedding(\"foo\", [0.1, 0.3]) bar = Embedding(\"bar\", [0.7, 0.2]) foo > bar ``` \"\"\" return ( self . vector . dot ( other . vector )) / ( other . vector . dot ( other . vector )) Measures the size of one embedding to another one. Usage: from whatlies.embedding import Embedding foo = Embedding ( \"foo\" , [ 0.1 , 0.3 ]) bar = Embedding ( \"bar\" , [ 0.7 , 0.2 ]) foo > bar __neg__ ( self ) \u00b6 Show source code in whatlies/embedding.py 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 def __neg__ ( self ): \"\"\" Negate an embedding. Usage: ```python from whatlies.embedding import Embedding foo = Embedding(\"foo\", [0.1, 0.3]) assert (- foo).vector == - foo.vector ``` \"\"\" copied = deepcopy ( self ) copied . name = f \"(- { self . name } )\" copied . vector = - self . vector return copied Negate an embedding. Usage: from whatlies.embedding import Embedding foo = Embedding ( \"foo\" , [ 0.1 , 0.3 ]) assert ( - foo ) . vector == - foo . vector __or__ ( self , other ) \u00b6 Show source code in whatlies/embedding.py 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 def __or__ ( self , other ): \"\"\" Makes one embedding orthogonal to the other one. Usage: ```python from whatlies.embedding import Embedding foo = Embedding(\"foo\", [0.1, 0.3]) bar = Embedding(\"bar\", [0.7, 0.2]) foo | bar ``` \"\"\" copied = deepcopy ( self ) copied . name = f \"( { self . name } | { other . name } )\" copied . vector = self . vector - ( self >> other ) . vector return copied Makes one embedding orthogonal to the other one. Usage: from whatlies.embedding import Embedding foo = Embedding ( \"foo\" , [ 0.1 , 0.3 ]) bar = Embedding ( \"bar\" , [ 0.7 , 0.2 ]) foo | bar __rshift__ ( self , other ) \u00b6 Show source code in whatlies/embedding.py 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 def __rshift__ ( self , other ): \"\"\" Maps an embedding unto another one. Usage: ```python from whatlies.embedding import Embedding foo = Embedding(\"foo\", [0.1, 0.3]) bar = Embedding(\"bar\", [0.7, 0.2]) foo >> bar ``` \"\"\" copied = deepcopy ( self ) new_vec = ( ( self . vector . dot ( other . vector )) / ( other . vector . dot ( other . vector )) * other . vector ) copied . name = f \"( { self . name } >> { other . name } )\" copied . vector = new_vec return copied Maps an embedding unto another one. Usage: from whatlies.embedding import Embedding foo = Embedding ( \"foo\" , [ 0.1 , 0.3 ]) bar = Embedding ( \"bar\" , [ 0.7 , 0.2 ]) foo >> bar __sub__ ( self , other ) \u00b6 Show source code in whatlies/embedding.py 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 def __sub__ ( self , other ): \"\"\" Subtract two embeddings. Usage: ```python from whatlies.embedding import Embedding foo = Embedding(\"foo\", [0.1, 0.3]) bar = Embedding(\"bar\", [0.7, 0.2]) foo - bar ``` \"\"\" copied = deepcopy ( self ) copied . name = f \"( { self . name } - { other . name } )\" copied . vector = self . vector - other . vector return copied Subtract two embeddings. Usage: from whatlies.embedding import Embedding foo = Embedding ( \"foo\" , [ 0.1 , 0.3 ]) bar = Embedding ( \"bar\" , [ 0.7 , 0.2 ]) foo - bar copy ( self ) \u00b6 Show source code in whatlies/embedding.py 50 51 52 53 54 def copy ( self ): \"\"\" Returns a deepcopy of the embdding. \"\"\" return deepcopy ( self ) Returns a deepcopy of the embdding. distance ( self , other , metric = 'cosine' ) \u00b6 Show source code in whatlies/embedding.py 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 def distance ( self , other , metric : str = \"cosine\" ): \"\"\" Calculates the vector distance between two embeddings. Arguments: other: the other embedding you're comparing against metric: the distance metric to use, the list of valid options can be found [here](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise_distances.html) **Usage** ```python from whatlies.embedding import Embedding foo = Embedding(\"foo\", [1.0, 0.0]) bar = Embedding(\"bar\", [0.0, 0.5]) foo.distance(bar) foo.distance(bar, metric=\"euclidean\") foo.distance(bar, metric=\"cosine\") ``` \"\"\" return pairwise_distances ([ self . vector ], [ other . vector ], metric = metric )[ 0 ][ 0 ] Calculates the vector distance between two embeddings. Parameters Name Type Description Default other the other embedding you're comparing against required metric str the distance metric to use, the list of valid options can be found here 'cosine' Usage from whatlies.embedding import Embedding foo = Embedding ( \"foo\" , [ 1.0 , 0.0 ]) bar = Embedding ( \"bar\" , [ 0.0 , 0.5 ]) foo . distance ( bar ) foo . distance ( bar , metric = \"euclidean\" ) foo . distance ( bar , metric = \"cosine\" ) plot ( self , kind = 'arrow' , x_axis = 0 , y_axis = 1 , axis_metric = None , x_label = None , y_label = None , title = None , color = None , show_ops = False , annot = True , axis_option = None ) \u00b6 Show source code in whatlies/embedding.py 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 def plot ( self , kind : str = \"arrow\" , x_axis : Union [ int , \"Embedding\" ] = 0 , y_axis : Union [ int , \"Embedding\" ] = 1 , axis_metric : Optional [ Union [ str , Callable , Sequence ]] = None , x_label : Optional [ str ] = None , y_label : Optional [ str ] = None , title : Optional [ str ] = None , color : str = None , show_ops : bool = False , annot : bool = True , axis_option : Optional [ str ] = None , ): \"\"\" Handles the logic to perform a 2d plot in matplotlib. Arguments: kind: what kind of plot to make, can be `scatter`, `arrow` or `text` x_axis: the x-axis to be used, must be given when dim > 2; if an integer, the corresponding dimension of embedding is used. y_axis: the y-axis to be used, must be given when dim > 2; if an integer, the corresponding dimension of embedding is used. axis_metric: the metric used to project an embedding on the axes; only used when the corresponding axis (i.e. `x_axis` or `y_axis`) is an `Embedding` instance. It could be a string (`'cosine_similarity'`, `'cosine_distance'` or `'euclidean'`), or a callable that takes two vectors as input and returns a scalar value as output. To set different metrics for x- and y-axis, a list or a tuple of two elements could be given. By default (`None`), normalized scalar projection (i.e. `>` operator) is used. x_label: an optional label used for x-axis; if not given, it is set based on `x_axis` value. y_label: an optional label used for y-axis; if not given, it is set based on `y_axis` value. title: an optional title for the plot. color: the color of the dots show_ops: setting to also show the applied operations, only works for `text` annot: should the points be annotated axis_option: a string which is passed as `option` argument to `matplotlib.pyplot.axis` in order to control axis properties (e.g. using `'equal'` make circles shown circular in the plot). This might be useful for preserving geometric relationships (e.g. orthogonality) in the generated plot. See `matplotlib.pyplot.axis` [documentation](https://matplotlib.org/3.1.0/api/_as_gen/matplotlib.pyplot.axis.html#matplotlib-pyplot-axis) for possible values and their description. **Usage** ```python from whatlies.embedding import Embedding foo = Embedding(\"foo\", [0.1, 0.3]) bar = Embedding(\"bar\", [0.7, 0.2]) foo.plot(kind=\"arrow\", annot=True) bar.plot(kind=\"arrow\", annot=True) ``` \"\"\" if isinstance ( axis_metric , ( list , tuple )): x_axis_metric = axis_metric [ 0 ] y_axis_metric = axis_metric [ 1 ] else : x_axis_metric = axis_metric y_axis_metric = axis_metric x_val , x_lab = self . _get_plot_axis_value_and_label ( x_axis , x_axis_metric , dir = \"x\" ) y_val , y_lab = self . _get_plot_axis_value_and_label ( y_axis , y_axis_metric , dir = \"y\" ) x_label = x_lab if x_label is None else x_label y_label = y_lab if y_label is None else y_label emb_plot = Embedding ( name = self . name , vector = [ x_val , y_val ], orig = self . orig ) handle_2d_plot ( emb_plot , kind = kind , color = color , xlabel = x_label , ylabel = y_label , title = title , show_operations = show_ops , annot = annot , axis_option = axis_option , ) return self Handles the logic to perform a 2d plot in matplotlib. Parameters Name Type Description Default kind str what kind of plot to make, can be scatter , arrow or text 'arrow' x_axis Union[int, ForwardRef('Embedding')] the x-axis to be used, must be given when dim > 2; if an integer, the corresponding dimension of embedding is used. 0 y_axis Union[int, ForwardRef('Embedding')] the y-axis to be used, must be given when dim > 2; if an integer, the corresponding dimension of embedding is used. 1 axis_metric Optional[Union[str, Callable, Sequence]] the metric used to project an embedding on the axes; only used when the corresponding axis (i.e. x_axis or y_axis ) is an Embedding instance. It could be a string ( 'cosine_similarity' , 'cosine_distance' or 'euclidean' ), or a callable that takes two vectors as input and returns a scalar value as output. To set different metrics for x- and y-axis, a list or a tuple of two elements could be given. By default ( None ), normalized scalar projection (i.e. > operator) is used. None x_label Optional[str] an optional label used for x-axis; if not given, it is set based on x_axis value. None y_label Optional[str] an optional label used for y-axis; if not given, it is set based on y_axis value. None title Optional[str] an optional title for the plot. None color str the color of the dots None show_ops bool setting to also show the applied operations, only works for text False annot bool should the points be annotated True axis_option Optional[str] a string which is passed as option argument to matplotlib.pyplot.axis in order to control axis properties (e.g. using 'equal' make circles shown circular in the plot). This might be useful for preserving geometric relationships (e.g. orthogonality) in the generated plot. See matplotlib.pyplot.axis documentation for possible values and their description. None Usage from whatlies.embedding import Embedding foo = Embedding ( \"foo\" , [ 0.1 , 0.3 ]) bar = Embedding ( \"bar\" , [ 0.7 , 0.2 ]) foo . plot ( kind = \"arrow\" , annot = True ) bar . plot ( kind = \"arrow\" , annot = True )","title":"Embedding"},{"location":"api/embedding/#whatliesembeddingembedding","text":"This object represents a word embedding. It contains a vector and a name. Parameters Name Type Description Default name the name of this embedding, includes operations required vector the numerical representation of the embedding required orig original name of embedding, is left alone None Usage: from whatlies.embedding import Embedding foo = Embedding ( \"foo\" , [ 0.1 , 0.3 ]) bar = Embedding ( \"bar\" , [ 0.7 , 0.2 ]) foo | bar foo - bar + bar","title":"whatlies.embedding.Embedding"},{"location":"api/embedding/#whatlies.embedding.Embedding.ndim","text":"Return the dimension of embedding vector.","title":"ndim"},{"location":"api/embedding/#whatlies.embedding.Embedding.norm","text":"Gives the norm of the vector of the embedding","title":"norm"},{"location":"api/embedding/#whatlies.embedding.Embedding.copy","text":"Show source code in whatlies/embedding.py 50 51 52 53 54 def copy ( self ): \"\"\" Returns a deepcopy of the embdding. \"\"\" return deepcopy ( self ) Returns a deepcopy of the embdding.","title":"copy()"},{"location":"api/embedding/#whatlies.embedding.Embedding.distance","text":"Show source code in whatlies/embedding.py 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 def distance ( self , other , metric : str = \"cosine\" ): \"\"\" Calculates the vector distance between two embeddings. Arguments: other: the other embedding you're comparing against metric: the distance metric to use, the list of valid options can be found [here](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise_distances.html) **Usage** ```python from whatlies.embedding import Embedding foo = Embedding(\"foo\", [1.0, 0.0]) bar = Embedding(\"bar\", [0.0, 0.5]) foo.distance(bar) foo.distance(bar, metric=\"euclidean\") foo.distance(bar, metric=\"cosine\") ``` \"\"\" return pairwise_distances ([ self . vector ], [ other . vector ], metric = metric )[ 0 ][ 0 ] Calculates the vector distance between two embeddings. Parameters Name Type Description Default other the other embedding you're comparing against required metric str the distance metric to use, the list of valid options can be found here 'cosine' Usage from whatlies.embedding import Embedding foo = Embedding ( \"foo\" , [ 1.0 , 0.0 ]) bar = Embedding ( \"bar\" , [ 0.0 , 0.5 ]) foo . distance ( bar ) foo . distance ( bar , metric = \"euclidean\" ) foo . distance ( bar , metric = \"cosine\" )","title":"distance()"},{"location":"api/embedding/#whatlies.embedding.Embedding.plot","text":"Show source code in whatlies/embedding.py 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 def plot ( self , kind : str = \"arrow\" , x_axis : Union [ int , \"Embedding\" ] = 0 , y_axis : Union [ int , \"Embedding\" ] = 1 , axis_metric : Optional [ Union [ str , Callable , Sequence ]] = None , x_label : Optional [ str ] = None , y_label : Optional [ str ] = None , title : Optional [ str ] = None , color : str = None , show_ops : bool = False , annot : bool = True , axis_option : Optional [ str ] = None , ): \"\"\" Handles the logic to perform a 2d plot in matplotlib. Arguments: kind: what kind of plot to make, can be `scatter`, `arrow` or `text` x_axis: the x-axis to be used, must be given when dim > 2; if an integer, the corresponding dimension of embedding is used. y_axis: the y-axis to be used, must be given when dim > 2; if an integer, the corresponding dimension of embedding is used. axis_metric: the metric used to project an embedding on the axes; only used when the corresponding axis (i.e. `x_axis` or `y_axis`) is an `Embedding` instance. It could be a string (`'cosine_similarity'`, `'cosine_distance'` or `'euclidean'`), or a callable that takes two vectors as input and returns a scalar value as output. To set different metrics for x- and y-axis, a list or a tuple of two elements could be given. By default (`None`), normalized scalar projection (i.e. `>` operator) is used. x_label: an optional label used for x-axis; if not given, it is set based on `x_axis` value. y_label: an optional label used for y-axis; if not given, it is set based on `y_axis` value. title: an optional title for the plot. color: the color of the dots show_ops: setting to also show the applied operations, only works for `text` annot: should the points be annotated axis_option: a string which is passed as `option` argument to `matplotlib.pyplot.axis` in order to control axis properties (e.g. using `'equal'` make circles shown circular in the plot). This might be useful for preserving geometric relationships (e.g. orthogonality) in the generated plot. See `matplotlib.pyplot.axis` [documentation](https://matplotlib.org/3.1.0/api/_as_gen/matplotlib.pyplot.axis.html#matplotlib-pyplot-axis) for possible values and their description. **Usage** ```python from whatlies.embedding import Embedding foo = Embedding(\"foo\", [0.1, 0.3]) bar = Embedding(\"bar\", [0.7, 0.2]) foo.plot(kind=\"arrow\", annot=True) bar.plot(kind=\"arrow\", annot=True) ``` \"\"\" if isinstance ( axis_metric , ( list , tuple )): x_axis_metric = axis_metric [ 0 ] y_axis_metric = axis_metric [ 1 ] else : x_axis_metric = axis_metric y_axis_metric = axis_metric x_val , x_lab = self . _get_plot_axis_value_and_label ( x_axis , x_axis_metric , dir = \"x\" ) y_val , y_lab = self . _get_plot_axis_value_and_label ( y_axis , y_axis_metric , dir = \"y\" ) x_label = x_lab if x_label is None else x_label y_label = y_lab if y_label is None else y_label emb_plot = Embedding ( name = self . name , vector = [ x_val , y_val ], orig = self . orig ) handle_2d_plot ( emb_plot , kind = kind , color = color , xlabel = x_label , ylabel = y_label , title = title , show_operations = show_ops , annot = annot , axis_option = axis_option , ) return self Handles the logic to perform a 2d plot in matplotlib. Parameters Name Type Description Default kind str what kind of plot to make, can be scatter , arrow or text 'arrow' x_axis Union[int, ForwardRef('Embedding')] the x-axis to be used, must be given when dim > 2; if an integer, the corresponding dimension of embedding is used. 0 y_axis Union[int, ForwardRef('Embedding')] the y-axis to be used, must be given when dim > 2; if an integer, the corresponding dimension of embedding is used. 1 axis_metric Optional[Union[str, Callable, Sequence]] the metric used to project an embedding on the axes; only used when the corresponding axis (i.e. x_axis or y_axis ) is an Embedding instance. It could be a string ( 'cosine_similarity' , 'cosine_distance' or 'euclidean' ), or a callable that takes two vectors as input and returns a scalar value as output. To set different metrics for x- and y-axis, a list or a tuple of two elements could be given. By default ( None ), normalized scalar projection (i.e. > operator) is used. None x_label Optional[str] an optional label used for x-axis; if not given, it is set based on x_axis value. None y_label Optional[str] an optional label used for y-axis; if not given, it is set based on y_axis value. None title Optional[str] an optional title for the plot. None color str the color of the dots None show_ops bool setting to also show the applied operations, only works for text False annot bool should the points be annotated True axis_option Optional[str] a string which is passed as option argument to matplotlib.pyplot.axis in order to control axis properties (e.g. using 'equal' make circles shown circular in the plot). This might be useful for preserving geometric relationships (e.g. orthogonality) in the generated plot. See matplotlib.pyplot.axis documentation for possible values and their description. None Usage from whatlies.embedding import Embedding foo = Embedding ( \"foo\" , [ 0.1 , 0.3 ]) bar = Embedding ( \"bar\" , [ 0.7 , 0.2 ]) foo . plot ( kind = \"arrow\" , annot = True ) bar . plot ( kind = \"arrow\" , annot = True )","title":"plot()"},{"location":"api/embeddingset/","text":"whatlies.embeddingset.EmbeddingSet \u00b6 This object represents a set of Embedding s. You can use the same operations as an Embedding but here we apply it to the entire set instead of a single Embedding . Parameters embeddings : list of Embedding , or a single dictionary containing name: Embedding pairs name : custom name of embeddingset Usage: from whatlies.embedding import Embedding from whatlies.embeddingset import EmbeddingSet foo = Embedding ( \"foo\" , [ 0.1 , 0.3 ]) bar = Embedding ( \"bar\" , [ 0.7 , 0.2 ]) emb = EmbeddingSet ( foo , bar ) emb = EmbeddingSet ({ 'foo' : foo , 'bar' : bar ) ndim : (property, readonly) \u00b6 Return dimension of embedding vectors in embeddingset. __add__ ( self , other ) \u00b6 Show source code in whatlies/embeddingset.py 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 def __add__ ( self , other ): \"\"\" Adds an embedding to each element in the embeddingset. Usage: ```python from whatlies.embedding import Embedding from whatlies.embeddingset import EmbeddingSet foo = Embedding(\"foo\", [0.1, 0.3]) bar = Embedding(\"bar\", [0.7, 0.2]) buz = Embedding(\"buz\", [0.1, 0.9]) emb = EmbeddingSet(foo, bar) (emb).plot(kind=\"arrow\") (emb + buz).plot(kind=\"arrow\") ``` \"\"\" new_embeddings = { k : emb + other for k , emb in self . embeddings . items ()} return EmbeddingSet ( new_embeddings , name = f \"( { self . name } + { other . name } )\" ) Adds an embedding to each element in the embeddingset. Usage: from whatlies.embedding import Embedding from whatlies.embeddingset import EmbeddingSet foo = Embedding ( \"foo\" , [ 0.1 , 0.3 ]) bar = Embedding ( \"bar\" , [ 0.7 , 0.2 ]) buz = Embedding ( \"buz\" , [ 0.1 , 0.9 ]) emb = EmbeddingSet ( foo , bar ) ( emb ) . plot ( kind = \"arrow\" ) ( emb + buz ) . plot ( kind = \"arrow\" ) __contains__ ( self , item ) \u00b6 Show source code in whatlies/embeddingset.py 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 def __contains__ ( self , item ): \"\"\" Checks if an item is in the embeddingset. Usage: ```python from whatlies.embedding import Embedding from whatlies.embeddingset import EmbeddingSet foo = Embedding(\"foo\", [0.1, 0.3]) bar = Embedding(\"bar\", [0.7, 0.2]) buz = Embedding(\"buz\", [0.1, 0.9]) emb = EmbeddingSet(foo, bar) \"foo\" in emb # True \"dinosaur\" in emb # False ``` \"\"\" return item in self . embeddings . keys () Checks if an item is in the embeddingset. Usage: from whatlies.embedding import Embedding from whatlies.embeddingset import EmbeddingSet foo = Embedding ( \"foo\" , [ 0.1 , 0.3 ]) bar = Embedding ( \"bar\" , [ 0.7 , 0.2 ]) buz = Embedding ( \"buz\" , [ 0.1 , 0.9 ]) emb = EmbeddingSet ( foo , bar ) \"foo\" in emb # True \"dinosaur\" in emb # False __getitem__ ( self , thing ) \u00b6 Show source code in whatlies/embeddingset.py 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 def __getitem__ ( self , thing ): \"\"\" Retreive a single embedding from the embeddingset. Usage: ```python from whatlies.embeddingset import EmbeddingSet foo = Embedding(\"foo\", [0.1, 0.3, 0.10]) bar = Embedding(\"bar\", [0.7, 0.2, 0.11]) buz = Embedding(\"buz\", [0.1, 0.9, 0.12]) emb = EmbeddingSet(foo, bar, buz) emb[\"buz\"] ``` \"\"\" if isinstance ( thing , str ): return self . embeddings [ thing ] new_embeddings = { t : self [ t ] for t in thing } names = \",\" . join ( thing ) return EmbeddingSet ( new_embeddings , name = f \" { self . name } .subset( { names } )\" ) Retreive a single embedding from the embeddingset. Usage: from whatlies.embeddingset import EmbeddingSet foo = Embedding ( \"foo\" , [ 0.1 , 0.3 , 0.10 ]) bar = Embedding ( \"bar\" , [ 0.7 , 0.2 , 0.11 ]) buz = Embedding ( \"buz\" , [ 0.1 , 0.9 , 0.12 ]) emb = EmbeddingSet ( foo , bar , buz ) emb [ \"buz\" ] __iter__ ( self ) \u00b6 Show source code in whatlies/embeddingset.py 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 def __iter__ ( self ): \"\"\" Iterate over all the embeddings in the embeddingset. Usage: ```python from whatlies.embedding import Embedding from whatlies.embeddingset import EmbeddingSet foo = Embedding(\"foo\", [0.1, 0.3]) bar = Embedding(\"bar\", [0.7, 0.2]) buz = Embedding(\"buz\", [0.1, 0.9]) emb = EmbeddingSet(foo, bar) [e for e in emb] ``` \"\"\" return self . embeddings . values () . __iter__ () Iterate over all the embeddings in the embeddingset. Usage: from whatlies.embedding import Embedding from whatlies.embeddingset import EmbeddingSet foo = Embedding ( \"foo\" , [ 0.1 , 0.3 ]) bar = Embedding ( \"bar\" , [ 0.7 , 0.2 ]) buz = Embedding ( \"buz\" , [ 0.1 , 0.9 ]) emb = EmbeddingSet ( foo , bar ) [ e for e in emb ] __or__ ( self , other ) \u00b6 Show source code in whatlies/embeddingset.py 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 def __or__ ( self , other ): \"\"\" Makes every element in the embeddingset othogonal to the passed embedding. Usage: ```python from whatlies.embedding import Embedding from whatlies.embeddingset import EmbeddingSet foo = Embedding(\"foo\", [0.1, 0.3]) bar = Embedding(\"bar\", [0.7, 0.2]) buz = Embedding(\"buz\", [0.1, 0.9]) emb = EmbeddingSet(foo, bar) (emb).plot(kind=\"arrow\") (emb | buz).plot(kind=\"arrow\") ``` \"\"\" new_embeddings = { k : emb | other for k , emb in self . embeddings . items ()} return EmbeddingSet ( new_embeddings , name = f \"( { self . name } | { other . name } )\" ) Makes every element in the embeddingset othogonal to the passed embedding. Usage: from whatlies.embedding import Embedding from whatlies.embeddingset import EmbeddingSet foo = Embedding ( \"foo\" , [ 0.1 , 0.3 ]) bar = Embedding ( \"bar\" , [ 0.7 , 0.2 ]) buz = Embedding ( \"buz\" , [ 0.1 , 0.9 ]) emb = EmbeddingSet ( foo , bar ) ( emb ) . plot ( kind = \"arrow\" ) ( emb | buz ) . plot ( kind = \"arrow\" ) __rshift__ ( self , other ) \u00b6 Show source code in whatlies/embeddingset.py 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 def __rshift__ ( self , other ): \"\"\" Maps every embedding in the embedding set unto the passed embedding. Usage: ```python from whatlies.embedding import Embedding from whatlies.embeddingset import EmbeddingSet foo = Embedding(\"foo\", [0.1, 0.3]) bar = Embedding(\"bar\", [0.7, 0.2]) buz = Embedding(\"buz\", [0.1, 0.9]) emb = EmbeddingSet(foo, bar) (emb).plot(kind=\"arrow\") (emb >> buz).plot(kind=\"arrow\") ``` \"\"\" new_embeddings = { k : emb >> other for k , emb in self . embeddings . items ()} return EmbeddingSet ( new_embeddings , name = f \"( { self . name } >> { other . name } )\" ) Maps every embedding in the embedding set unto the passed embedding. Usage: from whatlies.embedding import Embedding from whatlies.embeddingset import EmbeddingSet foo = Embedding ( \"foo\" , [ 0.1 , 0.3 ]) bar = Embedding ( \"bar\" , [ 0.7 , 0.2 ]) buz = Embedding ( \"buz\" , [ 0.1 , 0.9 ]) emb = EmbeddingSet ( foo , bar ) ( emb ) . plot ( kind = \"arrow\" ) ( emb >> buz ) . plot ( kind = \"arrow\" ) __sub__ ( self , other ) \u00b6 Show source code in whatlies/embeddingset.py 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 def __sub__ ( self , other ): \"\"\" Subtracts an embedding from each element in the embeddingset. Usage: ```python from whatlies.embedding import Embedding from whatlies.embeddingset import EmbeddingSet foo = Embedding(\"foo\", [0.1, 0.3]) bar = Embedding(\"bar\", [0.7, 0.2]) buz = Embedding(\"buz\", [0.1, 0.9]) emb = EmbeddingSet(foo, bar) (emb).plot(kind=\"arrow\") (emb - buz).plot(kind=\"arrow\") ``` \"\"\" new_embeddings = { k : emb - other for k , emb in self . embeddings . items ()} return EmbeddingSet ( new_embeddings , name = f \"( { self . name } - { other . name } )\" ) Subtracts an embedding from each element in the embeddingset. Usage: from whatlies.embedding import Embedding from whatlies.embeddingset import EmbeddingSet foo = Embedding ( \"foo\" , [ 0.1 , 0.3 ]) bar = Embedding ( \"bar\" , [ 0.7 , 0.2 ]) buz = Embedding ( \"buz\" , [ 0.1 , 0.9 ]) emb = EmbeddingSet ( foo , bar ) ( emb ) . plot ( kind = \"arrow\" ) ( emb - buz ) . plot ( kind = \"arrow\" ) add_property ( self , name , func ) \u00b6 Show source code in whatlies/embeddingset.py 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 def add_property ( self , name , func ): \"\"\" Adds a property to every embedding in the set. Very useful for plotting because a property can be used to assign colors. Arguments: name: name of the property to add func: function that receives an embedding and needs to output the property value Usage: ```python from whatlies.embeddingset import EmbeddingSet foo = Embedding(\"foo\", [0.1, 0.3, 0.10]) bar = Embedding(\"bar\", [0.7, 0.2, 0.11]) emb = EmbeddingSet(foo, bar) emb_with_property = emb.add_property('example', lambda d: 'group-one') ``` \"\"\" return EmbeddingSet ( { k : e . add_property ( name , func ) for k , e in self . embeddings . items ()} ) Adds a property to every embedding in the set. Very useful for plotting because a property can be used to assign colors. Parameters Name Type Description Default name name of the property to add required func function that receives an embedding and needs to output the property value required Usage: from whatlies.embeddingset import EmbeddingSet foo = Embedding ( \"foo\" , [ 0.1 , 0.3 , 0.10 ]) bar = Embedding ( \"bar\" , [ 0.7 , 0.2 , 0.11 ]) emb = EmbeddingSet ( foo , bar ) emb_with_property = emb . add_property ( 'example' , lambda d : 'group-one' ) assign ( self , ** kwargs ) \u00b6 Show source code in whatlies/embeddingset.py 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 def assign ( self , ** kwargs ): \"\"\" Adds properties to every embedding in the set based on the keyword arguments. This is very useful for plotting because a property can be used to assign colors. This method is very similar to `.add_property` but it might be more convenient when you want to assign multiple properties in one single statement. Arguments: kwargs: (name, func)-pairs that describe the name of the property as well as a value to assign. The value can be a single value, iterable or a function. The function expects an `Embedding` object as input. Usage: ```python from whatlies.embeddingset import EmbeddingSet foo = Embedding(\"foo\", [0.1, 0.3, 0.10]) bar = Embedding(\"bar\", [0.7, 0.2, 0.11]) emb = EmbeddingSet(foo, bar) emb_with_property1 = emb.assign(dim0=lambda d: d.vector[0], dim1=lambda d: d.vector[1], dim2=lambda d: d.vector[2]) emb_with_property2 = emb.assign(group=[\"foo_grp\", \"bar_grp\"]) emb_with_property3 = emb.assign(constant=1) ``` \"\"\" new_set = {} for idx , ( k , e ) in enumerate ( self . embeddings . items ()): new_emb = e for name , val in kwargs . items (): if callable ( val ): new_emb = new_emb . add_property ( name , val ) elif hasattr ( val , \"__iter__\" ) and not isinstance ( val , str ): # We want to support lists, tuples, numpy arrays but not strings # those need to be handle as if they're literals. if len ( val ) != len ( self ): raise ValueError ( f \"If you're passing an iterable to `.assign` then it must have the same length as the `EmbeddingSet`. \\n Got: { len ( val ) } . Expected: { len ( self ) } .\" ) new_emb = new_emb . add_property ( name , lambda d : val [ idx ]) else : new_emb = new_emb . add_property ( name , lambda d : val ) new_set [ k ] = new_emb return EmbeddingSet ( new_set ) Adds properties to every embedding in the set based on the keyword arguments. This is very useful for plotting because a property can be used to assign colors. This method is very similar to .add_property but it might be more convenient when you want to assign multiple properties in one single statement. Parameters Name Type Description Default **kwargs (name, func)-pairs that describe the name of the property as well as a value to assign. The value can be a single value, iterable or a function. The function expects an Embedding object as input. {} Usage: from whatlies.embeddingset import EmbeddingSet foo = Embedding ( \"foo\" , [ 0.1 , 0.3 , 0.10 ]) bar = Embedding ( \"bar\" , [ 0.7 , 0.2 , 0.11 ]) emb = EmbeddingSet ( foo , bar ) emb_with_property1 = emb . assign ( dim0 = lambda d : d . vector [ 0 ], dim1 = lambda d : d . vector [ 1 ], dim2 = lambda d : d . vector [ 2 ]) emb_with_property2 = emb . assign ( group = [ \"foo_grp\" , \"bar_grp\" ]) emb_with_property3 = emb . assign ( constant = 1 ) average ( self , name = None ) \u00b6 Show source code in whatlies/embeddingset.py 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 def average ( self , name = None ): \"\"\" Takes the average over all the embedding vectors in the embeddingset. Turns it into a new `Embedding`. Arguments: name: manually specify the name of the average embedding Usage: ```python from whatlies.embeddingset import EmbeddingSet foo = Embedding(\"foo\", [1.0, 0.0]) bar = Embedding(\"bar\", [0.0, 1.0]) emb = EmbeddingSet(foo, bar) emb.average().vector # [0.5, 0,5] emb.average(name=\"the-average\").vector # [0.5, 0.5] ``` \"\"\" name = f \" { self . name } .average()\" if not name else name x = self . to_X () return Embedding ( name , np . mean ( x , axis = 0 )) Takes the average over all the embedding vectors in the embeddingset. Turns it into a new Embedding . Parameters Name Type Description Default name manually specify the name of the average embedding None Usage: from whatlies.embeddingset import EmbeddingSet foo = Embedding ( \"foo\" , [ 1.0 , 0.0 ]) bar = Embedding ( \"bar\" , [ 0.0 , 1.0 ]) emb = EmbeddingSet ( foo , bar ) emb . average () . vector # [0.5, 0,5] emb . average ( name = \"the-average\" ) . vector # [0.5, 0.5] compare_against ( self , other , mapping = None ) \u00b6 Show source code in whatlies/embeddingset.py 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 def compare_against ( self , other : Union [ str , Embedding ], mapping : Optional [ Callable ] = None ) -> List : \"\"\" Compare (or map) the embeddigns in the embeddingset to a given embedding, optionally using a custom mapping function. Arguments: other: an `Embedding` instance, or name of an existing embedding; it is used for comparison with each embedding in the embeddingset. mapping: an optional callable used for for comparison that takes two 1D vector arrays as input; if not given, the normalized scalar projection (i.e. `>` operator) is used. \"\"\" if isinstance ( other , str ): other = self [ other ] if mapping is None : return [ v > other for v in self . embeddings . values ()] elif callable ( mapping ): return [ mapping ( v . vector , other . vector ) for v in self . embeddings . values ()] else : raise ValueError ( f \"Unrecognized mapping value/type, got: { mapping } \" ) Compare (or map) the embeddigns in the embeddingset to a given embedding, optionally using a custom mapping function. Parameters Name Type Description Default other Union[str, whatlies.embedding.Embedding] an Embedding instance, or name of an existing embedding; it is used for comparison with each embedding in the embeddingset. required mapping Optional[Callable] an optional callable used for for comparison that takes two 1D vector arrays as input; if not given, the normalized scalar projection (i.e. > operator) is used. None embset_similar ( self , emb , n = 10 , metric = 'cosine' ) \u00b6 Show source code in whatlies/embeddingset.py 547 548 549 550 551 552 553 554 555 556 557 558 559 560 def embset_similar ( self , emb : Union [ str , Embedding ], n : int = 10 , metric = \"cosine\" ): \"\"\" Retreive an [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] that are the most simmilar to the passed query. Arguments: emb: query to use n: the number of items you'd like to see returned metric: metric to use to calculate distance, must be scipy or sklearn compatible Returns: An [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] containing the similar embeddings. \"\"\" embs = [ w [ 0 ] for w in self . score_similar ( emb , n , metric )] return EmbeddingSet ({ w . name : w for w in embs }) Retreive an EmbeddingSet that are the most simmilar to the passed query. Parameters Name Type Description Default emb Union[str, whatlies.embedding.Embedding] query to use required n int the number of items you'd like to see returned 10 metric metric to use to calculate distance, must be scipy or sklearn compatible 'cosine' Returns Type Description `` An EmbeddingSet containing the similar embeddings. filter ( self , func ) \u00b6 Show source code in whatlies/embeddingset.py 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 def filter ( self , func ): \"\"\" Filters the collection of embeddings based on a predicate function. Arguments: func: callable that accepts a single embedding and outputs a boolean ```python from whatlies.embeddingset import EmbeddingSet foo = Embedding(\"foo\", [0.1, 0.3, 0.10]) bar = Embedding(\"bar\", [0.7, 0.2, 0.11]) buz = Embedding(\"buz\", [0.1, 0.9, 0.12]) xyz = Embedding(\"xyz\", [0.1, 0.9, 0.12]) emb = EmbeddingSet(foo, bar, buz, xyz) emb.filter(lambda e: \"foo\" not in e.name) ``` \"\"\" return EmbeddingSet ({ k : v for k , v in self . embeddings . items () if func ( v )}) Filters the collection of embeddings based on a predicate function. Parameters Name Type Description Default func callable that accepts a single embedding and outputs a boolean required from whatlies.embeddingset import EmbeddingSet foo = Embedding ( \"foo\" , [ 0.1 , 0.3 , 0.10 ]) bar = Embedding ( \"bar\" , [ 0.7 , 0.2 , 0.11 ]) buz = Embedding ( \"buz\" , [ 0.1 , 0.9 , 0.12 ]) xyz = Embedding ( \"xyz\" , [ 0.1 , 0.9 , 0.12 ]) emb = EmbeddingSet ( foo , bar , buz , xyz ) emb . filter ( lambda e : \"foo\" not in e . name ) from_names_X ( names , X ) (classmethod) \u00b6 Show source code in whatlies/embeddingset.py 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 @classmethod def from_names_X ( cls , names , X ): \"\"\" Constructs an `EmbeddingSet` instance from the given embedding names and vectors. Arguments: names: an iterable containing the names of embeddings X: an iterable of 1D vectors, or a 2D numpy array; it should have the same length as `names` Usage: ```python from whatlies.embeddingset import EmbeddingSet names = [\"foo\", \"bar\", \"buz\"] vecs = [ [0.1, 0.3], [0.7, 0.2], [0.1, 0.9], ] emb = EmbeddingSet.from_names_X(names, vecs) ``` \"\"\" X = np . array ( X ) if len ( X ) != len ( names ): raise ValueError ( f \"The number of given names ( { len ( names ) } ) and vectors ( { len ( X ) } ) should be the same.\" ) return cls ({ n : Embedding ( n , v ) for n , v in zip ( names , X )}) Constructs an EmbeddingSet instance from the given embedding names and vectors. Parameters Name Type Description Default names an iterable containing the names of embeddings required X an iterable of 1D vectors, or a 2D numpy array; it should have the same length as names required Usage: from whatlies.embeddingset import EmbeddingSet names = [ \"foo\" , \"bar\" , \"buz\" ] vecs = [ [ 0.1 , 0.3 ], [ 0.7 , 0.2 ], [ 0.1 , 0.9 ], ] emb = EmbeddingSet . from_names_X ( names , vecs ) merge ( self , other ) \u00b6 Show source code in whatlies/embeddingset.py 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 def merge ( self , other ): \"\"\" Concatenates two embeddingssets together Arguments: other: another embeddingset Usage: ```python from whatlies.embeddingset import EmbeddingSet foo = Embedding(\"foo\", [0.1, 0.3, 0.10]) bar = Embedding(\"bar\", [0.7, 0.2, 0.11]) buz = Embedding(\"buz\", [0.1, 0.9, 0.12]) xyz = Embedding(\"xyz\", [0.1, 0.9, 0.12]) emb1 = EmbeddingSet(foo, bar) emb2 = EmbeddingSet(xyz, buz) both = emb1.merge(emb2) ``` \"\"\" return EmbeddingSet ({ ** self . embeddings , ** other . embeddings }) Concatenates two embeddingssets together Parameters Name Type Description Default other another embeddingset required Usage: from whatlies.embeddingset import EmbeddingSet foo = Embedding ( \"foo\" , [ 0.1 , 0.3 , 0.10 ]) bar = Embedding ( \"bar\" , [ 0.7 , 0.2 , 0.11 ]) buz = Embedding ( \"buz\" , [ 0.1 , 0.9 , 0.12 ]) xyz = Embedding ( \"xyz\" , [ 0.1 , 0.9 , 0.12 ]) emb1 = EmbeddingSet ( foo , bar ) emb2 = EmbeddingSet ( xyz , buz ) both = emb1 . merge ( emb2 ) movement_df ( self , other , metric = 'euclidean' ) \u00b6 Show source code in whatlies/embeddingset.py 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 def movement_df ( self , other , metric = \"euclidean\" ): \"\"\" Creates a dataframe that shows the movement from one embeddingset to another one. Arguments: other: the other embeddingset to compare against, will only keep the overlap metric: metric to use to calculate movement, must be scipy or sklearn compatible Usage: ```python from whatlies.language import SpacyLanguage lang = SpacyLanguage(\"en_core_web_sm\") names = ['red', 'blue', 'green', 'yellow', 'cat', 'dog', 'mouse', 'rat', 'bike', 'car'] emb = lang[names] emb_ort = lang[names] | lang['cat'] emb.movement_df(emb_ort) ``` \"\"\" overlap = list ( set ( self . embeddings . keys ()) . intersection ( set ( other . embeddings . keys ())) ) mat1 = np . array ([ w . vector for w in self [ overlap ]]) mat2 = np . array ([ w . vector for w in other [ overlap ]]) return ( pd . DataFrame ( { \"name\" : overlap , \"movement\" : paired_distances ( mat1 , mat2 , metric = metric ), } ) . sort_values ([ \"movement\" ], ascending = False ) . reset_index () ) Creates a dataframe that shows the movement from one embeddingset to another one. Parameters Name Type Description Default other the other embeddingset to compare against, will only keep the overlap required metric metric to use to calculate movement, must be scipy or sklearn compatible 'euclidean' Usage: from whatlies.language import SpacyLanguage lang = SpacyLanguage ( \"en_core_web_sm\" ) names = [ 'red' , 'blue' , 'green' , 'yellow' , 'cat' , 'dog' , 'mouse' , 'rat' , 'bike' , 'car' ] emb = lang [ names ] emb_ort = lang [ names ] | lang [ 'cat' ] emb . movement_df ( emb_ort ) pipe ( self , func , * args , ** kwargs ) \u00b6 Show source code in whatlies/embeddingset.py 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 def pipe ( self , func , * args , ** kwargs ): \"\"\" Applies a function to the embedding set. Useful for method chaining and chunks of code that repeat. Arguments: func: callable that accepts an `EmbeddingSet` set as its first argument args: arguments to also pass to the function kwargs: keyword arguments to also pass to the function ```python from whatlies.language import SpacyLanguage, BytePairLanguage lang_sp = SpacyLanguage(\"en_core_web_sm\") lang_bp = BytePairLanguage(\"en\", dim=25, vs=1000) text = [\"cat\", \"dog\", \"rat\", \"blue\", \"red\", \"yellow\"] def make_plot(embset): return (embset .plot_interactive(\"dog\", \"blue\") .properties(height=200, width=200)) p1 = lang_sp[text].pipe(make_plot) p2 = lang_bp[text].pipe(make_plot) p1 | p2 ``` \"\"\" return func ( self , * args , ** kwargs ) Applies a function to the embedding set. Useful for method chaining and chunks of code that repeat. Parameters Name Type Description Default func callable that accepts an EmbeddingSet set as its first argument required *args arguments to also pass to the function () **kwargs keyword arguments to also pass to the function {} from whatlies.language import SpacyLanguage , BytePairLanguage lang_sp = SpacyLanguage ( \"en_core_web_sm\" ) lang_bp = BytePairLanguage ( \"en\" , dim = 25 , vs = 1000 ) text = [ \"cat\" , \"dog\" , \"rat\" , \"blue\" , \"red\" , \"yellow\" ] def make_plot ( embset ): return ( embset . plot_interactive ( \"dog\" , \"blue\" ) . properties ( height = 200 , width = 200 )) p1 = lang_sp [ text ] . pipe ( make_plot ) p2 = lang_bp [ text ] . pipe ( make_plot ) p1 | p2 plot ( self , kind = 'arrow' , x_axis = 0 , y_axis = 1 , axis_metric = None , x_label = None , y_label = None , title = None , color = None , show_ops = False , annot = True , axis_option = None ) \u00b6 Show source code in whatlies/embeddingset.py 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 def plot ( self , kind : str = \"arrow\" , x_axis : Union [ int , str , Embedding ] = 0 , y_axis : Union [ int , str , Embedding ] = 1 , axis_metric : Optional [ Union [ str , Callable , Sequence ]] = None , x_label : Optional [ str ] = None , y_label : Optional [ str ] = None , title : Optional [ str ] = None , color : str = None , show_ops : bool = False , annot : bool = True , axis_option : Optional [ str ] = None , ): \"\"\" Makes (perhaps inferior) matplotlib plot. Consider using `plot_interactive` instead. Arguments: kind: what kind of plot to make, can be `scatter`, `arrow` or `text` x_axis: the x-axis to be used, must be given when dim > 2; if an integer, the corresponding dimension of embedding is used. y_axis: the y-axis to be used, must be given when dim > 2; if an integer, the corresponding dimension of embedding is used. axis_metric: the metric used to project each embedding on the axes; only used when the corresponding axis (i.e. `x_axis` or `y_axis`) is a string or an `Embedding` instance. It could be a string (`'cosine_similarity'`, `'cosine_distance'` or `'euclidean'`), or a callable that takes two vectors as input and returns a scalar value as output. To set different metrics for x- and y-axis, a list or a tuple of two elements could be given. By default (`None`), normalized scalar projection (i.e. `>` operator) is used. x_label: an optional label used for x-axis; if not given, it is set based on value of `x_axis`. y_label: an optional label used for y-axis; if not given, it is set based on value of `y_axis`. title: an optional title for the plot. color: the color of the dots show_ops: setting to also show the applied operations, only works for `text` annot: should the points be annotated axis_option: a string which is passed as `option` argument to `matplotlib.pyplot.axis` in order to control axis properties (e.g. using `'equal'` make circles shown circular in the plot). This might be useful for preserving geometric relationships (e.g. orthogonality) in the generated plot. See `matplotlib.pyplot.axis` [documentation](https://matplotlib.org/3.1.0/api/_as_gen/matplotlib.pyplot.axis.html#matplotlib-pyplot-axis) for possible values and their description. \"\"\" if isinstance ( x_axis , str ): x_axis = self [ x_axis ] if isinstance ( y_axis , str ): y_axis = self [ y_axis ] if isinstance ( axis_metric , ( list , tuple )): x_axis_metric = axis_metric [ 0 ] y_axis_metric = axis_metric [ 1 ] else : x_axis_metric = axis_metric y_axis_metric = axis_metric embeddings = [] for emb in self . embeddings . values (): x_val , x_lab = emb . _get_plot_axis_value_and_label ( x_axis , x_axis_metric , dir = \"x\" ) y_val , y_lab = emb . _get_plot_axis_value_and_label ( y_axis , y_axis_metric , dir = \"y\" ) emb_plot = Embedding ( name = emb . name , vector = [ x_val , y_val ], orig = emb . orig ) embeddings . append ( emb_plot ) x_label = x_lab if x_label is None else x_label y_label = y_lab if y_label is None else y_label handle_2d_plot ( embeddings , kind = kind , color = color , xlabel = x_label , ylabel = y_label , title = title , show_operations = show_ops , annot = annot , axis_option = axis_option , ) return self Makes (perhaps inferior) matplotlib plot. Consider using plot_interactive instead. Parameters Name Type Description Default kind str what kind of plot to make, can be scatter , arrow or text 'arrow' x_axis Union[int, str, whatlies.embedding.Embedding] the x-axis to be used, must be given when dim > 2; if an integer, the corresponding dimension of embedding is used. 0 y_axis Union[int, str, whatlies.embedding.Embedding] the y-axis to be used, must be given when dim > 2; if an integer, the corresponding dimension of embedding is used. 1 axis_metric Optional[Union[str, Callable, Sequence]] the metric used to project each embedding on the axes; only used when the corresponding axis (i.e. x_axis or y_axis ) is a string or an Embedding instance. It could be a string ( 'cosine_similarity' , 'cosine_distance' or 'euclidean' ), or a callable that takes two vectors as input and returns a scalar value as output. To set different metrics for x- and y-axis, a list or a tuple of two elements could be given. By default ( None ), normalized scalar projection (i.e. > operator) is used. None x_label Optional[str] an optional label used for x-axis; if not given, it is set based on value of x_axis . None y_label Optional[str] an optional label used for y-axis; if not given, it is set based on value of y_axis . None title Optional[str] an optional title for the plot. None color str the color of the dots None show_ops bool setting to also show the applied operations, only works for text False annot bool should the points be annotated True axis_option Optional[str] a string which is passed as option argument to matplotlib.pyplot.axis in order to control axis properties (e.g. using 'equal' make circles shown circular in the plot). This might be useful for preserving geometric relationships (e.g. orthogonality) in the generated plot. See matplotlib.pyplot.axis documentation for possible values and their description. None plot_3d ( self , x_axis = 0 , y_axis = 1 , z_axis = 2 , x_label = None , y_label = None , z_label = None , title = None , color = None , axis_metric = None , annot = True ) \u00b6 Show source code in whatlies/embeddingset.py 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 def plot_3d ( self , x_axis : Union [ int , str , Embedding ] = 0 , y_axis : Union [ int , str , Embedding ] = 1 , z_axis : Union [ int , str , Embedding ] = 2 , x_label : Optional [ str ] = None , y_label : Optional [ str ] = None , z_label : Optional [ str ] = None , title : Optional [ str ] = None , color : str = None , axis_metric : Optional [ Union [ str , Callable , Sequence ]] = None , annot : bool = True , ): \"\"\" Creates a 3d visualisation of the embedding. Arguments: x_axis: the x-axis to be used, must be given when dim > 3; if an integer, the corresponding dimension of embedding is used. y_axis: the y-axis to be used, must be given when dim > 3; if an integer, the corresponding dimension of embedding is used. z_axis: the z-axis to be used, must be given when dim > 3; if an integer, the corresponding dimension of embedding is used. x_label: an optional label used for x-axis; if not given, it is set based on value of `x_axis`. y_label: an optional label used for y-axis; if not given, it is set based on value of `y_axis`. z_label: an optional label used for z-axis; if not given, it is set based on value of `z_axis`. title: an optional title for the plot. color: the property to user for the color axis_metric: the metric used to project each embedding on the axes; only used when the corresponding axis is a string or an `Embedding` instance. It could be a string (`'cosine_similarity'`, `'cosine_distance'` or `'euclidean'`), or a callable that takes two vectors as input and returns a scalar value as output. To set different metrics of the three different axes, you can pass a list/tuple of size three that describes the metrics you're interested in. By default (`None`), normalized scalar projection (i.e. `>` operator) is used. annot: drawn points should be annotated **Usage** ```python from whatlies.language import SpacyLanguage from whatlies.transformers import Pca words = [\"prince\", \"princess\", \"nurse\", \"doctor\", \"banker\", \"man\", \"woman\", \"cousin\", \"neice\", \"king\", \"queen\", \"dude\", \"guy\", \"gal\", \"fire\", \"dog\", \"cat\", \"mouse\", \"red\", \"blue\", \"green\", \"yellow\", \"water\", \"person\", \"family\", \"brother\", \"sister\"] lang = SpacyLanguage(\"en_core_web_sm\") emb = lang[words] emb.transform(Pca(3)).plot_3d(annot=True) emb.transform(Pca(3)).plot_3d(\"king\", \"dog\", \"red\") emb.transform(Pca(3)).plot_3d(\"king\", \"dog\", \"red\", axis_metric=\"cosine_distance\") ``` \"\"\" if isinstance ( x_axis , str ): x_axis = self [ x_axis ] if isinstance ( y_axis , str ): y_axis = self [ y_axis ] if isinstance ( z_axis , str ): z_axis = self [ z_axis ] if isinstance ( axis_metric , ( list , tuple )): x_axis_metric = axis_metric [ 0 ] y_axis_metric = axis_metric [ 1 ] z_axis_metric = axis_metric [ 2 ] else : x_axis_metric = axis_metric y_axis_metric = axis_metric z_axis_metric = axis_metric # Determine axes values and labels if isinstance ( x_axis , int ): x_val = self . to_X ()[:, x_axis ] x_lab = \"Dimension \" + str ( x_axis ) else : x_axis_metric = Embedding . _get_plot_axis_metric_callable ( x_axis_metric ) x_val = self . compare_against ( x_axis , mapping = x_axis_metric ) x_lab = x_axis . name x_lab = x_label if x_label is not None else x_lab if isinstance ( y_axis , int ): y_val = self . to_X ()[:, y_axis ] y_lab = \"Dimension \" + str ( y_axis ) else : y_axis_metric = Embedding . _get_plot_axis_metric_callable ( y_axis_metric ) y_val = self . compare_against ( y_axis , mapping = y_axis_metric ) y_lab = y_axis . name y_lab = y_label if y_label is not None else y_lab if isinstance ( z_axis , int ): z_val = self . to_X ()[:, z_axis ] z_lab = \"Dimension \" + str ( z_axis ) else : z_axis_metric = Embedding . _get_plot_axis_metric_callable ( z_axis_metric ) z_val = self . compare_against ( z_axis , mapping = z_axis_metric ) z_lab = z_axis . name z_lab = z_label if z_label is not None else z_lab # Save relevant information in a dataframe for plotting later. plot_df = pd . DataFrame ( { \"x_axis\" : x_val , \"y_axis\" : y_val , \"z_axis\" : z_val , \"name\" : [ v . name for v in self . embeddings . values ()], \"original\" : [ v . orig for v in self . embeddings . values ()], } ) # Deal with the colors of the dots. if color : plot_df [ \"color\" ] = [ getattr ( v , color ) if hasattr ( v , color ) else \"\" for v in self . embeddings . values () ] color_map = { k : v for v , k in enumerate ( set ( plot_df [ \"color\" ]))} color_val = [ color_map [ k ] if not isinstance ( k , float ) else k for k in plot_df [ \"color\" ] ] else : color_val = None ax = plt . axes ( projection = \"3d\" ) ax . scatter3D ( plot_df [ \"x_axis\" ], plot_df [ \"y_axis\" ], plot_df [ \"z_axis\" ], c = color_val , s = 25 ) # Set the labels, titles, text annotations. ax . set_xlabel ( x_lab ) ax . set_ylabel ( y_lab ) ax . set_zlabel ( z_lab ) if annot : for i , row in plot_df . iterrows (): ax . text ( row [ \"x_axis\" ], row [ \"y_axis\" ], row [ \"z_axis\" ] + 0.05 , row [ \"original\" ] ) if title : ax . set_title ( label = title ) return ax Creates a 3d visualisation of the embedding. Parameters Name Type Description Default x_axis Union[int, str, whatlies.embedding.Embedding] the x-axis to be used, must be given when dim > 3; if an integer, the corresponding dimension of embedding is used. 0 y_axis Union[int, str, whatlies.embedding.Embedding] the y-axis to be used, must be given when dim > 3; if an integer, the corresponding dimension of embedding is used. 1 z_axis Union[int, str, whatlies.embedding.Embedding] the z-axis to be used, must be given when dim > 3; if an integer, the corresponding dimension of embedding is used. 2 x_label Optional[str] an optional label used for x-axis; if not given, it is set based on value of x_axis . None y_label Optional[str] an optional label used for y-axis; if not given, it is set based on value of y_axis . None z_label Optional[str] an optional label used for z-axis; if not given, it is set based on value of z_axis . None title Optional[str] an optional title for the plot. None color str the property to user for the color None axis_metric Optional[Union[str, Callable, Sequence]] the metric used to project each embedding on the axes; only used when the corresponding axis is a string or an Embedding instance. It could be a string ( 'cosine_similarity' , 'cosine_distance' or 'euclidean' ), or a callable that takes two vectors as input and returns a scalar value as output. To set different metrics of the three different axes, you can pass a list/tuple of size three that describes the metrics you're interested in. By default ( None ), normalized scalar projection (i.e. > operator) is used. None annot bool drawn points should be annotated True Usage from whatlies.language import SpacyLanguage from whatlies.transformers import Pca words = [ \"prince\" , \"princess\" , \"nurse\" , \"doctor\" , \"banker\" , \"man\" , \"woman\" , \"cousin\" , \"neice\" , \"king\" , \"queen\" , \"dude\" , \"guy\" , \"gal\" , \"fire\" , \"dog\" , \"cat\" , \"mouse\" , \"red\" , \"blue\" , \"green\" , \"yellow\" , \"water\" , \"person\" , \"family\" , \"brother\" , \"sister\" ] lang = SpacyLanguage ( \"en_core_web_sm\" ) emb = lang [ words ] emb . transform ( Pca ( 3 )) . plot_3d ( annot = True ) emb . transform ( Pca ( 3 )) . plot_3d ( \"king\" , \"dog\" , \"red\" ) emb . transform ( Pca ( 3 )) . plot_3d ( \"king\" , \"dog\" , \"red\" , axis_metric = \"cosine_distance\" ) plot_brush ( self , x_axis = 0 , y_axis = 1 , axis_metric = None , x_label = None , y_label = None , title = None , annot = False , color = None , n_show = 15 , interactive = False ) \u00b6 Show source code in whatlies/embeddingset.py 1198 1199 1200 1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213 1214 1215 1216 1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 1233 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248 1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291 1292 1293 1294 1295 1296 1297 1298 1299 1300 1301 1302 1303 1304 1305 1306 1307 1308 1309 1310 1311 1312 1313 1314 1315 1316 1317 1318 1319 1320 1321 1322 1323 1324 1325 1326 1327 1328 1329 1330 1331 1332 1333 1334 1335 1336 1337 1338 1339 1340 1341 1342 1343 1344 1345 1346 1347 1348 1349 1350 1351 1352 def plot_brush ( self , x_axis : Union [ int , str , Embedding ] = 0 , y_axis : Union [ int , str , Embedding ] = 1 , axis_metric : Optional [ Union [ str , Callable , Sequence ]] = None , x_label : Optional [ str ] = None , y_label : Optional [ str ] = None , title : Optional [ str ] = None , annot : bool = False , color : Union [ None , str ] = None , n_show : int = 15 , interactive : bool = False , ): \"\"\" Makes an interactive plot with a brush element. Arguments: x_axis: the x-axis to be used, must be given when dim > 2; if an integer, the corresponding dimension of embedding is used. y_axis: the y-axis to be used, must be given when dim > 2; if an integer, the corresponding dimension of embedding is used. axis_metric: the metric used to project each embedding on the axes; only used when the corresponding axis (i.e. `x_axis` or `y_axis`) is a string or an `Embedding` instance. It could be a string (`'cosine_similarity'`, `'cosine_distance'` or `'euclidean'`), or a callable that takes two vectors as input and returns a scalar value as output. To set different metrics for x- and y-axis, a list or a tuple of two elements could be given. By default (`None`), normalized scalar projection (i.e. `>` operator) is used. x_label: an optional label used for x-axis; if not given, it is set based on `x_axis` value. y_label: an optional label used for y-axis; if not given, it is set based on `y_axis` value. title: an optional title for the plot; if not given, it is set based on `x_axis` and `y_axis` values. annot: drawn points should be annotated color: a property that will be used for plotting n_show: number of points to show in text selection interactive: turn on/off the zoom/panning feature; if turned on, zoom/pan can be triggered when shift key is held **Usage** ```python from whatlies.language import SpacyLanguage from whatlies.transformers import Pca words = [\"prince\", \"princess\", \"nurse\", \"doctor\", \"banker\", \"man\", \"woman\", \"cousin\", \"neice\", \"king\", \"queen\", \"dude\", \"guy\", \"gal\", \"fire\", \"dog\", \"cat\", \"mouse\", \"red\", \"blue\", \"green\", \"yellow\", \"water\", \"person\", \"family\", \"brother\", \"sister\"] lang = SpacyLanguage(\"en_core_web_sm\") emb = lang[words].transform(Pca(2)) emb.plot_brush() ``` \"\"\" if isinstance ( x_axis , str ): x_axis = self [ x_axis ] if isinstance ( y_axis , str ): y_axis = self [ y_axis ] if isinstance ( axis_metric , ( list , tuple )): x_axis_metric = axis_metric [ 0 ] y_axis_metric = axis_metric [ 1 ] else : x_axis_metric = axis_metric y_axis_metric = axis_metric # Determine axes values and labels if isinstance ( x_axis , int ): x_val = self . to_X ()[:, x_axis ] x_lab = \"Dimension \" + str ( x_axis ) else : x_axis_metric = Embedding . _get_plot_axis_metric_callable ( x_axis_metric ) x_val = self . compare_against ( x_axis , mapping = x_axis_metric ) x_lab = x_axis . name if isinstance ( y_axis , int ): y_val = self . to_X ()[:, y_axis ] y_lab = \"Dimension \" + str ( y_axis ) else : y_axis_metric = Embedding . _get_plot_axis_metric_callable ( y_axis_metric ) y_val = self . compare_against ( y_axis , mapping = y_axis_metric ) y_lab = y_axis . name x_label = x_label if x_label is not None else x_lab y_label = y_label if y_label is not None else y_lab title = title if title is not None else \"Click and Drag Here\" plot_df = pd . DataFrame ( { \"x_axis\" : x_val , \"y_axis\" : y_val , \"name\" : [ v . name for v in self . embeddings . values ()], \"original\" : [ v . orig for v in self . embeddings . values ()], } ) if color : plot_df [ color ] = [ getattr ( v , color ) if hasattr ( v , color ) else \"\" for v in self . embeddings . values () ] result = ( alt . Chart ( plot_df ) . mark_circle ( size = 60 ) . encode ( x = alt . X ( \"x_axis\" , axis = alt . Axis ( title = x_label )), y = alt . X ( \"y_axis\" , axis = alt . Axis ( title = y_label )), tooltip = [ \"name\" , \"original\" ], color = alt . Color ( \":N\" , legend = None ) if not color else alt . Color ( color ), ) . properties ( title = title ) ) if annot : text = ( alt . Chart ( plot_df ) . mark_text ( dx =- 15 , dy = 3 , color = \"black\" ) . encode ( x = \"x_axis\" , y = \"y_axis\" , text = \"original\" , ) ) result = result + text brush = alt . selection_interval ( on = \"[mousedown[!event.shiftKey], mouseup] > mousemove\" , translate = \"[mousedown[!event.shiftKey], mouseup] > mousemove!\" , ) ranked_text = ( alt . Chart ( plot_df ) . mark_text () . encode ( y = alt . Y ( \"row_number:O\" , axis = None ), color = alt . Color ( \":N\" , legend = None ) if not color else alt . Color ( color ), ) . transform_window ( row_number = \"row_number()\" ) . transform_filter ( brush ) . transform_window ( rank = \"rank(row_number)\" ) . transform_filter ( alt . datum . rank < n_show ) ) text_plt = ranked_text . encode ( text = \"original:N\" ) . properties ( width = 250 , title = \"Text Selection\" ) if interactive : zoom = alt . selection_interval ( bind = \"scales\" , on = \"[mousedown[event.shiftKey], mouseup] > mousemove\" , translate = \"[mousedown[event.shiftKey], mouseup] > mousemove!\" , ) result = result . add_selection ( zoom , brush ) else : result = result . add_selection ( brush ) return result | text_plt Makes an interactive plot with a brush element. Parameters Name Type Description Default x_axis Union[int, str, whatlies.embedding.Embedding] the x-axis to be used, must be given when dim > 2; if an integer, the corresponding dimension of embedding is used. 0 y_axis Union[int, str, whatlies.embedding.Embedding] the y-axis to be used, must be given when dim > 2; if an integer, the corresponding dimension of embedding is used. 1 axis_metric Optional[Union[str, Callable, Sequence]] the metric used to project each embedding on the axes; only used when the corresponding axis (i.e. x_axis or y_axis ) is a string or an Embedding instance. It could be a string ( 'cosine_similarity' , 'cosine_distance' or 'euclidean' ), or a callable that takes two vectors as input and returns a scalar value as output. To set different metrics for x- and y-axis, a list or a tuple of two elements could be given. By default ( None ), normalized scalar projection (i.e. > operator) is used. None x_label Optional[str] an optional label used for x-axis; if not given, it is set based on x_axis value. None y_label Optional[str] an optional label used for y-axis; if not given, it is set based on y_axis value. None title Optional[str] an optional title for the plot; if not given, it is set based on x_axis and y_axis values. None annot bool drawn points should be annotated False color Union[NoneType, str] a property that will be used for plotting None n_show int number of points to show in text selection 15 interactive bool turn on/off the zoom/panning feature; if turned on, zoom/pan can be triggered when shift key is held False Usage from whatlies.language import SpacyLanguage from whatlies.transformers import Pca words = [ \"prince\" , \"princess\" , \"nurse\" , \"doctor\" , \"banker\" , \"man\" , \"woman\" , \"cousin\" , \"neice\" , \"king\" , \"queen\" , \"dude\" , \"guy\" , \"gal\" , \"fire\" , \"dog\" , \"cat\" , \"mouse\" , \"red\" , \"blue\" , \"green\" , \"yellow\" , \"water\" , \"person\" , \"family\" , \"brother\" , \"sister\" ] lang = SpacyLanguage ( \"en_core_web_sm\" ) emb = lang [ words ] . transform ( Pca ( 2 )) emb . plot_brush () plot_distance ( self , metric = 'cosine' , norm = False ) \u00b6 Show source code in whatlies/embeddingset.py 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 def plot_distance ( self , metric = \"cosine\" , norm = False ): \"\"\" Make a distance plot. Shows you the distance between all the word embeddings in the set. Arguments: metric: `'cosine'`, `'correlation'` or `'euclidean'` norm: normalise the vectors before calculating the distances Usage: ```python from whatlies.language import SpacyLanguage lang = SpacyLanguage(\"en_core_web_sm\") names = ['red', 'blue', 'green', 'yellow', 'cat', 'dog', 'mouse', 'rat', 'bike', 'car'] emb = lang[names] emb.plot_distance(metric='cosine') emb.plot_distance(metric='euclidean') emb.plot_distance(metric='correlation') ``` \"\"\" allowed_metrics = [ \"cosine\" , \"correlation\" , \"euclidean\" ] if metric not in allowed_metrics : raise ValueError ( f \"The `metric` argument must be in { allowed_metrics } , got: { metric } .\" ) vmin , vmax = 0 , 1 X = self . to_X ( norm = norm ) if metric == \"cosine\" : distances = cosine_distances ( X ) if metric == \"correlation\" : distances = 1 - np . corrcoef ( X ) vmin , vmax = - 1 , 1 if metric == \"euclidean\" : distances = euclidean_distances ( X ) vmin , vmax = 0 , np . max ( distances ) fig , ax = plt . subplots () plt . imshow ( distances , cmap = plt . cm . get_cmap () . reversed (), vmin = vmin , vmax = vmax ) plt . xticks ( range ( len ( self )), self . embeddings . keys ()) plt . yticks ( range ( len ( self )), self . embeddings . keys ()) plt . colorbar () # Rotate the tick labels and set their alignment. plt . setp ( ax . get_xticklabels (), rotation = 90 , ha = \"right\" , rotation_mode = \"anchor\" ) Make a distance plot. Shows you the distance between all the word embeddings in the set. Parameters Name Type Description Default metric 'cosine' , 'correlation' or 'euclidean' 'cosine' norm normalise the vectors before calculating the distances False Usage: from whatlies.language import SpacyLanguage lang = SpacyLanguage ( \"en_core_web_sm\" ) names = [ 'red' , 'blue' , 'green' , 'yellow' , 'cat' , 'dog' , 'mouse' , 'rat' , 'bike' , 'car' ] emb = lang [ names ] emb . plot_distance ( metric = 'cosine' ) emb . plot_distance ( metric = 'euclidean' ) emb . plot_distance ( metric = 'correlation' ) plot_interactive ( self , x_axis = 0 , y_axis = 1 , axis_metric = None , x_label = None , y_label = None , title = None , annot = True , color = None , interactive = True ) \u00b6 Show source code in whatlies/embeddingset.py 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 def plot_interactive ( self , x_axis : Union [ int , str , Embedding ] = 0 , y_axis : Union [ int , str , Embedding ] = 1 , axis_metric : Optional [ Union [ str , Callable , Sequence ]] = None , x_label : Optional [ str ] = None , y_label : Optional [ str ] = None , title : Optional [ str ] = None , annot : bool = True , color : Union [ None , str ] = None , interactive : bool = True , ): \"\"\" Makes highly interactive plot of the set of embeddings. Arguments: x_axis: the x-axis to be used, must be given when dim > 2; if an integer, the corresponding dimension of embedding is used. y_axis: the y-axis to be used, must be given when dim > 2; if an integer, the corresponding dimension of embedding is used. axis_metric: the metric used to project each embedding on the axes; only used when the corresponding axis (i.e. `x_axis` or `y_axis`) is a string or an `Embedding` instance. It could be a string (`'cosine_similarity'`, `'cosine_distance'` or `'euclidean'`), or a callable that takes two vectors as input and returns a scalar value as output. To set different metrics for x- and y-axis, a list or a tuple of two elements could be given. By default (`None`), normalized scalar projection (i.e. `>` operator) is used. x_label: an optional label used for x-axis; if not given, it is set based on `x_axis` value. y_label: an optional label used for y-axis; if not given, it is set based on `y_axis` value. title: an optional title for the plot; if not given, it is set based on `x_axis` and `y_axis` values. annot: drawn points should be annotated color: a property that will be used for plotting interactive: turn on/off the zoom/panning feature **Usage** ```python from whatlies.language import SpacyLanguage words = [\"prince\", \"princess\", \"nurse\", \"doctor\", \"banker\", \"man\", \"woman\", \"cousin\", \"neice\", \"king\", \"queen\", \"dude\", \"guy\", \"gal\", \"fire\", \"dog\", \"cat\", \"mouse\", \"red\", \"blue\", \"green\", \"yellow\", \"water\", \"person\", \"family\", \"brother\", \"sister\"] lang = SpacyLanguage(\"en_core_web_sm\") emb = lang[words] emb.plot_interactive('man', 'woman') ``` \"\"\" if isinstance ( x_axis , str ): x_axis = self [ x_axis ] if isinstance ( y_axis , str ): y_axis = self [ y_axis ] if isinstance ( axis_metric , ( list , tuple )): x_axis_metric = axis_metric [ 0 ] y_axis_metric = axis_metric [ 1 ] else : x_axis_metric = axis_metric y_axis_metric = axis_metric # Determine axes values and labels if isinstance ( x_axis , int ): x_val = self . to_X ()[:, x_axis ] x_lab = \"Dimension \" + str ( x_axis ) else : x_axis_metric = Embedding . _get_plot_axis_metric_callable ( x_axis_metric ) x_val = self . compare_against ( x_axis , mapping = x_axis_metric ) x_lab = x_axis . name if isinstance ( y_axis , int ): y_val = self . to_X ()[:, y_axis ] y_lab = \"Dimension \" + str ( y_axis ) else : y_axis_metric = Embedding . _get_plot_axis_metric_callable ( y_axis_metric ) y_val = self . compare_against ( y_axis , mapping = y_axis_metric ) y_lab = y_axis . name x_label = x_label if x_label is not None else x_lab y_label = y_label if y_label is not None else y_lab title = title if title is not None else f \" { x_lab } vs. { y_lab } \" plot_df = pd . DataFrame ( { \"x_axis\" : x_val , \"y_axis\" : y_val , \"name\" : [ v . name for v in self . embeddings . values ()], \"original\" : [ v . orig for v in self . embeddings . values ()], } ) if color : plot_df [ color ] = [ getattr ( v , color ) if hasattr ( v , color ) else \"\" for v in self . embeddings . values () ] result = ( alt . Chart ( plot_df ) . mark_circle ( size = 60 ) . encode ( x = alt . X ( \"x_axis\" , axis = alt . Axis ( title = x_label )), y = alt . X ( \"y_axis\" , axis = alt . Axis ( title = y_label )), tooltip = [ \"name\" , \"original\" ], color = alt . Color ( \":N\" , legend = None ) if not color else alt . Color ( color ), ) . properties ( title = title ) ) if interactive : result = result . interactive () if annot : text = ( alt . Chart ( plot_df ) . mark_text ( dx =- 15 , dy = 3 , color = \"black\" ) . encode ( x = \"x_axis\" , y = \"y_axis\" , text = \"original\" , ) ) result = result + text return result Makes highly interactive plot of the set of embeddings. Parameters Name Type Description Default x_axis Union[int, str, whatlies.embedding.Embedding] the x-axis to be used, must be given when dim > 2; if an integer, the corresponding dimension of embedding is used. 0 y_axis Union[int, str, whatlies.embedding.Embedding] the y-axis to be used, must be given when dim > 2; if an integer, the corresponding dimension of embedding is used. 1 axis_metric Optional[Union[str, Callable, Sequence]] the metric used to project each embedding on the axes; only used when the corresponding axis (i.e. x_axis or y_axis ) is a string or an Embedding instance. It could be a string ( 'cosine_similarity' , 'cosine_distance' or 'euclidean' ), or a callable that takes two vectors as input and returns a scalar value as output. To set different metrics for x- and y-axis, a list or a tuple of two elements could be given. By default ( None ), normalized scalar projection (i.e. > operator) is used. None x_label Optional[str] an optional label used for x-axis; if not given, it is set based on x_axis value. None y_label Optional[str] an optional label used for y-axis; if not given, it is set based on y_axis value. None title Optional[str] an optional title for the plot; if not given, it is set based on x_axis and y_axis values. None annot bool drawn points should be annotated True color Union[NoneType, str] a property that will be used for plotting None interactive bool turn on/off the zoom/panning feature True Usage from whatlies.language import SpacyLanguage words = [ \"prince\" , \"princess\" , \"nurse\" , \"doctor\" , \"banker\" , \"man\" , \"woman\" , \"cousin\" , \"neice\" , \"king\" , \"queen\" , \"dude\" , \"guy\" , \"gal\" , \"fire\" , \"dog\" , \"cat\" , \"mouse\" , \"red\" , \"blue\" , \"green\" , \"yellow\" , \"water\" , \"person\" , \"family\" , \"brother\" , \"sister\" ] lang = SpacyLanguage ( \"en_core_web_sm\" ) emb = lang [ words ] emb . plot_interactive ( 'man' , 'woman' ) plot_interactive_matrix ( self , * axes , axes_metric = None , annot = True , width = 200 , height = 200 ) \u00b6 Show source code in whatlies/embeddingset.py 1354 1355 1356 1357 1358 1359 1360 1361 1362 1363 1364 1365 1366 1367 1368 1369 1370 1371 1372 1373 1374 1375 1376 1377 1378 1379 1380 1381 1382 1383 1384 1385 1386 1387 1388 1389 1390 1391 1392 1393 1394 1395 1396 1397 1398 1399 1400 1401 1402 1403 1404 1405 1406 1407 1408 1409 1410 1411 1412 1413 1414 1415 1416 1417 1418 1419 1420 1421 1422 1423 1424 1425 1426 1427 1428 1429 1430 1431 1432 1433 1434 1435 1436 1437 1438 1439 1440 1441 1442 1443 1444 1445 def plot_interactive_matrix ( self , * axes : Union [ int , str , Embedding ], axes_metric : Optional [ Union [ str , Callable , Sequence ]] = None , annot : bool = True , width : int = 200 , height : int = 200 , ): \"\"\" Makes highly interactive plot of the set of embeddings. Arguments: axes: the axes that we wish to plot; each could be either an integer, the name of an existing embedding, or an `Embedding` instance (default: `0, 1`). axes_metric: the metric used to project each embedding on the axes; only used when the corresponding axis is a string or an `Embedding` instance. It could be a string (`'cosine_similarity'`, `'cosine_distance'` or `'euclidean'`), or a callable that takes two vectors as input and returns a scalar value as output. To set different metrics for different axes, a list or a tuple of the same length as `axes` could be given. By default (`None`), normalized scalar projection (i.e. `>` operator) is used. annot: drawn points should be annotated width: width of the visual height: height of the visual **Usage** ```python from whatlies.language import SpacyLanguage from whatlies.transformers import Pca words = [\"prince\", \"princess\", \"nurse\", \"doctor\", \"banker\", \"man\", \"woman\", \"cousin\", \"neice\", \"king\", \"queen\", \"dude\", \"guy\", \"gal\", \"fire\", \"dog\", \"cat\", \"mouse\", \"red\", \"blue\", \"green\", \"yellow\", \"water\", \"person\", \"family\", \"brother\", \"sister\"] lang = SpacyLanguage(\"en_core_web_sm\") emb = lang[words] emb.transform(Pca(3)).plot_interactive_matrix(0, 1, 2) ``` \"\"\" # Set default value of axes, if not given. if len ( axes ) == 0 : axes = [ 0 , 1 ] if isinstance ( axes_metric , ( list , tuple )) and len ( axes_metric ) != len ( axes ): raise ValueError ( f \"The number of given axes metrics should be the same as the number of given axes. Got { len ( axes ) } axes vs. { len ( axes_metric ) } metrics.\" ) if not isinstance ( axes_metric , ( list , tuple )): axes_metric = [ axes_metric ] * len ( axes ) # Get values of each axis according to their type. axes_vals = {} X = self . to_X () for axis , metric in zip ( axes , axes_metric ): if isinstance ( axis , int ): vals = X [:, axis ] axes_vals [ \"Dimension \" + str ( axis )] = vals else : if isinstance ( axis , str ): axis = self [ axis ] metric = Embedding . _get_plot_axis_metric_callable ( metric ) vals = self . compare_against ( axis , mapping = metric ) axes_vals [ axis . name ] = vals plot_df = pd . DataFrame ( axes_vals ) plot_df [ \"name\" ] = [ v . name for v in self . embeddings . values ()] plot_df [ \"original\" ] = [ v . orig for v in self . embeddings . values ()] axes_names = list ( axes_vals . keys ()) result = ( alt . Chart ( plot_df ) . mark_circle () . encode ( x = alt . X ( alt . repeat ( \"column\" ), type = \"quantitative\" ), y = alt . Y ( alt . repeat ( \"row\" ), type = \"quantitative\" ), tooltip = [ \"name\" , \"original\" ], ) ) if annot : text_stuff = result . mark_text ( dx =- 15 , dy = 3 , color = \"black\" ) . encode ( text = \"original\" , ) result = result + text_stuff result = ( result . properties ( width = width , height = height ) . repeat ( row = axes_names [:: - 1 ], column = axes_names ) . interactive () ) return result Makes highly interactive plot of the set of embeddings. Parameters Name Type Description Default *axes Union[int, str, whatlies.embedding.Embedding] the axes that we wish to plot; each could be either an integer, the name of an existing embedding, or an Embedding instance (default: 0, 1 ). () axes_metric Optional[Union[str, Callable, Sequence]] the metric used to project each embedding on the axes; only used when the corresponding axis is a string or an Embedding instance. It could be a string ( 'cosine_similarity' , 'cosine_distance' or 'euclidean' ), or a callable that takes two vectors as input and returns a scalar value as output. To set different metrics for different axes, a list or a tuple of the same length as axes could be given. By default ( None ), normalized scalar projection (i.e. > operator) is used. None annot bool drawn points should be annotated True width int width of the visual 200 height int height of the visual 200 Usage from whatlies.language import SpacyLanguage from whatlies.transformers import Pca words = [ \"prince\" , \"princess\" , \"nurse\" , \"doctor\" , \"banker\" , \"man\" , \"woman\" , \"cousin\" , \"neice\" , \"king\" , \"queen\" , \"dude\" , \"guy\" , \"gal\" , \"fire\" , \"dog\" , \"cat\" , \"mouse\" , \"red\" , \"blue\" , \"green\" , \"yellow\" , \"water\" , \"person\" , \"family\" , \"brother\" , \"sister\" ] lang = SpacyLanguage ( \"en_core_web_sm\" ) emb = lang [ words ] emb . transform ( Pca ( 3 )) . plot_interactive_matrix ( 0 , 1 , 2 ) plot_movement ( self , other , x_axis , y_axis , first_group_name = 'before' , second_group_name = 'after' , annot = True ) \u00b6 Show source code in whatlies/embeddingset.py 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 def plot_movement ( self , other , x_axis : Union [ str , Embedding ], y_axis : Union [ str , Embedding ], first_group_name = \"before\" , second_group_name = \"after\" , annot : bool = True , ): \"\"\" Makes highly interactive plot of the movement of embeddings between two sets of embeddings. Arguments: other: the other embeddingset x_axis: the x-axis to be used, must be given when dim > 2 y_axis: the y-axis to be used, must be given when dim > 2 first_group_name: the name to give to the first set of embeddings (default: \"before\") second_group_name: the name to give to the second set of embeddings (default: \"after\") annot: drawn points should be annotated **Usage** ```python from whatlies.language import SpacyLanguage words = [\"prince\", \"princess\", \"nurse\", \"doctor\", \"banker\", \"man\", \"woman\", \"cousin\", \"neice\", \"king\", \"queen\", \"dude\", \"guy\", \"gal\", \"fire\", \"dog\", \"cat\", \"mouse\", \"red\", \"blue\", \"green\", \"yellow\", \"water\", \"person\", \"family\", \"brother\", \"sister\"] lang = SpacyLanguage(\"en_core_web_sm\") emb = lang[words] emb_new = emb - emb['king'] emb.plot_movement(emb_new, 'man', 'woman') ``` \"\"\" if isinstance ( x_axis , str ): x_axis = self [ x_axis ] if isinstance ( y_axis , str ): y_axis = self [ y_axis ] df1 = ( self . to_axis_df ( x_axis , y_axis ) . set_index ( \"original\" ) . drop ( columns = [ \"name\" ]) ) df2 = ( other . to_axis_df ( x_axis , y_axis ) . set_index ( \"original\" ) . drop ( columns = [ \"name\" ]) . loc [ lambda d : d . index . isin ( df1 . index )] ) df_draw = ( pd . concat ([ df1 , df2 ]) . reset_index () . sort_values ([ \"original\" ]) . assign ( constant = 1 ) ) plots = [] for idx , grp_df in df_draw . groupby ( \"original\" ): _ = ( alt . Chart ( grp_df ) . mark_line ( color = \"gray\" , strokeDash = [ 2 , 1 ]) . encode ( x = \"x_axis:Q\" , y = \"y_axis:Q\" ) ) plots . append ( _ ) p0 = reduce ( lambda x , y : x + y , plots ) p1 = ( deepcopy ( self ) . add_property ( \"group\" , lambda d : first_group_name ) . plot_interactive ( x_axis , y_axis , annot = annot , color = \"group\" ) ) p2 = ( deepcopy ( other ) . add_property ( \"group\" , lambda d : second_group_name ) . plot_interactive ( x_axis , y_axis , annot = annot , color = \"group\" ) ) return p0 + p1 + p2 Makes highly interactive plot of the movement of embeddings between two sets of embeddings. Parameters Name Type Description Default other the other embeddingset required x_axis Union[str, whatlies.embedding.Embedding] the x-axis to be used, must be given when dim > 2 required y_axis Union[str, whatlies.embedding.Embedding] the y-axis to be used, must be given when dim > 2 required first_group_name the name to give to the first set of embeddings (default: \"before\") 'before' second_group_name the name to give to the second set of embeddings (default: \"after\") 'after' annot bool drawn points should be annotated True Usage from whatlies.language import SpacyLanguage words = [ \"prince\" , \"princess\" , \"nurse\" , \"doctor\" , \"banker\" , \"man\" , \"woman\" , \"cousin\" , \"neice\" , \"king\" , \"queen\" , \"dude\" , \"guy\" , \"gal\" , \"fire\" , \"dog\" , \"cat\" , \"mouse\" , \"red\" , \"blue\" , \"green\" , \"yellow\" , \"water\" , \"person\" , \"family\" , \"brother\" , \"sister\" ] lang = SpacyLanguage ( \"en_core_web_sm\" ) emb = lang [ words ] emb_new = emb - emb [ 'king' ] emb . plot_movement ( emb_new , 'man' , 'woman' ) plot_pixels ( self ) \u00b6 Show source code in whatlies/embeddingset.py 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 def plot_pixels ( self ): \"\"\" Makes a pixelchart of every embedding in the set. Usage: ```python from whatlies.language import SpacyLanguage from whatlies.transformers import Pca lang = SpacyLanguage(\"en_core_web_sm\") names = ['red', 'blue', 'green', 'yellow', 'cat', 'dog', 'mouse', 'rat', 'bike', 'car', 'motor', 'cycle', 'firehydrant', 'japan', 'germany', 'belgium'] emb = lang[names].transform(Pca(12)).filter(lambda e: 'pca' not in e.name) emb.plot_pixels() ``` ![](https://rasahq.github.io/whatlies/images/pixels.png) \"\"\" names = self . embeddings . keys () df = self . to_dataframe () plt . matshow ( df ) plt . yticks ( range ( len ( names )), names ) Makes a pixelchart of every embedding in the set. Usage: from whatlies.language import SpacyLanguage from whatlies.transformers import Pca lang = SpacyLanguage ( \"en_core_web_sm\" ) names = [ 'red' , 'blue' , 'green' , 'yellow' , 'cat' , 'dog' , 'mouse' , 'rat' , 'bike' , 'car' , 'motor' , 'cycle' , 'firehydrant' , 'japan' , 'germany' , 'belgium' ] emb = lang [ names ] . transform ( Pca ( 12 )) . filter ( lambda e : 'pca' not in e . name ) emb . plot_pixels () plot_similarity ( self , metric = 'cosine' , norm = False ) \u00b6 Show source code in whatlies/embeddingset.py 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 def plot_similarity ( self , metric = \"cosine\" , norm = False ): \"\"\" Make a similarity plot. Shows you the similarity between all the word embeddings in the set. Arguments: metric: `'cosine'` or `'correlation'` norm: normalise the embeddings before calculating the similarity Usage: ```python from whatlies.language import SpacyLanguage lang = SpacyLanguage(\"en_core_web_sm\") names = ['red', 'blue', 'green', 'yellow', 'cat', 'dog', 'mouse', 'rat', 'bike', 'car'] emb = lang[names] emb.plot_similarity() emb.plot_similarity(metric='correlation') ``` \"\"\" allowed_metrics = [ \"cosine\" , \"correlation\" ] if metric not in allowed_metrics : raise ValueError ( f \"The `metric` argument must be in { allowed_metrics } , got: { metric } .\" ) vmin , vmax = 0 , 1 X = self . to_X ( norm = norm ) if metric == \"cosine\" : similarity = cosine_similarity ( X ) if metric == \"correlation\" : similarity = np . corrcoef ( X ) vmin , vmax = - 1 , 1 fig , ax = plt . subplots () plt . imshow ( similarity , cmap = plt . cm . get_cmap (), vmin =- vmin , vmax = vmax ) plt . xticks ( range ( len ( self )), self . embeddings . keys ()) plt . yticks ( range ( len ( self )), self . embeddings . keys ()) plt . colorbar () # Rotate the tick labels and set their alignment. plt . setp ( ax . get_xticklabels (), rotation = 90 , ha = \"right\" , rotation_mode = \"anchor\" ) Make a similarity plot. Shows you the similarity between all the word embeddings in the set. Parameters Name Type Description Default metric 'cosine' or 'correlation' 'cosine' norm normalise the embeddings before calculating the similarity False Usage: from whatlies.language import SpacyLanguage lang = SpacyLanguage ( \"en_core_web_sm\" ) names = [ 'red' , 'blue' , 'green' , 'yellow' , 'cat' , 'dog' , 'mouse' , 'rat' , 'bike' , 'car' ] emb = lang [ names ] emb . plot_similarity () emb . plot_similarity ( metric = 'correlation' ) score_similar ( self , emb , n = 10 , metric = 'cosine' ) \u00b6 Show source code in whatlies/embeddingset.py 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 def score_similar ( self , emb : Union [ str , Embedding ], n : int = 10 , metric = \"cosine\" ): \"\"\" Retreive a list of (Embedding, score) tuples that are the most similar to the passed query. Arguments: emb: query to use n: the number of items you'd like to see returned metric: metric to use to calculate distance, must be scipy or sklearn compatible Returns: An list of ([Embedding][whatlies.embedding.Embedding], score) tuples. \"\"\" if n > len ( self ): raise ValueError ( f \"You cannot retreive (n= { n } ) more items than exist in the Embeddingset (len= { len ( self ) } )\" ) if isinstance ( emb , str ): if emb not in self . embeddings . keys (): raise ValueError ( f \"Embedding for ` { emb } ` does not exist in this EmbeddingSet\" ) emb = self [ emb ] vec = emb . vector queries = [ w for w in self . embeddings . keys ()] vector_matrix = self . to_X () distances = pairwise_distances ( vector_matrix , vec . reshape ( 1 , - 1 ), metric = metric ) by_similarity = sorted ( zip ( queries , distances ), key = lambda z : z [ 1 ]) return [( self [ q ], float ( d )) for q , d in by_similarity [: n ]] Retreive a list of (Embedding, score) tuples that are the most similar to the passed query. Parameters Name Type Description Default emb Union[str, whatlies.embedding.Embedding] query to use required n int the number of items you'd like to see returned 10 metric metric to use to calculate distance, must be scipy or sklearn compatible 'cosine' Returns Type Description `` An list of ( Embedding , score) tuples. to_dataframe ( self ) \u00b6 Show source code in whatlies/embeddingset.py 599 600 601 602 603 604 def to_dataframe ( self ): \"\"\" Turns the embeddingset into a pandas dataframe. \"\"\" mat = self . to_matrix () return pd . DataFrame ( mat , index = list ( self . embeddings . keys ())) Turns the embeddingset into a pandas dataframe. to_matrix ( self ) \u00b6 Show source code in whatlies/embeddingset.py 593 594 595 596 597 def to_matrix ( self ): \"\"\" Does exactly the same as `.to_X`. It takes the embedding vectors and turns it into a numpy array. \"\"\" return self . to_X () Does exactly the same as .to_X . It takes the embedding vectors and turns it into a numpy array. to_names_X ( self ) \u00b6 Show source code in whatlies/embeddingset.py 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 def to_names_X ( self ): \"\"\" Get the list of names as well as an array of vectors of all embeddings. Usage: ```python from whatlies.embedding import Embedding from whatlies.embeddingset import EmbeddingSet foo = Embedding(\"foo\", [0.1, 0.3]) bar = Embedding(\"bar\", [0.7, 0.2]) buz = Embedding(\"buz\", [0.1, 0.9]) emb = EmbeddingSet(foo, bar, buz) names, X = emb.to_names_X() ``` \"\"\" return list ( self . embeddings . keys ()), self . to_X () Get the list of names as well as an array of vectors of all embeddings. Usage: from whatlies.embedding import Embedding from whatlies.embeddingset import EmbeddingSet foo = Embedding ( \"foo\" , [ 0.1 , 0.3 ]) bar = Embedding ( \"bar\" , [ 0.7 , 0.2 ]) buz = Embedding ( \"buz\" , [ 0.1 , 0.9 ]) emb = EmbeddingSet ( foo , bar , buz ) names , X = emb . to_names_X () to_X ( self , norm = False ) \u00b6 Show source code in whatlies/embeddingset.py 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 def to_X ( self , norm = False ): \"\"\" Takes every vector in each embedding and turns it into a scikit-learn compatible `X` matrix. Usage: ```python from whatlies.embedding import Embedding from whatlies.embeddingset import EmbeddingSet foo = Embedding(\"foo\", [0.1, 0.3]) bar = Embedding(\"bar\", [0.7, 0.2]) buz = Embedding(\"buz\", [0.1, 0.9]) emb = EmbeddingSet(foo, bar, buz) X = emb.to_X() ``` \"\"\" X = np . array ([ i . vector for i in self . embeddings . values ()]) X = normalize ( X ) if norm else X return X Takes every vector in each embedding and turns it into a scikit-learn compatible X matrix. Usage: from whatlies.embedding import Embedding from whatlies.embeddingset import EmbeddingSet foo = Embedding ( \"foo\" , [ 0.1 , 0.3 ]) bar = Embedding ( \"bar\" , [ 0.7 , 0.2 ]) buz = Embedding ( \"buz\" , [ 0.1 , 0.9 ]) emb = EmbeddingSet ( foo , bar , buz ) X = emb . to_X () to_X_y ( self , y_label ) \u00b6 Show source code in whatlies/embeddingset.py 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 def to_X_y ( self , y_label ): \"\"\" Takes every vector in each embedding and turns it into a scikit-learn compatible `X` matrix. Also retreives an array with potential labels. Usage: ```python from whatlies.embedding import Embedding from whatlies.embeddingset import EmbeddingSet foo = Embedding(\"foo\", [0.1, 0.3]) bar = Embedding(\"bar\", [0.7, 0.2]) buz = Embedding(\"buz\", [0.1, 0.9]) bla = Embedding(\"bla\", [0.2, 0.8]) emb1 = EmbeddingSet(foo, bar).add_property(\"label\", lambda d: 'group-one') emb2 = EmbeddingSet(buz, bla).add_property(\"label\", lambda d: 'group-two') emb = emb1.merge(emb2) X, y = emb.to_X_y(y_label='label') ``` \"\"\" X = self . to_X () y = np . array ([ getattr ( e , y_label ) for e in self . embeddings . values ()]) return X , y Takes every vector in each embedding and turns it into a scikit-learn compatible X matrix. Also retreives an array with potential labels. Usage: from whatlies.embedding import Embedding from whatlies.embeddingset import EmbeddingSet foo = Embedding ( \"foo\" , [ 0.1 , 0.3 ]) bar = Embedding ( \"bar\" , [ 0.7 , 0.2 ]) buz = Embedding ( \"buz\" , [ 0.1 , 0.9 ]) bla = Embedding ( \"bla\" , [ 0.2 , 0.8 ]) emb1 = EmbeddingSet ( foo , bar ) . add_property ( \"label\" , lambda d : 'group-one' ) emb2 = EmbeddingSet ( buz , bla ) . add_property ( \"label\" , lambda d : 'group-two' ) emb = emb1 . merge ( emb2 ) X , y = emb . to_X_y ( y_label = 'label' ) transform ( self , transformer ) \u00b6 Show source code in whatlies/embeddingset.py 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 def transform ( self , transformer ): \"\"\" Applies a transformation on the entire set. Usage: ```python from whatlies.embeddingset import EmbeddingSet from whatlies.transformers import Pca foo = Embedding(\"foo\", [0.1, 0.3, 0.10]) bar = Embedding(\"bar\", [0.7, 0.2, 0.11]) buz = Embedding(\"buz\", [0.1, 0.9, 0.12]) emb = EmbeddingSet(foo, bar, buz).transform(Pca(2)) ``` \"\"\" return transformer ( self ) Applies a transformation on the entire set. Usage: from whatlies.embeddingset import EmbeddingSet from whatlies.transformers import Pca foo = Embedding ( \"foo\" , [ 0.1 , 0.3 , 0.10 ]) bar = Embedding ( \"bar\" , [ 0.7 , 0.2 , 0.11 ]) buz = Embedding ( \"buz\" , [ 0.1 , 0.9 , 0.12 ]) emb = EmbeddingSet ( foo , bar , buz ) . transform ( Pca ( 2 ))","title":"EmbeddingSet"},{"location":"api/embeddingset/#whatliesembeddingsetembeddingset","text":"This object represents a set of Embedding s. You can use the same operations as an Embedding but here we apply it to the entire set instead of a single Embedding . Parameters embeddings : list of Embedding , or a single dictionary containing name: Embedding pairs name : custom name of embeddingset Usage: from whatlies.embedding import Embedding from whatlies.embeddingset import EmbeddingSet foo = Embedding ( \"foo\" , [ 0.1 , 0.3 ]) bar = Embedding ( \"bar\" , [ 0.7 , 0.2 ]) emb = EmbeddingSet ( foo , bar ) emb = EmbeddingSet ({ 'foo' : foo , 'bar' : bar )","title":"whatlies.embeddingset.EmbeddingSet"},{"location":"api/embeddingset/#whatlies.embeddingset.EmbeddingSet.ndim","text":"Return dimension of embedding vectors in embeddingset.","title":"ndim"},{"location":"api/embeddingset/#whatlies.embeddingset.EmbeddingSet.add_property","text":"Show source code in whatlies/embeddingset.py 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 def add_property ( self , name , func ): \"\"\" Adds a property to every embedding in the set. Very useful for plotting because a property can be used to assign colors. Arguments: name: name of the property to add func: function that receives an embedding and needs to output the property value Usage: ```python from whatlies.embeddingset import EmbeddingSet foo = Embedding(\"foo\", [0.1, 0.3, 0.10]) bar = Embedding(\"bar\", [0.7, 0.2, 0.11]) emb = EmbeddingSet(foo, bar) emb_with_property = emb.add_property('example', lambda d: 'group-one') ``` \"\"\" return EmbeddingSet ( { k : e . add_property ( name , func ) for k , e in self . embeddings . items ()} ) Adds a property to every embedding in the set. Very useful for plotting because a property can be used to assign colors. Parameters Name Type Description Default name name of the property to add required func function that receives an embedding and needs to output the property value required Usage: from whatlies.embeddingset import EmbeddingSet foo = Embedding ( \"foo\" , [ 0.1 , 0.3 , 0.10 ]) bar = Embedding ( \"bar\" , [ 0.7 , 0.2 , 0.11 ]) emb = EmbeddingSet ( foo , bar ) emb_with_property = emb . add_property ( 'example' , lambda d : 'group-one' )","title":"add_property()"},{"location":"api/embeddingset/#whatlies.embeddingset.EmbeddingSet.assign","text":"Show source code in whatlies/embeddingset.py 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 def assign ( self , ** kwargs ): \"\"\" Adds properties to every embedding in the set based on the keyword arguments. This is very useful for plotting because a property can be used to assign colors. This method is very similar to `.add_property` but it might be more convenient when you want to assign multiple properties in one single statement. Arguments: kwargs: (name, func)-pairs that describe the name of the property as well as a value to assign. The value can be a single value, iterable or a function. The function expects an `Embedding` object as input. Usage: ```python from whatlies.embeddingset import EmbeddingSet foo = Embedding(\"foo\", [0.1, 0.3, 0.10]) bar = Embedding(\"bar\", [0.7, 0.2, 0.11]) emb = EmbeddingSet(foo, bar) emb_with_property1 = emb.assign(dim0=lambda d: d.vector[0], dim1=lambda d: d.vector[1], dim2=lambda d: d.vector[2]) emb_with_property2 = emb.assign(group=[\"foo_grp\", \"bar_grp\"]) emb_with_property3 = emb.assign(constant=1) ``` \"\"\" new_set = {} for idx , ( k , e ) in enumerate ( self . embeddings . items ()): new_emb = e for name , val in kwargs . items (): if callable ( val ): new_emb = new_emb . add_property ( name , val ) elif hasattr ( val , \"__iter__\" ) and not isinstance ( val , str ): # We want to support lists, tuples, numpy arrays but not strings # those need to be handle as if they're literals. if len ( val ) != len ( self ): raise ValueError ( f \"If you're passing an iterable to `.assign` then it must have the same length as the `EmbeddingSet`. \\n Got: { len ( val ) } . Expected: { len ( self ) } .\" ) new_emb = new_emb . add_property ( name , lambda d : val [ idx ]) else : new_emb = new_emb . add_property ( name , lambda d : val ) new_set [ k ] = new_emb return EmbeddingSet ( new_set ) Adds properties to every embedding in the set based on the keyword arguments. This is very useful for plotting because a property can be used to assign colors. This method is very similar to .add_property but it might be more convenient when you want to assign multiple properties in one single statement. Parameters Name Type Description Default **kwargs (name, func)-pairs that describe the name of the property as well as a value to assign. The value can be a single value, iterable or a function. The function expects an Embedding object as input. {} Usage: from whatlies.embeddingset import EmbeddingSet foo = Embedding ( \"foo\" , [ 0.1 , 0.3 , 0.10 ]) bar = Embedding ( \"bar\" , [ 0.7 , 0.2 , 0.11 ]) emb = EmbeddingSet ( foo , bar ) emb_with_property1 = emb . assign ( dim0 = lambda d : d . vector [ 0 ], dim1 = lambda d : d . vector [ 1 ], dim2 = lambda d : d . vector [ 2 ]) emb_with_property2 = emb . assign ( group = [ \"foo_grp\" , \"bar_grp\" ]) emb_with_property3 = emb . assign ( constant = 1 )","title":"assign()"},{"location":"api/embeddingset/#whatlies.embeddingset.EmbeddingSet.average","text":"Show source code in whatlies/embeddingset.py 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 def average ( self , name = None ): \"\"\" Takes the average over all the embedding vectors in the embeddingset. Turns it into a new `Embedding`. Arguments: name: manually specify the name of the average embedding Usage: ```python from whatlies.embeddingset import EmbeddingSet foo = Embedding(\"foo\", [1.0, 0.0]) bar = Embedding(\"bar\", [0.0, 1.0]) emb = EmbeddingSet(foo, bar) emb.average().vector # [0.5, 0,5] emb.average(name=\"the-average\").vector # [0.5, 0.5] ``` \"\"\" name = f \" { self . name } .average()\" if not name else name x = self . to_X () return Embedding ( name , np . mean ( x , axis = 0 )) Takes the average over all the embedding vectors in the embeddingset. Turns it into a new Embedding . Parameters Name Type Description Default name manually specify the name of the average embedding None Usage: from whatlies.embeddingset import EmbeddingSet foo = Embedding ( \"foo\" , [ 1.0 , 0.0 ]) bar = Embedding ( \"bar\" , [ 0.0 , 1.0 ]) emb = EmbeddingSet ( foo , bar ) emb . average () . vector # [0.5, 0,5] emb . average ( name = \"the-average\" ) . vector # [0.5, 0.5]","title":"average()"},{"location":"api/embeddingset/#whatlies.embeddingset.EmbeddingSet.compare_against","text":"Show source code in whatlies/embeddingset.py 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 def compare_against ( self , other : Union [ str , Embedding ], mapping : Optional [ Callable ] = None ) -> List : \"\"\" Compare (or map) the embeddigns in the embeddingset to a given embedding, optionally using a custom mapping function. Arguments: other: an `Embedding` instance, or name of an existing embedding; it is used for comparison with each embedding in the embeddingset. mapping: an optional callable used for for comparison that takes two 1D vector arrays as input; if not given, the normalized scalar projection (i.e. `>` operator) is used. \"\"\" if isinstance ( other , str ): other = self [ other ] if mapping is None : return [ v > other for v in self . embeddings . values ()] elif callable ( mapping ): return [ mapping ( v . vector , other . vector ) for v in self . embeddings . values ()] else : raise ValueError ( f \"Unrecognized mapping value/type, got: { mapping } \" ) Compare (or map) the embeddigns in the embeddingset to a given embedding, optionally using a custom mapping function. Parameters Name Type Description Default other Union[str, whatlies.embedding.Embedding] an Embedding instance, or name of an existing embedding; it is used for comparison with each embedding in the embeddingset. required mapping Optional[Callable] an optional callable used for for comparison that takes two 1D vector arrays as input; if not given, the normalized scalar projection (i.e. > operator) is used. None","title":"compare_against()"},{"location":"api/embeddingset/#whatlies.embeddingset.EmbeddingSet.embset_similar","text":"Show source code in whatlies/embeddingset.py 547 548 549 550 551 552 553 554 555 556 557 558 559 560 def embset_similar ( self , emb : Union [ str , Embedding ], n : int = 10 , metric = \"cosine\" ): \"\"\" Retreive an [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] that are the most simmilar to the passed query. Arguments: emb: query to use n: the number of items you'd like to see returned metric: metric to use to calculate distance, must be scipy or sklearn compatible Returns: An [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] containing the similar embeddings. \"\"\" embs = [ w [ 0 ] for w in self . score_similar ( emb , n , metric )] return EmbeddingSet ({ w . name : w for w in embs }) Retreive an EmbeddingSet that are the most simmilar to the passed query. Parameters Name Type Description Default emb Union[str, whatlies.embedding.Embedding] query to use required n int the number of items you'd like to see returned 10 metric metric to use to calculate distance, must be scipy or sklearn compatible 'cosine' Returns Type Description `` An EmbeddingSet containing the similar embeddings.","title":"embset_similar()"},{"location":"api/embeddingset/#whatlies.embeddingset.EmbeddingSet.filter","text":"Show source code in whatlies/embeddingset.py 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 def filter ( self , func ): \"\"\" Filters the collection of embeddings based on a predicate function. Arguments: func: callable that accepts a single embedding and outputs a boolean ```python from whatlies.embeddingset import EmbeddingSet foo = Embedding(\"foo\", [0.1, 0.3, 0.10]) bar = Embedding(\"bar\", [0.7, 0.2, 0.11]) buz = Embedding(\"buz\", [0.1, 0.9, 0.12]) xyz = Embedding(\"xyz\", [0.1, 0.9, 0.12]) emb = EmbeddingSet(foo, bar, buz, xyz) emb.filter(lambda e: \"foo\" not in e.name) ``` \"\"\" return EmbeddingSet ({ k : v for k , v in self . embeddings . items () if func ( v )}) Filters the collection of embeddings based on a predicate function. Parameters Name Type Description Default func callable that accepts a single embedding and outputs a boolean required from whatlies.embeddingset import EmbeddingSet foo = Embedding ( \"foo\" , [ 0.1 , 0.3 , 0.10 ]) bar = Embedding ( \"bar\" , [ 0.7 , 0.2 , 0.11 ]) buz = Embedding ( \"buz\" , [ 0.1 , 0.9 , 0.12 ]) xyz = Embedding ( \"xyz\" , [ 0.1 , 0.9 , 0.12 ]) emb = EmbeddingSet ( foo , bar , buz , xyz ) emb . filter ( lambda e : \"foo\" not in e . name )","title":"filter()"},{"location":"api/embeddingset/#whatlies.embeddingset.EmbeddingSet.from_names_X","text":"Show source code in whatlies/embeddingset.py 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 @classmethod def from_names_X ( cls , names , X ): \"\"\" Constructs an `EmbeddingSet` instance from the given embedding names and vectors. Arguments: names: an iterable containing the names of embeddings X: an iterable of 1D vectors, or a 2D numpy array; it should have the same length as `names` Usage: ```python from whatlies.embeddingset import EmbeddingSet names = [\"foo\", \"bar\", \"buz\"] vecs = [ [0.1, 0.3], [0.7, 0.2], [0.1, 0.9], ] emb = EmbeddingSet.from_names_X(names, vecs) ``` \"\"\" X = np . array ( X ) if len ( X ) != len ( names ): raise ValueError ( f \"The number of given names ( { len ( names ) } ) and vectors ( { len ( X ) } ) should be the same.\" ) return cls ({ n : Embedding ( n , v ) for n , v in zip ( names , X )}) Constructs an EmbeddingSet instance from the given embedding names and vectors. Parameters Name Type Description Default names an iterable containing the names of embeddings required X an iterable of 1D vectors, or a 2D numpy array; it should have the same length as names required Usage: from whatlies.embeddingset import EmbeddingSet names = [ \"foo\" , \"bar\" , \"buz\" ] vecs = [ [ 0.1 , 0.3 ], [ 0.7 , 0.2 ], [ 0.1 , 0.9 ], ] emb = EmbeddingSet . from_names_X ( names , vecs )","title":"from_names_X()"},{"location":"api/embeddingset/#whatlies.embeddingset.EmbeddingSet.merge","text":"Show source code in whatlies/embeddingset.py 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 def merge ( self , other ): \"\"\" Concatenates two embeddingssets together Arguments: other: another embeddingset Usage: ```python from whatlies.embeddingset import EmbeddingSet foo = Embedding(\"foo\", [0.1, 0.3, 0.10]) bar = Embedding(\"bar\", [0.7, 0.2, 0.11]) buz = Embedding(\"buz\", [0.1, 0.9, 0.12]) xyz = Embedding(\"xyz\", [0.1, 0.9, 0.12]) emb1 = EmbeddingSet(foo, bar) emb2 = EmbeddingSet(xyz, buz) both = emb1.merge(emb2) ``` \"\"\" return EmbeddingSet ({ ** self . embeddings , ** other . embeddings }) Concatenates two embeddingssets together Parameters Name Type Description Default other another embeddingset required Usage: from whatlies.embeddingset import EmbeddingSet foo = Embedding ( \"foo\" , [ 0.1 , 0.3 , 0.10 ]) bar = Embedding ( \"bar\" , [ 0.7 , 0.2 , 0.11 ]) buz = Embedding ( \"buz\" , [ 0.1 , 0.9 , 0.12 ]) xyz = Embedding ( \"xyz\" , [ 0.1 , 0.9 , 0.12 ]) emb1 = EmbeddingSet ( foo , bar ) emb2 = EmbeddingSet ( xyz , buz ) both = emb1 . merge ( emb2 )","title":"merge()"},{"location":"api/embeddingset/#whatlies.embeddingset.EmbeddingSet.movement_df","text":"Show source code in whatlies/embeddingset.py 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 def movement_df ( self , other , metric = \"euclidean\" ): \"\"\" Creates a dataframe that shows the movement from one embeddingset to another one. Arguments: other: the other embeddingset to compare against, will only keep the overlap metric: metric to use to calculate movement, must be scipy or sklearn compatible Usage: ```python from whatlies.language import SpacyLanguage lang = SpacyLanguage(\"en_core_web_sm\") names = ['red', 'blue', 'green', 'yellow', 'cat', 'dog', 'mouse', 'rat', 'bike', 'car'] emb = lang[names] emb_ort = lang[names] | lang['cat'] emb.movement_df(emb_ort) ``` \"\"\" overlap = list ( set ( self . embeddings . keys ()) . intersection ( set ( other . embeddings . keys ())) ) mat1 = np . array ([ w . vector for w in self [ overlap ]]) mat2 = np . array ([ w . vector for w in other [ overlap ]]) return ( pd . DataFrame ( { \"name\" : overlap , \"movement\" : paired_distances ( mat1 , mat2 , metric = metric ), } ) . sort_values ([ \"movement\" ], ascending = False ) . reset_index () ) Creates a dataframe that shows the movement from one embeddingset to another one. Parameters Name Type Description Default other the other embeddingset to compare against, will only keep the overlap required metric metric to use to calculate movement, must be scipy or sklearn compatible 'euclidean' Usage: from whatlies.language import SpacyLanguage lang = SpacyLanguage ( \"en_core_web_sm\" ) names = [ 'red' , 'blue' , 'green' , 'yellow' , 'cat' , 'dog' , 'mouse' , 'rat' , 'bike' , 'car' ] emb = lang [ names ] emb_ort = lang [ names ] | lang [ 'cat' ] emb . movement_df ( emb_ort )","title":"movement_df()"},{"location":"api/embeddingset/#whatlies.embeddingset.EmbeddingSet.pipe","text":"Show source code in whatlies/embeddingset.py 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 def pipe ( self , func , * args , ** kwargs ): \"\"\" Applies a function to the embedding set. Useful for method chaining and chunks of code that repeat. Arguments: func: callable that accepts an `EmbeddingSet` set as its first argument args: arguments to also pass to the function kwargs: keyword arguments to also pass to the function ```python from whatlies.language import SpacyLanguage, BytePairLanguage lang_sp = SpacyLanguage(\"en_core_web_sm\") lang_bp = BytePairLanguage(\"en\", dim=25, vs=1000) text = [\"cat\", \"dog\", \"rat\", \"blue\", \"red\", \"yellow\"] def make_plot(embset): return (embset .plot_interactive(\"dog\", \"blue\") .properties(height=200, width=200)) p1 = lang_sp[text].pipe(make_plot) p2 = lang_bp[text].pipe(make_plot) p1 | p2 ``` \"\"\" return func ( self , * args , ** kwargs ) Applies a function to the embedding set. Useful for method chaining and chunks of code that repeat. Parameters Name Type Description Default func callable that accepts an EmbeddingSet set as its first argument required *args arguments to also pass to the function () **kwargs keyword arguments to also pass to the function {} from whatlies.language import SpacyLanguage , BytePairLanguage lang_sp = SpacyLanguage ( \"en_core_web_sm\" ) lang_bp = BytePairLanguage ( \"en\" , dim = 25 , vs = 1000 ) text = [ \"cat\" , \"dog\" , \"rat\" , \"blue\" , \"red\" , \"yellow\" ] def make_plot ( embset ): return ( embset . plot_interactive ( \"dog\" , \"blue\" ) . properties ( height = 200 , width = 200 )) p1 = lang_sp [ text ] . pipe ( make_plot ) p2 = lang_bp [ text ] . pipe ( make_plot ) p1 | p2","title":"pipe()"},{"location":"api/embeddingset/#whatlies.embeddingset.EmbeddingSet.plot","text":"Show source code in whatlies/embeddingset.py 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 def plot ( self , kind : str = \"arrow\" , x_axis : Union [ int , str , Embedding ] = 0 , y_axis : Union [ int , str , Embedding ] = 1 , axis_metric : Optional [ Union [ str , Callable , Sequence ]] = None , x_label : Optional [ str ] = None , y_label : Optional [ str ] = None , title : Optional [ str ] = None , color : str = None , show_ops : bool = False , annot : bool = True , axis_option : Optional [ str ] = None , ): \"\"\" Makes (perhaps inferior) matplotlib plot. Consider using `plot_interactive` instead. Arguments: kind: what kind of plot to make, can be `scatter`, `arrow` or `text` x_axis: the x-axis to be used, must be given when dim > 2; if an integer, the corresponding dimension of embedding is used. y_axis: the y-axis to be used, must be given when dim > 2; if an integer, the corresponding dimension of embedding is used. axis_metric: the metric used to project each embedding on the axes; only used when the corresponding axis (i.e. `x_axis` or `y_axis`) is a string or an `Embedding` instance. It could be a string (`'cosine_similarity'`, `'cosine_distance'` or `'euclidean'`), or a callable that takes two vectors as input and returns a scalar value as output. To set different metrics for x- and y-axis, a list or a tuple of two elements could be given. By default (`None`), normalized scalar projection (i.e. `>` operator) is used. x_label: an optional label used for x-axis; if not given, it is set based on value of `x_axis`. y_label: an optional label used for y-axis; if not given, it is set based on value of `y_axis`. title: an optional title for the plot. color: the color of the dots show_ops: setting to also show the applied operations, only works for `text` annot: should the points be annotated axis_option: a string which is passed as `option` argument to `matplotlib.pyplot.axis` in order to control axis properties (e.g. using `'equal'` make circles shown circular in the plot). This might be useful for preserving geometric relationships (e.g. orthogonality) in the generated plot. See `matplotlib.pyplot.axis` [documentation](https://matplotlib.org/3.1.0/api/_as_gen/matplotlib.pyplot.axis.html#matplotlib-pyplot-axis) for possible values and their description. \"\"\" if isinstance ( x_axis , str ): x_axis = self [ x_axis ] if isinstance ( y_axis , str ): y_axis = self [ y_axis ] if isinstance ( axis_metric , ( list , tuple )): x_axis_metric = axis_metric [ 0 ] y_axis_metric = axis_metric [ 1 ] else : x_axis_metric = axis_metric y_axis_metric = axis_metric embeddings = [] for emb in self . embeddings . values (): x_val , x_lab = emb . _get_plot_axis_value_and_label ( x_axis , x_axis_metric , dir = \"x\" ) y_val , y_lab = emb . _get_plot_axis_value_and_label ( y_axis , y_axis_metric , dir = \"y\" ) emb_plot = Embedding ( name = emb . name , vector = [ x_val , y_val ], orig = emb . orig ) embeddings . append ( emb_plot ) x_label = x_lab if x_label is None else x_label y_label = y_lab if y_label is None else y_label handle_2d_plot ( embeddings , kind = kind , color = color , xlabel = x_label , ylabel = y_label , title = title , show_operations = show_ops , annot = annot , axis_option = axis_option , ) return self Makes (perhaps inferior) matplotlib plot. Consider using plot_interactive instead. Parameters Name Type Description Default kind str what kind of plot to make, can be scatter , arrow or text 'arrow' x_axis Union[int, str, whatlies.embedding.Embedding] the x-axis to be used, must be given when dim > 2; if an integer, the corresponding dimension of embedding is used. 0 y_axis Union[int, str, whatlies.embedding.Embedding] the y-axis to be used, must be given when dim > 2; if an integer, the corresponding dimension of embedding is used. 1 axis_metric Optional[Union[str, Callable, Sequence]] the metric used to project each embedding on the axes; only used when the corresponding axis (i.e. x_axis or y_axis ) is a string or an Embedding instance. It could be a string ( 'cosine_similarity' , 'cosine_distance' or 'euclidean' ), or a callable that takes two vectors as input and returns a scalar value as output. To set different metrics for x- and y-axis, a list or a tuple of two elements could be given. By default ( None ), normalized scalar projection (i.e. > operator) is used. None x_label Optional[str] an optional label used for x-axis; if not given, it is set based on value of x_axis . None y_label Optional[str] an optional label used for y-axis; if not given, it is set based on value of y_axis . None title Optional[str] an optional title for the plot. None color str the color of the dots None show_ops bool setting to also show the applied operations, only works for text False annot bool should the points be annotated True axis_option Optional[str] a string which is passed as option argument to matplotlib.pyplot.axis in order to control axis properties (e.g. using 'equal' make circles shown circular in the plot). This might be useful for preserving geometric relationships (e.g. orthogonality) in the generated plot. See matplotlib.pyplot.axis documentation for possible values and their description. None","title":"plot()"},{"location":"api/embeddingset/#whatlies.embeddingset.EmbeddingSet.plot_3d","text":"Show source code in whatlies/embeddingset.py 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 def plot_3d ( self , x_axis : Union [ int , str , Embedding ] = 0 , y_axis : Union [ int , str , Embedding ] = 1 , z_axis : Union [ int , str , Embedding ] = 2 , x_label : Optional [ str ] = None , y_label : Optional [ str ] = None , z_label : Optional [ str ] = None , title : Optional [ str ] = None , color : str = None , axis_metric : Optional [ Union [ str , Callable , Sequence ]] = None , annot : bool = True , ): \"\"\" Creates a 3d visualisation of the embedding. Arguments: x_axis: the x-axis to be used, must be given when dim > 3; if an integer, the corresponding dimension of embedding is used. y_axis: the y-axis to be used, must be given when dim > 3; if an integer, the corresponding dimension of embedding is used. z_axis: the z-axis to be used, must be given when dim > 3; if an integer, the corresponding dimension of embedding is used. x_label: an optional label used for x-axis; if not given, it is set based on value of `x_axis`. y_label: an optional label used for y-axis; if not given, it is set based on value of `y_axis`. z_label: an optional label used for z-axis; if not given, it is set based on value of `z_axis`. title: an optional title for the plot. color: the property to user for the color axis_metric: the metric used to project each embedding on the axes; only used when the corresponding axis is a string or an `Embedding` instance. It could be a string (`'cosine_similarity'`, `'cosine_distance'` or `'euclidean'`), or a callable that takes two vectors as input and returns a scalar value as output. To set different metrics of the three different axes, you can pass a list/tuple of size three that describes the metrics you're interested in. By default (`None`), normalized scalar projection (i.e. `>` operator) is used. annot: drawn points should be annotated **Usage** ```python from whatlies.language import SpacyLanguage from whatlies.transformers import Pca words = [\"prince\", \"princess\", \"nurse\", \"doctor\", \"banker\", \"man\", \"woman\", \"cousin\", \"neice\", \"king\", \"queen\", \"dude\", \"guy\", \"gal\", \"fire\", \"dog\", \"cat\", \"mouse\", \"red\", \"blue\", \"green\", \"yellow\", \"water\", \"person\", \"family\", \"brother\", \"sister\"] lang = SpacyLanguage(\"en_core_web_sm\") emb = lang[words] emb.transform(Pca(3)).plot_3d(annot=True) emb.transform(Pca(3)).plot_3d(\"king\", \"dog\", \"red\") emb.transform(Pca(3)).plot_3d(\"king\", \"dog\", \"red\", axis_metric=\"cosine_distance\") ``` \"\"\" if isinstance ( x_axis , str ): x_axis = self [ x_axis ] if isinstance ( y_axis , str ): y_axis = self [ y_axis ] if isinstance ( z_axis , str ): z_axis = self [ z_axis ] if isinstance ( axis_metric , ( list , tuple )): x_axis_metric = axis_metric [ 0 ] y_axis_metric = axis_metric [ 1 ] z_axis_metric = axis_metric [ 2 ] else : x_axis_metric = axis_metric y_axis_metric = axis_metric z_axis_metric = axis_metric # Determine axes values and labels if isinstance ( x_axis , int ): x_val = self . to_X ()[:, x_axis ] x_lab = \"Dimension \" + str ( x_axis ) else : x_axis_metric = Embedding . _get_plot_axis_metric_callable ( x_axis_metric ) x_val = self . compare_against ( x_axis , mapping = x_axis_metric ) x_lab = x_axis . name x_lab = x_label if x_label is not None else x_lab if isinstance ( y_axis , int ): y_val = self . to_X ()[:, y_axis ] y_lab = \"Dimension \" + str ( y_axis ) else : y_axis_metric = Embedding . _get_plot_axis_metric_callable ( y_axis_metric ) y_val = self . compare_against ( y_axis , mapping = y_axis_metric ) y_lab = y_axis . name y_lab = y_label if y_label is not None else y_lab if isinstance ( z_axis , int ): z_val = self . to_X ()[:, z_axis ] z_lab = \"Dimension \" + str ( z_axis ) else : z_axis_metric = Embedding . _get_plot_axis_metric_callable ( z_axis_metric ) z_val = self . compare_against ( z_axis , mapping = z_axis_metric ) z_lab = z_axis . name z_lab = z_label if z_label is not None else z_lab # Save relevant information in a dataframe for plotting later. plot_df = pd . DataFrame ( { \"x_axis\" : x_val , \"y_axis\" : y_val , \"z_axis\" : z_val , \"name\" : [ v . name for v in self . embeddings . values ()], \"original\" : [ v . orig for v in self . embeddings . values ()], } ) # Deal with the colors of the dots. if color : plot_df [ \"color\" ] = [ getattr ( v , color ) if hasattr ( v , color ) else \"\" for v in self . embeddings . values () ] color_map = { k : v for v , k in enumerate ( set ( plot_df [ \"color\" ]))} color_val = [ color_map [ k ] if not isinstance ( k , float ) else k for k in plot_df [ \"color\" ] ] else : color_val = None ax = plt . axes ( projection = \"3d\" ) ax . scatter3D ( plot_df [ \"x_axis\" ], plot_df [ \"y_axis\" ], plot_df [ \"z_axis\" ], c = color_val , s = 25 ) # Set the labels, titles, text annotations. ax . set_xlabel ( x_lab ) ax . set_ylabel ( y_lab ) ax . set_zlabel ( z_lab ) if annot : for i , row in plot_df . iterrows (): ax . text ( row [ \"x_axis\" ], row [ \"y_axis\" ], row [ \"z_axis\" ] + 0.05 , row [ \"original\" ] ) if title : ax . set_title ( label = title ) return ax Creates a 3d visualisation of the embedding. Parameters Name Type Description Default x_axis Union[int, str, whatlies.embedding.Embedding] the x-axis to be used, must be given when dim > 3; if an integer, the corresponding dimension of embedding is used. 0 y_axis Union[int, str, whatlies.embedding.Embedding] the y-axis to be used, must be given when dim > 3; if an integer, the corresponding dimension of embedding is used. 1 z_axis Union[int, str, whatlies.embedding.Embedding] the z-axis to be used, must be given when dim > 3; if an integer, the corresponding dimension of embedding is used. 2 x_label Optional[str] an optional label used for x-axis; if not given, it is set based on value of x_axis . None y_label Optional[str] an optional label used for y-axis; if not given, it is set based on value of y_axis . None z_label Optional[str] an optional label used for z-axis; if not given, it is set based on value of z_axis . None title Optional[str] an optional title for the plot. None color str the property to user for the color None axis_metric Optional[Union[str, Callable, Sequence]] the metric used to project each embedding on the axes; only used when the corresponding axis is a string or an Embedding instance. It could be a string ( 'cosine_similarity' , 'cosine_distance' or 'euclidean' ), or a callable that takes two vectors as input and returns a scalar value as output. To set different metrics of the three different axes, you can pass a list/tuple of size three that describes the metrics you're interested in. By default ( None ), normalized scalar projection (i.e. > operator) is used. None annot bool drawn points should be annotated True Usage from whatlies.language import SpacyLanguage from whatlies.transformers import Pca words = [ \"prince\" , \"princess\" , \"nurse\" , \"doctor\" , \"banker\" , \"man\" , \"woman\" , \"cousin\" , \"neice\" , \"king\" , \"queen\" , \"dude\" , \"guy\" , \"gal\" , \"fire\" , \"dog\" , \"cat\" , \"mouse\" , \"red\" , \"blue\" , \"green\" , \"yellow\" , \"water\" , \"person\" , \"family\" , \"brother\" , \"sister\" ] lang = SpacyLanguage ( \"en_core_web_sm\" ) emb = lang [ words ] emb . transform ( Pca ( 3 )) . plot_3d ( annot = True ) emb . transform ( Pca ( 3 )) . plot_3d ( \"king\" , \"dog\" , \"red\" ) emb . transform ( Pca ( 3 )) . plot_3d ( \"king\" , \"dog\" , \"red\" , axis_metric = \"cosine_distance\" )","title":"plot_3d()"},{"location":"api/embeddingset/#whatlies.embeddingset.EmbeddingSet.plot_brush","text":"Show source code in whatlies/embeddingset.py 1198 1199 1200 1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213 1214 1215 1216 1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 1233 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248 1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291 1292 1293 1294 1295 1296 1297 1298 1299 1300 1301 1302 1303 1304 1305 1306 1307 1308 1309 1310 1311 1312 1313 1314 1315 1316 1317 1318 1319 1320 1321 1322 1323 1324 1325 1326 1327 1328 1329 1330 1331 1332 1333 1334 1335 1336 1337 1338 1339 1340 1341 1342 1343 1344 1345 1346 1347 1348 1349 1350 1351 1352 def plot_brush ( self , x_axis : Union [ int , str , Embedding ] = 0 , y_axis : Union [ int , str , Embedding ] = 1 , axis_metric : Optional [ Union [ str , Callable , Sequence ]] = None , x_label : Optional [ str ] = None , y_label : Optional [ str ] = None , title : Optional [ str ] = None , annot : bool = False , color : Union [ None , str ] = None , n_show : int = 15 , interactive : bool = False , ): \"\"\" Makes an interactive plot with a brush element. Arguments: x_axis: the x-axis to be used, must be given when dim > 2; if an integer, the corresponding dimension of embedding is used. y_axis: the y-axis to be used, must be given when dim > 2; if an integer, the corresponding dimension of embedding is used. axis_metric: the metric used to project each embedding on the axes; only used when the corresponding axis (i.e. `x_axis` or `y_axis`) is a string or an `Embedding` instance. It could be a string (`'cosine_similarity'`, `'cosine_distance'` or `'euclidean'`), or a callable that takes two vectors as input and returns a scalar value as output. To set different metrics for x- and y-axis, a list or a tuple of two elements could be given. By default (`None`), normalized scalar projection (i.e. `>` operator) is used. x_label: an optional label used for x-axis; if not given, it is set based on `x_axis` value. y_label: an optional label used for y-axis; if not given, it is set based on `y_axis` value. title: an optional title for the plot; if not given, it is set based on `x_axis` and `y_axis` values. annot: drawn points should be annotated color: a property that will be used for plotting n_show: number of points to show in text selection interactive: turn on/off the zoom/panning feature; if turned on, zoom/pan can be triggered when shift key is held **Usage** ```python from whatlies.language import SpacyLanguage from whatlies.transformers import Pca words = [\"prince\", \"princess\", \"nurse\", \"doctor\", \"banker\", \"man\", \"woman\", \"cousin\", \"neice\", \"king\", \"queen\", \"dude\", \"guy\", \"gal\", \"fire\", \"dog\", \"cat\", \"mouse\", \"red\", \"blue\", \"green\", \"yellow\", \"water\", \"person\", \"family\", \"brother\", \"sister\"] lang = SpacyLanguage(\"en_core_web_sm\") emb = lang[words].transform(Pca(2)) emb.plot_brush() ``` \"\"\" if isinstance ( x_axis , str ): x_axis = self [ x_axis ] if isinstance ( y_axis , str ): y_axis = self [ y_axis ] if isinstance ( axis_metric , ( list , tuple )): x_axis_metric = axis_metric [ 0 ] y_axis_metric = axis_metric [ 1 ] else : x_axis_metric = axis_metric y_axis_metric = axis_metric # Determine axes values and labels if isinstance ( x_axis , int ): x_val = self . to_X ()[:, x_axis ] x_lab = \"Dimension \" + str ( x_axis ) else : x_axis_metric = Embedding . _get_plot_axis_metric_callable ( x_axis_metric ) x_val = self . compare_against ( x_axis , mapping = x_axis_metric ) x_lab = x_axis . name if isinstance ( y_axis , int ): y_val = self . to_X ()[:, y_axis ] y_lab = \"Dimension \" + str ( y_axis ) else : y_axis_metric = Embedding . _get_plot_axis_metric_callable ( y_axis_metric ) y_val = self . compare_against ( y_axis , mapping = y_axis_metric ) y_lab = y_axis . name x_label = x_label if x_label is not None else x_lab y_label = y_label if y_label is not None else y_lab title = title if title is not None else \"Click and Drag Here\" plot_df = pd . DataFrame ( { \"x_axis\" : x_val , \"y_axis\" : y_val , \"name\" : [ v . name for v in self . embeddings . values ()], \"original\" : [ v . orig for v in self . embeddings . values ()], } ) if color : plot_df [ color ] = [ getattr ( v , color ) if hasattr ( v , color ) else \"\" for v in self . embeddings . values () ] result = ( alt . Chart ( plot_df ) . mark_circle ( size = 60 ) . encode ( x = alt . X ( \"x_axis\" , axis = alt . Axis ( title = x_label )), y = alt . X ( \"y_axis\" , axis = alt . Axis ( title = y_label )), tooltip = [ \"name\" , \"original\" ], color = alt . Color ( \":N\" , legend = None ) if not color else alt . Color ( color ), ) . properties ( title = title ) ) if annot : text = ( alt . Chart ( plot_df ) . mark_text ( dx =- 15 , dy = 3 , color = \"black\" ) . encode ( x = \"x_axis\" , y = \"y_axis\" , text = \"original\" , ) ) result = result + text brush = alt . selection_interval ( on = \"[mousedown[!event.shiftKey], mouseup] > mousemove\" , translate = \"[mousedown[!event.shiftKey], mouseup] > mousemove!\" , ) ranked_text = ( alt . Chart ( plot_df ) . mark_text () . encode ( y = alt . Y ( \"row_number:O\" , axis = None ), color = alt . Color ( \":N\" , legend = None ) if not color else alt . Color ( color ), ) . transform_window ( row_number = \"row_number()\" ) . transform_filter ( brush ) . transform_window ( rank = \"rank(row_number)\" ) . transform_filter ( alt . datum . rank < n_show ) ) text_plt = ranked_text . encode ( text = \"original:N\" ) . properties ( width = 250 , title = \"Text Selection\" ) if interactive : zoom = alt . selection_interval ( bind = \"scales\" , on = \"[mousedown[event.shiftKey], mouseup] > mousemove\" , translate = \"[mousedown[event.shiftKey], mouseup] > mousemove!\" , ) result = result . add_selection ( zoom , brush ) else : result = result . add_selection ( brush ) return result | text_plt Makes an interactive plot with a brush element. Parameters Name Type Description Default x_axis Union[int, str, whatlies.embedding.Embedding] the x-axis to be used, must be given when dim > 2; if an integer, the corresponding dimension of embedding is used. 0 y_axis Union[int, str, whatlies.embedding.Embedding] the y-axis to be used, must be given when dim > 2; if an integer, the corresponding dimension of embedding is used. 1 axis_metric Optional[Union[str, Callable, Sequence]] the metric used to project each embedding on the axes; only used when the corresponding axis (i.e. x_axis or y_axis ) is a string or an Embedding instance. It could be a string ( 'cosine_similarity' , 'cosine_distance' or 'euclidean' ), or a callable that takes two vectors as input and returns a scalar value as output. To set different metrics for x- and y-axis, a list or a tuple of two elements could be given. By default ( None ), normalized scalar projection (i.e. > operator) is used. None x_label Optional[str] an optional label used for x-axis; if not given, it is set based on x_axis value. None y_label Optional[str] an optional label used for y-axis; if not given, it is set based on y_axis value. None title Optional[str] an optional title for the plot; if not given, it is set based on x_axis and y_axis values. None annot bool drawn points should be annotated False color Union[NoneType, str] a property that will be used for plotting None n_show int number of points to show in text selection 15 interactive bool turn on/off the zoom/panning feature; if turned on, zoom/pan can be triggered when shift key is held False Usage from whatlies.language import SpacyLanguage from whatlies.transformers import Pca words = [ \"prince\" , \"princess\" , \"nurse\" , \"doctor\" , \"banker\" , \"man\" , \"woman\" , \"cousin\" , \"neice\" , \"king\" , \"queen\" , \"dude\" , \"guy\" , \"gal\" , \"fire\" , \"dog\" , \"cat\" , \"mouse\" , \"red\" , \"blue\" , \"green\" , \"yellow\" , \"water\" , \"person\" , \"family\" , \"brother\" , \"sister\" ] lang = SpacyLanguage ( \"en_core_web_sm\" ) emb = lang [ words ] . transform ( Pca ( 2 )) emb . plot_brush ()","title":"plot_brush()"},{"location":"api/embeddingset/#whatlies.embeddingset.EmbeddingSet.plot_distance","text":"Show source code in whatlies/embeddingset.py 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 def plot_distance ( self , metric = \"cosine\" , norm = False ): \"\"\" Make a distance plot. Shows you the distance between all the word embeddings in the set. Arguments: metric: `'cosine'`, `'correlation'` or `'euclidean'` norm: normalise the vectors before calculating the distances Usage: ```python from whatlies.language import SpacyLanguage lang = SpacyLanguage(\"en_core_web_sm\") names = ['red', 'blue', 'green', 'yellow', 'cat', 'dog', 'mouse', 'rat', 'bike', 'car'] emb = lang[names] emb.plot_distance(metric='cosine') emb.plot_distance(metric='euclidean') emb.plot_distance(metric='correlation') ``` \"\"\" allowed_metrics = [ \"cosine\" , \"correlation\" , \"euclidean\" ] if metric not in allowed_metrics : raise ValueError ( f \"The `metric` argument must be in { allowed_metrics } , got: { metric } .\" ) vmin , vmax = 0 , 1 X = self . to_X ( norm = norm ) if metric == \"cosine\" : distances = cosine_distances ( X ) if metric == \"correlation\" : distances = 1 - np . corrcoef ( X ) vmin , vmax = - 1 , 1 if metric == \"euclidean\" : distances = euclidean_distances ( X ) vmin , vmax = 0 , np . max ( distances ) fig , ax = plt . subplots () plt . imshow ( distances , cmap = plt . cm . get_cmap () . reversed (), vmin = vmin , vmax = vmax ) plt . xticks ( range ( len ( self )), self . embeddings . keys ()) plt . yticks ( range ( len ( self )), self . embeddings . keys ()) plt . colorbar () # Rotate the tick labels and set their alignment. plt . setp ( ax . get_xticklabels (), rotation = 90 , ha = \"right\" , rotation_mode = \"anchor\" ) Make a distance plot. Shows you the distance between all the word embeddings in the set. Parameters Name Type Description Default metric 'cosine' , 'correlation' or 'euclidean' 'cosine' norm normalise the vectors before calculating the distances False Usage: from whatlies.language import SpacyLanguage lang = SpacyLanguage ( \"en_core_web_sm\" ) names = [ 'red' , 'blue' , 'green' , 'yellow' , 'cat' , 'dog' , 'mouse' , 'rat' , 'bike' , 'car' ] emb = lang [ names ] emb . plot_distance ( metric = 'cosine' ) emb . plot_distance ( metric = 'euclidean' ) emb . plot_distance ( metric = 'correlation' )","title":"plot_distance()"},{"location":"api/embeddingset/#whatlies.embeddingset.EmbeddingSet.plot_interactive","text":"Show source code in whatlies/embeddingset.py 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 def plot_interactive ( self , x_axis : Union [ int , str , Embedding ] = 0 , y_axis : Union [ int , str , Embedding ] = 1 , axis_metric : Optional [ Union [ str , Callable , Sequence ]] = None , x_label : Optional [ str ] = None , y_label : Optional [ str ] = None , title : Optional [ str ] = None , annot : bool = True , color : Union [ None , str ] = None , interactive : bool = True , ): \"\"\" Makes highly interactive plot of the set of embeddings. Arguments: x_axis: the x-axis to be used, must be given when dim > 2; if an integer, the corresponding dimension of embedding is used. y_axis: the y-axis to be used, must be given when dim > 2; if an integer, the corresponding dimension of embedding is used. axis_metric: the metric used to project each embedding on the axes; only used when the corresponding axis (i.e. `x_axis` or `y_axis`) is a string or an `Embedding` instance. It could be a string (`'cosine_similarity'`, `'cosine_distance'` or `'euclidean'`), or a callable that takes two vectors as input and returns a scalar value as output. To set different metrics for x- and y-axis, a list or a tuple of two elements could be given. By default (`None`), normalized scalar projection (i.e. `>` operator) is used. x_label: an optional label used for x-axis; if not given, it is set based on `x_axis` value. y_label: an optional label used for y-axis; if not given, it is set based on `y_axis` value. title: an optional title for the plot; if not given, it is set based on `x_axis` and `y_axis` values. annot: drawn points should be annotated color: a property that will be used for plotting interactive: turn on/off the zoom/panning feature **Usage** ```python from whatlies.language import SpacyLanguage words = [\"prince\", \"princess\", \"nurse\", \"doctor\", \"banker\", \"man\", \"woman\", \"cousin\", \"neice\", \"king\", \"queen\", \"dude\", \"guy\", \"gal\", \"fire\", \"dog\", \"cat\", \"mouse\", \"red\", \"blue\", \"green\", \"yellow\", \"water\", \"person\", \"family\", \"brother\", \"sister\"] lang = SpacyLanguage(\"en_core_web_sm\") emb = lang[words] emb.plot_interactive('man', 'woman') ``` \"\"\" if isinstance ( x_axis , str ): x_axis = self [ x_axis ] if isinstance ( y_axis , str ): y_axis = self [ y_axis ] if isinstance ( axis_metric , ( list , tuple )): x_axis_metric = axis_metric [ 0 ] y_axis_metric = axis_metric [ 1 ] else : x_axis_metric = axis_metric y_axis_metric = axis_metric # Determine axes values and labels if isinstance ( x_axis , int ): x_val = self . to_X ()[:, x_axis ] x_lab = \"Dimension \" + str ( x_axis ) else : x_axis_metric = Embedding . _get_plot_axis_metric_callable ( x_axis_metric ) x_val = self . compare_against ( x_axis , mapping = x_axis_metric ) x_lab = x_axis . name if isinstance ( y_axis , int ): y_val = self . to_X ()[:, y_axis ] y_lab = \"Dimension \" + str ( y_axis ) else : y_axis_metric = Embedding . _get_plot_axis_metric_callable ( y_axis_metric ) y_val = self . compare_against ( y_axis , mapping = y_axis_metric ) y_lab = y_axis . name x_label = x_label if x_label is not None else x_lab y_label = y_label if y_label is not None else y_lab title = title if title is not None else f \" { x_lab } vs. { y_lab } \" plot_df = pd . DataFrame ( { \"x_axis\" : x_val , \"y_axis\" : y_val , \"name\" : [ v . name for v in self . embeddings . values ()], \"original\" : [ v . orig for v in self . embeddings . values ()], } ) if color : plot_df [ color ] = [ getattr ( v , color ) if hasattr ( v , color ) else \"\" for v in self . embeddings . values () ] result = ( alt . Chart ( plot_df ) . mark_circle ( size = 60 ) . encode ( x = alt . X ( \"x_axis\" , axis = alt . Axis ( title = x_label )), y = alt . X ( \"y_axis\" , axis = alt . Axis ( title = y_label )), tooltip = [ \"name\" , \"original\" ], color = alt . Color ( \":N\" , legend = None ) if not color else alt . Color ( color ), ) . properties ( title = title ) ) if interactive : result = result . interactive () if annot : text = ( alt . Chart ( plot_df ) . mark_text ( dx =- 15 , dy = 3 , color = \"black\" ) . encode ( x = \"x_axis\" , y = \"y_axis\" , text = \"original\" , ) ) result = result + text return result Makes highly interactive plot of the set of embeddings. Parameters Name Type Description Default x_axis Union[int, str, whatlies.embedding.Embedding] the x-axis to be used, must be given when dim > 2; if an integer, the corresponding dimension of embedding is used. 0 y_axis Union[int, str, whatlies.embedding.Embedding] the y-axis to be used, must be given when dim > 2; if an integer, the corresponding dimension of embedding is used. 1 axis_metric Optional[Union[str, Callable, Sequence]] the metric used to project each embedding on the axes; only used when the corresponding axis (i.e. x_axis or y_axis ) is a string or an Embedding instance. It could be a string ( 'cosine_similarity' , 'cosine_distance' or 'euclidean' ), or a callable that takes two vectors as input and returns a scalar value as output. To set different metrics for x- and y-axis, a list or a tuple of two elements could be given. By default ( None ), normalized scalar projection (i.e. > operator) is used. None x_label Optional[str] an optional label used for x-axis; if not given, it is set based on x_axis value. None y_label Optional[str] an optional label used for y-axis; if not given, it is set based on y_axis value. None title Optional[str] an optional title for the plot; if not given, it is set based on x_axis and y_axis values. None annot bool drawn points should be annotated True color Union[NoneType, str] a property that will be used for plotting None interactive bool turn on/off the zoom/panning feature True Usage from whatlies.language import SpacyLanguage words = [ \"prince\" , \"princess\" , \"nurse\" , \"doctor\" , \"banker\" , \"man\" , \"woman\" , \"cousin\" , \"neice\" , \"king\" , \"queen\" , \"dude\" , \"guy\" , \"gal\" , \"fire\" , \"dog\" , \"cat\" , \"mouse\" , \"red\" , \"blue\" , \"green\" , \"yellow\" , \"water\" , \"person\" , \"family\" , \"brother\" , \"sister\" ] lang = SpacyLanguage ( \"en_core_web_sm\" ) emb = lang [ words ] emb . plot_interactive ( 'man' , 'woman' )","title":"plot_interactive()"},{"location":"api/embeddingset/#whatlies.embeddingset.EmbeddingSet.plot_interactive_matrix","text":"Show source code in whatlies/embeddingset.py 1354 1355 1356 1357 1358 1359 1360 1361 1362 1363 1364 1365 1366 1367 1368 1369 1370 1371 1372 1373 1374 1375 1376 1377 1378 1379 1380 1381 1382 1383 1384 1385 1386 1387 1388 1389 1390 1391 1392 1393 1394 1395 1396 1397 1398 1399 1400 1401 1402 1403 1404 1405 1406 1407 1408 1409 1410 1411 1412 1413 1414 1415 1416 1417 1418 1419 1420 1421 1422 1423 1424 1425 1426 1427 1428 1429 1430 1431 1432 1433 1434 1435 1436 1437 1438 1439 1440 1441 1442 1443 1444 1445 def plot_interactive_matrix ( self , * axes : Union [ int , str , Embedding ], axes_metric : Optional [ Union [ str , Callable , Sequence ]] = None , annot : bool = True , width : int = 200 , height : int = 200 , ): \"\"\" Makes highly interactive plot of the set of embeddings. Arguments: axes: the axes that we wish to plot; each could be either an integer, the name of an existing embedding, or an `Embedding` instance (default: `0, 1`). axes_metric: the metric used to project each embedding on the axes; only used when the corresponding axis is a string or an `Embedding` instance. It could be a string (`'cosine_similarity'`, `'cosine_distance'` or `'euclidean'`), or a callable that takes two vectors as input and returns a scalar value as output. To set different metrics for different axes, a list or a tuple of the same length as `axes` could be given. By default (`None`), normalized scalar projection (i.e. `>` operator) is used. annot: drawn points should be annotated width: width of the visual height: height of the visual **Usage** ```python from whatlies.language import SpacyLanguage from whatlies.transformers import Pca words = [\"prince\", \"princess\", \"nurse\", \"doctor\", \"banker\", \"man\", \"woman\", \"cousin\", \"neice\", \"king\", \"queen\", \"dude\", \"guy\", \"gal\", \"fire\", \"dog\", \"cat\", \"mouse\", \"red\", \"blue\", \"green\", \"yellow\", \"water\", \"person\", \"family\", \"brother\", \"sister\"] lang = SpacyLanguage(\"en_core_web_sm\") emb = lang[words] emb.transform(Pca(3)).plot_interactive_matrix(0, 1, 2) ``` \"\"\" # Set default value of axes, if not given. if len ( axes ) == 0 : axes = [ 0 , 1 ] if isinstance ( axes_metric , ( list , tuple )) and len ( axes_metric ) != len ( axes ): raise ValueError ( f \"The number of given axes metrics should be the same as the number of given axes. Got { len ( axes ) } axes vs. { len ( axes_metric ) } metrics.\" ) if not isinstance ( axes_metric , ( list , tuple )): axes_metric = [ axes_metric ] * len ( axes ) # Get values of each axis according to their type. axes_vals = {} X = self . to_X () for axis , metric in zip ( axes , axes_metric ): if isinstance ( axis , int ): vals = X [:, axis ] axes_vals [ \"Dimension \" + str ( axis )] = vals else : if isinstance ( axis , str ): axis = self [ axis ] metric = Embedding . _get_plot_axis_metric_callable ( metric ) vals = self . compare_against ( axis , mapping = metric ) axes_vals [ axis . name ] = vals plot_df = pd . DataFrame ( axes_vals ) plot_df [ \"name\" ] = [ v . name for v in self . embeddings . values ()] plot_df [ \"original\" ] = [ v . orig for v in self . embeddings . values ()] axes_names = list ( axes_vals . keys ()) result = ( alt . Chart ( plot_df ) . mark_circle () . encode ( x = alt . X ( alt . repeat ( \"column\" ), type = \"quantitative\" ), y = alt . Y ( alt . repeat ( \"row\" ), type = \"quantitative\" ), tooltip = [ \"name\" , \"original\" ], ) ) if annot : text_stuff = result . mark_text ( dx =- 15 , dy = 3 , color = \"black\" ) . encode ( text = \"original\" , ) result = result + text_stuff result = ( result . properties ( width = width , height = height ) . repeat ( row = axes_names [:: - 1 ], column = axes_names ) . interactive () ) return result Makes highly interactive plot of the set of embeddings. Parameters Name Type Description Default *axes Union[int, str, whatlies.embedding.Embedding] the axes that we wish to plot; each could be either an integer, the name of an existing embedding, or an Embedding instance (default: 0, 1 ). () axes_metric Optional[Union[str, Callable, Sequence]] the metric used to project each embedding on the axes; only used when the corresponding axis is a string or an Embedding instance. It could be a string ( 'cosine_similarity' , 'cosine_distance' or 'euclidean' ), or a callable that takes two vectors as input and returns a scalar value as output. To set different metrics for different axes, a list or a tuple of the same length as axes could be given. By default ( None ), normalized scalar projection (i.e. > operator) is used. None annot bool drawn points should be annotated True width int width of the visual 200 height int height of the visual 200 Usage from whatlies.language import SpacyLanguage from whatlies.transformers import Pca words = [ \"prince\" , \"princess\" , \"nurse\" , \"doctor\" , \"banker\" , \"man\" , \"woman\" , \"cousin\" , \"neice\" , \"king\" , \"queen\" , \"dude\" , \"guy\" , \"gal\" , \"fire\" , \"dog\" , \"cat\" , \"mouse\" , \"red\" , \"blue\" , \"green\" , \"yellow\" , \"water\" , \"person\" , \"family\" , \"brother\" , \"sister\" ] lang = SpacyLanguage ( \"en_core_web_sm\" ) emb = lang [ words ] emb . transform ( Pca ( 3 )) . plot_interactive_matrix ( 0 , 1 , 2 )","title":"plot_interactive_matrix()"},{"location":"api/embeddingset/#whatlies.embeddingset.EmbeddingSet.plot_movement","text":"Show source code in whatlies/embeddingset.py 995 996 997 998 999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 def plot_movement ( self , other , x_axis : Union [ str , Embedding ], y_axis : Union [ str , Embedding ], first_group_name = \"before\" , second_group_name = \"after\" , annot : bool = True , ): \"\"\" Makes highly interactive plot of the movement of embeddings between two sets of embeddings. Arguments: other: the other embeddingset x_axis: the x-axis to be used, must be given when dim > 2 y_axis: the y-axis to be used, must be given when dim > 2 first_group_name: the name to give to the first set of embeddings (default: \"before\") second_group_name: the name to give to the second set of embeddings (default: \"after\") annot: drawn points should be annotated **Usage** ```python from whatlies.language import SpacyLanguage words = [\"prince\", \"princess\", \"nurse\", \"doctor\", \"banker\", \"man\", \"woman\", \"cousin\", \"neice\", \"king\", \"queen\", \"dude\", \"guy\", \"gal\", \"fire\", \"dog\", \"cat\", \"mouse\", \"red\", \"blue\", \"green\", \"yellow\", \"water\", \"person\", \"family\", \"brother\", \"sister\"] lang = SpacyLanguage(\"en_core_web_sm\") emb = lang[words] emb_new = emb - emb['king'] emb.plot_movement(emb_new, 'man', 'woman') ``` \"\"\" if isinstance ( x_axis , str ): x_axis = self [ x_axis ] if isinstance ( y_axis , str ): y_axis = self [ y_axis ] df1 = ( self . to_axis_df ( x_axis , y_axis ) . set_index ( \"original\" ) . drop ( columns = [ \"name\" ]) ) df2 = ( other . to_axis_df ( x_axis , y_axis ) . set_index ( \"original\" ) . drop ( columns = [ \"name\" ]) . loc [ lambda d : d . index . isin ( df1 . index )] ) df_draw = ( pd . concat ([ df1 , df2 ]) . reset_index () . sort_values ([ \"original\" ]) . assign ( constant = 1 ) ) plots = [] for idx , grp_df in df_draw . groupby ( \"original\" ): _ = ( alt . Chart ( grp_df ) . mark_line ( color = \"gray\" , strokeDash = [ 2 , 1 ]) . encode ( x = \"x_axis:Q\" , y = \"y_axis:Q\" ) ) plots . append ( _ ) p0 = reduce ( lambda x , y : x + y , plots ) p1 = ( deepcopy ( self ) . add_property ( \"group\" , lambda d : first_group_name ) . plot_interactive ( x_axis , y_axis , annot = annot , color = \"group\" ) ) p2 = ( deepcopy ( other ) . add_property ( \"group\" , lambda d : second_group_name ) . plot_interactive ( x_axis , y_axis , annot = annot , color = \"group\" ) ) return p0 + p1 + p2 Makes highly interactive plot of the movement of embeddings between two sets of embeddings. Parameters Name Type Description Default other the other embeddingset required x_axis Union[str, whatlies.embedding.Embedding] the x-axis to be used, must be given when dim > 2 required y_axis Union[str, whatlies.embedding.Embedding] the y-axis to be used, must be given when dim > 2 required first_group_name the name to give to the first set of embeddings (default: \"before\") 'before' second_group_name the name to give to the second set of embeddings (default: \"after\") 'after' annot bool drawn points should be annotated True Usage from whatlies.language import SpacyLanguage words = [ \"prince\" , \"princess\" , \"nurse\" , \"doctor\" , \"banker\" , \"man\" , \"woman\" , \"cousin\" , \"neice\" , \"king\" , \"queen\" , \"dude\" , \"guy\" , \"gal\" , \"fire\" , \"dog\" , \"cat\" , \"mouse\" , \"red\" , \"blue\" , \"green\" , \"yellow\" , \"water\" , \"person\" , \"family\" , \"brother\" , \"sister\" ] lang = SpacyLanguage ( \"en_core_web_sm\" ) emb = lang [ words ] emb_new = emb - emb [ 'king' ] emb . plot_movement ( emb_new , 'man' , 'woman' )","title":"plot_movement()"},{"location":"api/embeddingset/#whatlies.embeddingset.EmbeddingSet.plot_pixels","text":"Show source code in whatlies/embeddingset.py 968 969 970 971 972 973 974 975 976 977 978 979 980 981 982 983 984 985 986 987 988 989 990 991 992 993 def plot_pixels ( self ): \"\"\" Makes a pixelchart of every embedding in the set. Usage: ```python from whatlies.language import SpacyLanguage from whatlies.transformers import Pca lang = SpacyLanguage(\"en_core_web_sm\") names = ['red', 'blue', 'green', 'yellow', 'cat', 'dog', 'mouse', 'rat', 'bike', 'car', 'motor', 'cycle', 'firehydrant', 'japan', 'germany', 'belgium'] emb = lang[names].transform(Pca(12)).filter(lambda e: 'pca' not in e.name) emb.plot_pixels() ``` ![](https://rasahq.github.io/whatlies/images/pixels.png) \"\"\" names = self . embeddings . keys () df = self . to_dataframe () plt . matshow ( df ) plt . yticks ( range ( len ( names )), names ) Makes a pixelchart of every embedding in the set. Usage: from whatlies.language import SpacyLanguage from whatlies.transformers import Pca lang = SpacyLanguage ( \"en_core_web_sm\" ) names = [ 'red' , 'blue' , 'green' , 'yellow' , 'cat' , 'dog' , 'mouse' , 'rat' , 'bike' , 'car' , 'motor' , 'cycle' , 'firehydrant' , 'japan' , 'germany' , 'belgium' ] emb = lang [ names ] . transform ( Pca ( 12 )) . filter ( lambda e : 'pca' not in e . name ) emb . plot_pixels ()","title":"plot_pixels()"},{"location":"api/embeddingset/#whatlies.embeddingset.EmbeddingSet.plot_similarity","text":"Show source code in whatlies/embeddingset.py 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 def plot_similarity ( self , metric = \"cosine\" , norm = False ): \"\"\" Make a similarity plot. Shows you the similarity between all the word embeddings in the set. Arguments: metric: `'cosine'` or `'correlation'` norm: normalise the embeddings before calculating the similarity Usage: ```python from whatlies.language import SpacyLanguage lang = SpacyLanguage(\"en_core_web_sm\") names = ['red', 'blue', 'green', 'yellow', 'cat', 'dog', 'mouse', 'rat', 'bike', 'car'] emb = lang[names] emb.plot_similarity() emb.plot_similarity(metric='correlation') ``` \"\"\" allowed_metrics = [ \"cosine\" , \"correlation\" ] if metric not in allowed_metrics : raise ValueError ( f \"The `metric` argument must be in { allowed_metrics } , got: { metric } .\" ) vmin , vmax = 0 , 1 X = self . to_X ( norm = norm ) if metric == \"cosine\" : similarity = cosine_similarity ( X ) if metric == \"correlation\" : similarity = np . corrcoef ( X ) vmin , vmax = - 1 , 1 fig , ax = plt . subplots () plt . imshow ( similarity , cmap = plt . cm . get_cmap (), vmin =- vmin , vmax = vmax ) plt . xticks ( range ( len ( self )), self . embeddings . keys ()) plt . yticks ( range ( len ( self )), self . embeddings . keys ()) plt . colorbar () # Rotate the tick labels and set their alignment. plt . setp ( ax . get_xticklabels (), rotation = 90 , ha = \"right\" , rotation_mode = \"anchor\" ) Make a similarity plot. Shows you the similarity between all the word embeddings in the set. Parameters Name Type Description Default metric 'cosine' or 'correlation' 'cosine' norm normalise the embeddings before calculating the similarity False Usage: from whatlies.language import SpacyLanguage lang = SpacyLanguage ( \"en_core_web_sm\" ) names = [ 'red' , 'blue' , 'green' , 'yellow' , 'cat' , 'dog' , 'mouse' , 'rat' , 'bike' , 'car' ] emb = lang [ names ] emb . plot_similarity () emb . plot_similarity ( metric = 'correlation' )","title":"plot_similarity()"},{"location":"api/embeddingset/#whatlies.embeddingset.EmbeddingSet.score_similar","text":"Show source code in whatlies/embeddingset.py 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 def score_similar ( self , emb : Union [ str , Embedding ], n : int = 10 , metric = \"cosine\" ): \"\"\" Retreive a list of (Embedding, score) tuples that are the most similar to the passed query. Arguments: emb: query to use n: the number of items you'd like to see returned metric: metric to use to calculate distance, must be scipy or sklearn compatible Returns: An list of ([Embedding][whatlies.embedding.Embedding], score) tuples. \"\"\" if n > len ( self ): raise ValueError ( f \"You cannot retreive (n= { n } ) more items than exist in the Embeddingset (len= { len ( self ) } )\" ) if isinstance ( emb , str ): if emb not in self . embeddings . keys (): raise ValueError ( f \"Embedding for ` { emb } ` does not exist in this EmbeddingSet\" ) emb = self [ emb ] vec = emb . vector queries = [ w for w in self . embeddings . keys ()] vector_matrix = self . to_X () distances = pairwise_distances ( vector_matrix , vec . reshape ( 1 , - 1 ), metric = metric ) by_similarity = sorted ( zip ( queries , distances ), key = lambda z : z [ 1 ]) return [( self [ q ], float ( d )) for q , d in by_similarity [: n ]] Retreive a list of (Embedding, score) tuples that are the most similar to the passed query. Parameters Name Type Description Default emb Union[str, whatlies.embedding.Embedding] query to use required n int the number of items you'd like to see returned 10 metric metric to use to calculate distance, must be scipy or sklearn compatible 'cosine' Returns Type Description `` An list of ( Embedding , score) tuples.","title":"score_similar()"},{"location":"api/embeddingset/#whatlies.embeddingset.EmbeddingSet.to_dataframe","text":"Show source code in whatlies/embeddingset.py 599 600 601 602 603 604 def to_dataframe ( self ): \"\"\" Turns the embeddingset into a pandas dataframe. \"\"\" mat = self . to_matrix () return pd . DataFrame ( mat , index = list ( self . embeddings . keys ())) Turns the embeddingset into a pandas dataframe.","title":"to_dataframe()"},{"location":"api/embeddingset/#whatlies.embeddingset.EmbeddingSet.to_matrix","text":"Show source code in whatlies/embeddingset.py 593 594 595 596 597 def to_matrix ( self ): \"\"\" Does exactly the same as `.to_X`. It takes the embedding vectors and turns it into a numpy array. \"\"\" return self . to_X () Does exactly the same as .to_X . It takes the embedding vectors and turns it into a numpy array.","title":"to_matrix()"},{"location":"api/embeddingset/#whatlies.embeddingset.EmbeddingSet.to_names_X","text":"Show source code in whatlies/embeddingset.py 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 def to_names_X ( self ): \"\"\" Get the list of names as well as an array of vectors of all embeddings. Usage: ```python from whatlies.embedding import Embedding from whatlies.embeddingset import EmbeddingSet foo = Embedding(\"foo\", [0.1, 0.3]) bar = Embedding(\"bar\", [0.7, 0.2]) buz = Embedding(\"buz\", [0.1, 0.9]) emb = EmbeddingSet(foo, bar, buz) names, X = emb.to_names_X() ``` \"\"\" return list ( self . embeddings . keys ()), self . to_X () Get the list of names as well as an array of vectors of all embeddings. Usage: from whatlies.embedding import Embedding from whatlies.embeddingset import EmbeddingSet foo = Embedding ( \"foo\" , [ 0.1 , 0.3 ]) bar = Embedding ( \"bar\" , [ 0.7 , 0.2 ]) buz = Embedding ( \"buz\" , [ 0.1 , 0.9 ]) emb = EmbeddingSet ( foo , bar , buz ) names , X = emb . to_names_X ()","title":"to_names_X()"},{"location":"api/embeddingset/#whatlies.embeddingset.EmbeddingSet.to_X","text":"Show source code in whatlies/embeddingset.py 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 def to_X ( self , norm = False ): \"\"\" Takes every vector in each embedding and turns it into a scikit-learn compatible `X` matrix. Usage: ```python from whatlies.embedding import Embedding from whatlies.embeddingset import EmbeddingSet foo = Embedding(\"foo\", [0.1, 0.3]) bar = Embedding(\"bar\", [0.7, 0.2]) buz = Embedding(\"buz\", [0.1, 0.9]) emb = EmbeddingSet(foo, bar, buz) X = emb.to_X() ``` \"\"\" X = np . array ([ i . vector for i in self . embeddings . values ()]) X = normalize ( X ) if norm else X return X Takes every vector in each embedding and turns it into a scikit-learn compatible X matrix. Usage: from whatlies.embedding import Embedding from whatlies.embeddingset import EmbeddingSet foo = Embedding ( \"foo\" , [ 0.1 , 0.3 ]) bar = Embedding ( \"bar\" , [ 0.7 , 0.2 ]) buz = Embedding ( \"buz\" , [ 0.1 , 0.9 ]) emb = EmbeddingSet ( foo , bar , buz ) X = emb . to_X ()","title":"to_X()"},{"location":"api/embeddingset/#whatlies.embeddingset.EmbeddingSet.to_X_y","text":"Show source code in whatlies/embeddingset.py 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 def to_X_y ( self , y_label ): \"\"\" Takes every vector in each embedding and turns it into a scikit-learn compatible `X` matrix. Also retreives an array with potential labels. Usage: ```python from whatlies.embedding import Embedding from whatlies.embeddingset import EmbeddingSet foo = Embedding(\"foo\", [0.1, 0.3]) bar = Embedding(\"bar\", [0.7, 0.2]) buz = Embedding(\"buz\", [0.1, 0.9]) bla = Embedding(\"bla\", [0.2, 0.8]) emb1 = EmbeddingSet(foo, bar).add_property(\"label\", lambda d: 'group-one') emb2 = EmbeddingSet(buz, bla).add_property(\"label\", lambda d: 'group-two') emb = emb1.merge(emb2) X, y = emb.to_X_y(y_label='label') ``` \"\"\" X = self . to_X () y = np . array ([ getattr ( e , y_label ) for e in self . embeddings . values ()]) return X , y Takes every vector in each embedding and turns it into a scikit-learn compatible X matrix. Also retreives an array with potential labels. Usage: from whatlies.embedding import Embedding from whatlies.embeddingset import EmbeddingSet foo = Embedding ( \"foo\" , [ 0.1 , 0.3 ]) bar = Embedding ( \"bar\" , [ 0.7 , 0.2 ]) buz = Embedding ( \"buz\" , [ 0.1 , 0.9 ]) bla = Embedding ( \"bla\" , [ 0.2 , 0.8 ]) emb1 = EmbeddingSet ( foo , bar ) . add_property ( \"label\" , lambda d : 'group-one' ) emb2 = EmbeddingSet ( buz , bla ) . add_property ( \"label\" , lambda d : 'group-two' ) emb = emb1 . merge ( emb2 ) X , y = emb . to_X_y ( y_label = 'label' )","title":"to_X_y()"},{"location":"api/embeddingset/#whatlies.embeddingset.EmbeddingSet.transform","text":"Show source code in whatlies/embeddingset.py 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 def transform ( self , transformer ): \"\"\" Applies a transformation on the entire set. Usage: ```python from whatlies.embeddingset import EmbeddingSet from whatlies.transformers import Pca foo = Embedding(\"foo\", [0.1, 0.3, 0.10]) bar = Embedding(\"bar\", [0.7, 0.2, 0.11]) buz = Embedding(\"buz\", [0.1, 0.9, 0.12]) emb = EmbeddingSet(foo, bar, buz).transform(Pca(2)) ``` \"\"\" return transformer ( self ) Applies a transformation on the entire set. Usage: from whatlies.embeddingset import EmbeddingSet from whatlies.transformers import Pca foo = Embedding ( \"foo\" , [ 0.1 , 0.3 , 0.10 ]) bar = Embedding ( \"bar\" , [ 0.7 , 0.2 , 0.11 ]) buz = Embedding ( \"buz\" , [ 0.1 , 0.9 , 0.12 ]) emb = EmbeddingSet ( foo , bar , buz ) . transform ( Pca ( 2 ))","title":"transform()"},{"location":"api/helpers/","text":"reverse_strings \u00b6 This helper will reverse the strings in the embeddingset. This can be useful for making matplotlib plots with Arabic texts. This helper is meant to be used via EmbeddingSet.pipe() . Parameters Name Type Description Default embset EmbeddingSet to adapt required Usage: from whatlies.helpers import reverse_strings from whatlies.language import BytePairLanguage translation = { \"man\" : \"\u0631\u062c\u0644\" , \"woman\" : \"\u0627\u0645\u0631\u0623\u0629\" , \"king\" : \"\u0645\u0644\u0643\" , \"queen\" : \"\u0645\u0644\u0643\u0629\" , \"brother\" : \"\u0623\u062e\" , \"sister\" : \"\u0623\u062e\u062a\" , \"cat\" : \"\u0642\u0637\u0629\" , \"dog\" : \"\u0643\u0644\u0628\" , \"lion\" : \"\u0623\u0633\u062f\" , \"puppy\" : \"\u062c\u0631\u0648\" , \"male student\" : \"\u0637\u0627\u0644\u0628\" , \"female student\" : \"\u0637\u0627\u0644\u0628\u0629\" , \"university\" : \"\u062c\u0627\u0645\u0639\u0629\" , \"school\" : \"\u0645\u062f\u0631\u0633\u0629\" , \"kitten\" : \" \u0642\u0637\u0629 \u0635\u063a\u064a\u0631\u0629\" , \"apple\" : \"\u062a\u0641\u0627\u062d\u0629\" , \"orange\" : \"\u0628\u0631\u062a\u0642\u0627\u0644\" , \"cabbage\" : \"\u0643\u0631\u0646\u0628\" , \"carrot\" : \"\u062c\u0632\u0631\u0629\" } lang_cv = BytePairLanguage ( \"ar\" ) arabic_words = list ( words . values ()) # before lang_cv [ translation ] . plot_similarity () # after lang_cv [ translation ] . pipe ( reverse_strings ) . plot_similarity ()","title":"Helpers"},{"location":"api/helpers/#reverse_strings","text":"This helper will reverse the strings in the embeddingset. This can be useful for making matplotlib plots with Arabic texts. This helper is meant to be used via EmbeddingSet.pipe() . Parameters Name Type Description Default embset EmbeddingSet to adapt required Usage: from whatlies.helpers import reverse_strings from whatlies.language import BytePairLanguage translation = { \"man\" : \"\u0631\u062c\u0644\" , \"woman\" : \"\u0627\u0645\u0631\u0623\u0629\" , \"king\" : \"\u0645\u0644\u0643\" , \"queen\" : \"\u0645\u0644\u0643\u0629\" , \"brother\" : \"\u0623\u062e\" , \"sister\" : \"\u0623\u062e\u062a\" , \"cat\" : \"\u0642\u0637\u0629\" , \"dog\" : \"\u0643\u0644\u0628\" , \"lion\" : \"\u0623\u0633\u062f\" , \"puppy\" : \"\u062c\u0631\u0648\" , \"male student\" : \"\u0637\u0627\u0644\u0628\" , \"female student\" : \"\u0637\u0627\u0644\u0628\u0629\" , \"university\" : \"\u062c\u0627\u0645\u0639\u0629\" , \"school\" : \"\u0645\u062f\u0631\u0633\u0629\" , \"kitten\" : \" \u0642\u0637\u0629 \u0635\u063a\u064a\u0631\u0629\" , \"apple\" : \"\u062a\u0641\u0627\u062d\u0629\" , \"orange\" : \"\u0628\u0631\u062a\u0642\u0627\u0644\" , \"cabbage\" : \"\u0643\u0631\u0646\u0628\" , \"carrot\" : \"\u062c\u0632\u0631\u0629\" } lang_cv = BytePairLanguage ( \"ar\" ) arabic_words = list ( words . values ()) # before lang_cv [ translation ] . plot_similarity () # after lang_cv [ translation ] . pipe ( reverse_strings ) . plot_similarity ()","title":"reverse_strings"},{"location":"api/language/bpemb_lang/","text":"whatlies.language.BytePairLanguage \u00b6 This object is used to lazily fetch Embedding s or EmbeddingSet s from a Byte-Pair Encoding backend. This object is meant for retreival, not plotting. This language represents token-free pre-trained subword embeddings. Originally created by Benjamin Heinzerling and Michael Strube. Important These vectors will auto-download by the BPEmb package . You can also specify \"multi\" to download multi language embeddings. A full list of available languages can be found here . The article that belongs to this work can be found here Recognition should be given to Benjamin Heinzerling and Michael Strube for making these available. The availability of vocabulary size as well as dimensionality can be varified on the project website. See here for an example link in English. Please credit the original authors if you use their work. Warning This class used to be called BytePairLang . Parameters Name Type Description Default lang name of the model to load required vs vocabulary size of the byte pair model 10000 dim the embedding dimensionality 100 cache_dir The folder in which downloaded BPEmb files will be cached PosixPath('/home/vincent/.cache/bpemb') Typically the vocabulary size given from this backend can be of size 1000, 3000, 5000, 10000, 25000, 50000, 100000 or 200000. The available dimensionality of the embbeddings typically are 25, 50, 100, 200 and 300. Usage : > from whatlies.language import BytePairLanguage > lang = BytePairLanguage ( lang = \"en\" ) > lang [ 'python' ] > lang = BytePairLanguage ( lang = \"multi\" ) > lang [[ 'hund' , 'hond' , 'dog' ]] __getitem__ ( self , item ) \u00b6 Show source code in language/_bpemblang.py 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 def __getitem__ ( self , item ): \"\"\" Retreive a single embedding or a set of embeddings. If an embedding contains multiple sub-tokens then we'll average them before retreival. Arguments: item: single string or list of strings **Usage** ```python > lang = BytePairLanguage(lang=\"en\") > lang['python'] > lang[['python', 'snake']] > lang[['nobody expects', 'the spanish inquisition']] ``` \"\"\" if isinstance ( item , str ): with warnings . catch_warnings (): warnings . filterwarnings ( \"ignore\" , category = RuntimeWarning ) return Embedding ( item , self . module . embed ( item ) . mean ( axis = 0 )) if isinstance ( item , list ): return EmbeddingSet ( * [ self [ i ] for i in item ]) raise ValueError ( f \"Item must be list of string got { item } .\" ) Retreive a single embedding or a set of embeddings. If an embedding contains multiple sub-tokens then we'll average them before retreival. Parameters Name Type Description Default item single string or list of strings required Usage > lang = BytePairLanguage ( lang = \"en\" ) > lang [ 'python' ] > lang [[ 'python' , 'snake' ]] > lang [[ 'nobody expects' , 'the spanish inquisition' ]] embset_similar ( self , emb , n = 10 , lower = False , metric = 'cosine' ) \u00b6 Show source code in language/_bpemblang.py 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 def embset_similar ( self , emb : Union [ str , Embedding ], n : int = 10 , lower = False , metric = \"cosine\" , ) -> EmbeddingSet : \"\"\" Retreive an [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] that are the most similar to the passed query. Arguments: emb: query to use n: the number of items you'd like to see returned metric: metric to use to calculate distance, must be scipy or sklearn compatible lower: only fetch lower case tokens Important: This method is incredibly slow at the moment without a good `top_n` setting due to [this bug](https://github.com/facebookresearch/fastText/issues/1040). Returns: An [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] containing the similar embeddings. \"\"\" embs = [ w [ 0 ] for w in self . score_similar ( emb = emb , n = n , lower = lower , metric = metric ) ] return EmbeddingSet ({ w . name : w for w in embs }) Retreive an EmbeddingSet that are the most similar to the passed query. Parameters Name Type Description Default emb Union[str, whatlies.embedding.Embedding] query to use required n int the number of items you'd like to see returned 10 metric metric to use to calculate distance, must be scipy or sklearn compatible 'cosine' lower only fetch lower case tokens False Important This method is incredibly slow at the moment without a good top_n setting due to this bug . Returns Type Description EmbeddingSet An EmbeddingSet containing the similar embeddings. score_similar ( self , emb , n = 10 , metric = 'cosine' , lower = False ) \u00b6 Show source code in language/_bpemblang.py 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 def score_similar ( self , emb : Union [ str , Embedding ], n : int = 10 , metric = \"cosine\" , lower = False , ) -> List : \"\"\" Retreive a list of (Embedding, score) tuples that are the most similar to the passed query. Arguments: emb: query to use n: the number of items you'd like to see returned metric: metric to use to calculate distance, must be scipy or sklearn compatible lower: only fetch lower case tokens Returns: An list of ([Embedding][whatlies.embedding.Embedding], score) tuples. \"\"\" if isinstance ( emb , str ): emb = self [ emb ] queries = self . _prepare_queries ( lower = lower ) distances = self . _calculate_distances ( emb = emb , queries = queries , metric = metric ) by_similarity = sorted ( zip ( queries , distances ), key = lambda z : z [ 1 ]) if len ( queries ) < n : warnings . warn ( f \"We could only find { len ( queries ) } feasible words. Consider changing `top_n` or `lower`\" , UserWarning , ) return [( self [ q ], float ( d )) for q , d in by_similarity [: n ]] Retreive a list of (Embedding, score) tuples that are the most similar to the passed query. Parameters Name Type Description Default emb Union[str, whatlies.embedding.Embedding] query to use required n int the number of items you'd like to see returned 10 metric metric to use to calculate distance, must be scipy or sklearn compatible 'cosine' lower only fetch lower case tokens False Returns Type Description List An list of ( Embedding , score) tuples.","title":"BytePair"},{"location":"api/language/bpemb_lang/#whatlieslanguagebytepairlanguage","text":"This object is used to lazily fetch Embedding s or EmbeddingSet s from a Byte-Pair Encoding backend. This object is meant for retreival, not plotting. This language represents token-free pre-trained subword embeddings. Originally created by Benjamin Heinzerling and Michael Strube. Important These vectors will auto-download by the BPEmb package . You can also specify \"multi\" to download multi language embeddings. A full list of available languages can be found here . The article that belongs to this work can be found here Recognition should be given to Benjamin Heinzerling and Michael Strube for making these available. The availability of vocabulary size as well as dimensionality can be varified on the project website. See here for an example link in English. Please credit the original authors if you use their work. Warning This class used to be called BytePairLang . Parameters Name Type Description Default lang name of the model to load required vs vocabulary size of the byte pair model 10000 dim the embedding dimensionality 100 cache_dir The folder in which downloaded BPEmb files will be cached PosixPath('/home/vincent/.cache/bpemb') Typically the vocabulary size given from this backend can be of size 1000, 3000, 5000, 10000, 25000, 50000, 100000 or 200000. The available dimensionality of the embbeddings typically are 25, 50, 100, 200 and 300. Usage : > from whatlies.language import BytePairLanguage > lang = BytePairLanguage ( lang = \"en\" ) > lang [ 'python' ] > lang = BytePairLanguage ( lang = \"multi\" ) > lang [[ 'hund' , 'hond' , 'dog' ]]","title":"whatlies.language.BytePairLanguage"},{"location":"api/language/bpemb_lang/#whatlies.language._bpemblang.BytePairLanguage.embset_similar","text":"Show source code in language/_bpemblang.py 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 def embset_similar ( self , emb : Union [ str , Embedding ], n : int = 10 , lower = False , metric = \"cosine\" , ) -> EmbeddingSet : \"\"\" Retreive an [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] that are the most similar to the passed query. Arguments: emb: query to use n: the number of items you'd like to see returned metric: metric to use to calculate distance, must be scipy or sklearn compatible lower: only fetch lower case tokens Important: This method is incredibly slow at the moment without a good `top_n` setting due to [this bug](https://github.com/facebookresearch/fastText/issues/1040). Returns: An [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] containing the similar embeddings. \"\"\" embs = [ w [ 0 ] for w in self . score_similar ( emb = emb , n = n , lower = lower , metric = metric ) ] return EmbeddingSet ({ w . name : w for w in embs }) Retreive an EmbeddingSet that are the most similar to the passed query. Parameters Name Type Description Default emb Union[str, whatlies.embedding.Embedding] query to use required n int the number of items you'd like to see returned 10 metric metric to use to calculate distance, must be scipy or sklearn compatible 'cosine' lower only fetch lower case tokens False Important This method is incredibly slow at the moment without a good top_n setting due to this bug . Returns Type Description EmbeddingSet An EmbeddingSet containing the similar embeddings.","title":"embset_similar()"},{"location":"api/language/bpemb_lang/#whatlies.language._bpemblang.BytePairLanguage.score_similar","text":"Show source code in language/_bpemblang.py 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 def score_similar ( self , emb : Union [ str , Embedding ], n : int = 10 , metric = \"cosine\" , lower = False , ) -> List : \"\"\" Retreive a list of (Embedding, score) tuples that are the most similar to the passed query. Arguments: emb: query to use n: the number of items you'd like to see returned metric: metric to use to calculate distance, must be scipy or sklearn compatible lower: only fetch lower case tokens Returns: An list of ([Embedding][whatlies.embedding.Embedding], score) tuples. \"\"\" if isinstance ( emb , str ): emb = self [ emb ] queries = self . _prepare_queries ( lower = lower ) distances = self . _calculate_distances ( emb = emb , queries = queries , metric = metric ) by_similarity = sorted ( zip ( queries , distances ), key = lambda z : z [ 1 ]) if len ( queries ) < n : warnings . warn ( f \"We could only find { len ( queries ) } feasible words. Consider changing `top_n` or `lower`\" , UserWarning , ) return [( self [ q ], float ( d )) for q , d in by_similarity [: n ]] Retreive a list of (Embedding, score) tuples that are the most similar to the passed query. Parameters Name Type Description Default emb Union[str, whatlies.embedding.Embedding] query to use required n int the number of items you'd like to see returned 10 metric metric to use to calculate distance, must be scipy or sklearn compatible 'cosine' lower only fetch lower case tokens False Returns Type Description List An list of ( Embedding , score) tuples.","title":"score_similar()"},{"location":"api/language/convert_lang/","text":"whatlies.language.ConveRTLanguage \u00b6 This object is used to fetch Embedding s or EmbeddingSet s from a ConveRT model. This object is meant for retreival, not plotting. Important This object will automatically download a large file if it is not cached yet. This language model does not contain a vocabulary, so it cannot be used to retreive similar tokens. Use an EmbeddingSet instead. This language backend might require you to manually install extra dependencies unless you installed via either; pip install whatlies[tfhub] pip install whatlies[all] Parameters Name Type Description Default model_id str identifier used for loading the corresponding TFHub module, we currently only allow 'convert' . 'convert' Usage : > from whatlies.language import ConveRTLanguage > lang = ConveRTLanguage () > lang [ 'bank' ] __getitem__ ( self , query ) \u00b6 Show source code in language/_convert_lang.py 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 def __getitem__ ( self , query : Union [ str , List [ str ]] ) -> Union [ Embedding , EmbeddingSet ]: \"\"\" Retreive a single embedding or a set of embeddings. Arguments: query: single string or list of strings **Usage** ```python > from whatlies.language import ConveRTLanguage > lang = ConveRTLanguage() > lang['bank'] > lang = ConveRTLanguage() > lang[['bank of the river', 'money on the bank', 'bank']] ``` \"\"\" if isinstance ( query , str ): query_tensor = tf . convert_to_tensor ([ query ]) encoding = self . model ( query_tensor ) if self . signature == \"encode_sequence\" : vec = encoding [ \"sequence_encoding\" ] . numpy () . sum ( axis = 1 )[ 0 ] else : vec = encoding [ \"default\" ] . numpy ()[ 0 ] return Embedding ( query , vec ) return EmbeddingSet ( * [ self [ tok ] for tok in query ]) Retreive a single embedding or a set of embeddings. Parameters Name Type Description Default query Union[str, List[str]] single string or list of strings required Usage > from whatlies.language import ConveRTLanguage > lang = ConveRTLanguage () > lang [ 'bank' ] > lang = ConveRTLanguage () > lang [[ 'bank of the river' , 'money on the bank' , 'bank' ]]","title":"ConveRT"},{"location":"api/language/convert_lang/#whatlieslanguageconvertlanguage","text":"This object is used to fetch Embedding s or EmbeddingSet s from a ConveRT model. This object is meant for retreival, not plotting. Important This object will automatically download a large file if it is not cached yet. This language model does not contain a vocabulary, so it cannot be used to retreive similar tokens. Use an EmbeddingSet instead. This language backend might require you to manually install extra dependencies unless you installed via either; pip install whatlies[tfhub] pip install whatlies[all] Parameters Name Type Description Default model_id str identifier used for loading the corresponding TFHub module, we currently only allow 'convert' . 'convert' Usage : > from whatlies.language import ConveRTLanguage > lang = ConveRTLanguage () > lang [ 'bank' ]","title":"whatlies.language.ConveRTLanguage"},{"location":"api/language/countvector_lang/","text":"whatlies.language.CountVectorLanguage \u00b6 This object is used to lazily fetch Embedding s or EmbeddingSet s from a countvector language backend. This object is meant for retreival, not plotting. This model will first train a scikit-learn CountVectorizer after which it will perform dimensionality reduction to make the numeric representation a vector. The reduction occurs via TruncatedSVD , also from scikit-learn. Warning This method does not implement a word embedding in the traditional sense. The interpretation needs to be altered. The information that is captured here only relates to the words/characters that are used in the text. There is no notion of meaning that should be suggested. Also, in order to keep this system consistent with the rest of the api you train the system when you retreive vectors if you just use __getitem__ . If you want to seperate train/test you need to call fit_manual yourself or use it in a scikit-learn pipeline. Parameters Name Type Description Default n_components int Number of components that TruncatedSVD will reduce to. required lowercase bool If the tokens need to be lowercased beforehand. True analyzer str Which analyzer to use, can be \"word\", \"char\", \"char_wb\". 'char' ngram_range Tuple[int, int] The range that specifies how many ngrams to use. (1, 2) min_df Union[int, float] Ignore terms that have a document frequency strictly lower than the given threshold. 1 max_df Union[int, float] Ignore terms that have a document frequency strictly higher than the given threshold. 1.0 binary bool Determines if the counts are binary or if they can accumulate. False strip_accents str Remove accents and perform normalisation. Can be set to \"ascii\" or \"unicode\". None random_state int Random state for SVD algorithm. 42 For more elaborate explainers on these arguments, check out the scikit-learn documentation . Usage : > from whatlies.language import CountVectorLanguage > lang = CountVectorLanguage ( n_components = 2 , ngram_range = ( 1 , 2 ), analyzer = \"char\" ) > lang [[ 'pizza' , 'pizzas' , 'firehouse' , 'firehydrant' ]] __getitem__ ( self , query ) \u00b6 Show source code in language/_countvector_lang.py 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 def __getitem__ ( self , query : Union [ str , List [ str ]]): \"\"\" Retreive a set of embeddings. Arguments: query: list of strings **Usage** ```python > from whatlies.language import CountVectorLanguage > lang = CountVectorLanguage(n_components=2, ngram_range=(1, 2), analyzer=\"char\") > lang[['pizza', 'pizzas', 'firehouse', 'firehydrant']] ``` \"\"\" orig_str = isinstance ( query , str ) if orig_str : query = [ query ] if any ([ len ( q ) == 0 for q in query ]): raise ValueError ( \"You've passed an empty string to the language model which is not allowed.\" ) if self . fitted_manual : X = self . cv . transform ( query ) X_vec = self . svd . transform ( X ) else : X = self . cv . fit_transform ( query ) X_vec = self . svd . fit_transform ( X ) if orig_str : return Embedding ( name = query [ 0 ], vector = X_vec [ 0 ]) return EmbeddingSet ( * [ Embedding ( name = n , vector = v ) for n , v in zip ( query , X_vec )] ) Retreive a set of embeddings. Parameters Name Type Description Default query Union[str, List[str]] list of strings required Usage > from whatlies.language import CountVectorLanguage > lang = CountVectorLanguage ( n_components = 2 , ngram_range = ( 1 , 2 ), analyzer = \"char\" ) > lang [[ 'pizza' , 'pizzas' , 'firehouse' , 'firehydrant' ]] embset_similar ( self , emb , n = 10 , lower = False , metric = 'cosine' ) \u00b6 Show source code in language/_countvector_lang.py 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 def embset_similar ( self , emb : Union [ str , Embedding ], n : int = 10 , lower = False , metric = \"cosine\" , ) -> EmbeddingSet : \"\"\" Retreive an [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] that are the most similar to the passed query. Note that we will only consider words that were passed in the `.fit_manual()` step. Arguments: emb: query to use n: the number of items you'd like to see returned metric: metric to use to calculate distance, must be scipy or sklearn compatible lower: only fetch lower case tokens Returns: An [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] containing the similar embeddings. \"\"\" embs = [ w [ 0 ] for w in self . score_similar ( emb = emb , n = n , lower = lower , metric = metric ) ] return EmbeddingSet ({ w . name : w for w in embs }) Retreive an EmbeddingSet that are the most similar to the passed query. Note that we will only consider words that were passed in the .fit_manual() step. Parameters Name Type Description Default emb Union[str, whatlies.embedding.Embedding] query to use required n int the number of items you'd like to see returned 10 metric metric to use to calculate distance, must be scipy or sklearn compatible 'cosine' lower only fetch lower case tokens False Returns Type Description EmbeddingSet An EmbeddingSet containing the similar embeddings. fit_manual ( self , query ) \u00b6 Show source code in language/_countvector_lang.py 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 def fit_manual ( self , query ): \"\"\" Fit the model manually. This way you can call `__getitem__` independantly of training. Arguments: query: list of strings **Usage** ```python > from whatlies.language import CountVectorLanguage > lang = CountVectorLanguage(n_components=2, ngram_range=(1, 2), analyzer=\"char\") > lang.fit_manual(['pizza', 'pizzas', 'firehouse', 'firehydrant']) > lang[['piza', 'pizza', 'pizzaz', 'fyrehouse', 'firehouse', 'fyrehidrant']] ``` \"\"\" if any ([ len ( q ) == 0 for q in query ]): raise ValueError ( \"You've passed an empty string to the language model which is not allowed.\" ) X = self . cv . fit_transform ( query ) self . svd . fit ( X ) self . fitted_manual = True self . corpus = query return self Fit the model manually. This way you can call __getitem__ independantly of training. Parameters Name Type Description Default query list of strings required Usage > from whatlies.language import CountVectorLanguage > lang = CountVectorLanguage ( n_components = 2 , ngram_range = ( 1 , 2 ), analyzer = \"char\" ) > lang . fit_manual ([ 'pizza' , 'pizzas' , 'firehouse' , 'firehydrant' ]) > lang [[ 'piza' , 'pizza' , 'pizzaz' , 'fyrehouse' , 'firehouse' , 'fyrehidrant' ]] score_similar ( self , emb , n = 10 , metric = 'cosine' , lower = False ) \u00b6 Show source code in language/_countvector_lang.py 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 def score_similar ( self , emb : Union [ str , Embedding ], n : int = 10 , metric = \"cosine\" , lower = False , ) -> List : \"\"\" Retreive a list of (Embedding, score) tuples that are the most similar to the passed query. Note that we will only consider words that were passed in the `.fit_manual()` step. Arguments: emb: query to use n: the number of items you'd like to see returned metric: metric to use to calculate distance, must be scipy or sklearn compatible lower: only fetch lower case tokens Returns: An list of ([Embedding][whatlies.embedding.Embedding], score) tuples. \"\"\" if isinstance ( emb , str ): emb = self [ emb ] queries = self . _prepare_queries ( lower = lower ) distances = self . _calculate_distances ( emb = emb , queries = queries , metric = metric ) by_similarity = sorted ( zip ( queries , distances ), key = lambda z : z [ 1 ]) if len ( self . corpus ) < n : raise ValueError ( f \"You're trying to retreive { n } items while the corpus only trained on { len ( self . corpus ) } .\" ) if len ( queries ) < n : warnings . warn ( f \"We could only find { len ( queries ) } feasible words. Consider changing `top_n` or `lower`\" , UserWarning , ) return [( self [ q ], float ( d )) for q , d in by_similarity [: n ]] Retreive a list of (Embedding, score) tuples that are the most similar to the passed query. Note that we will only consider words that were passed in the .fit_manual() step. Parameters Name Type Description Default emb Union[str, whatlies.embedding.Embedding] query to use required n int the number of items you'd like to see returned 10 metric metric to use to calculate distance, must be scipy or sklearn compatible 'cosine' lower only fetch lower case tokens False Returns Type Description List An list of ( Embedding , score) tuples.","title":"CountVector"},{"location":"api/language/countvector_lang/#whatlieslanguagecountvectorlanguage","text":"This object is used to lazily fetch Embedding s or EmbeddingSet s from a countvector language backend. This object is meant for retreival, not plotting. This model will first train a scikit-learn CountVectorizer after which it will perform dimensionality reduction to make the numeric representation a vector. The reduction occurs via TruncatedSVD , also from scikit-learn. Warning This method does not implement a word embedding in the traditional sense. The interpretation needs to be altered. The information that is captured here only relates to the words/characters that are used in the text. There is no notion of meaning that should be suggested. Also, in order to keep this system consistent with the rest of the api you train the system when you retreive vectors if you just use __getitem__ . If you want to seperate train/test you need to call fit_manual yourself or use it in a scikit-learn pipeline. Parameters Name Type Description Default n_components int Number of components that TruncatedSVD will reduce to. required lowercase bool If the tokens need to be lowercased beforehand. True analyzer str Which analyzer to use, can be \"word\", \"char\", \"char_wb\". 'char' ngram_range Tuple[int, int] The range that specifies how many ngrams to use. (1, 2) min_df Union[int, float] Ignore terms that have a document frequency strictly lower than the given threshold. 1 max_df Union[int, float] Ignore terms that have a document frequency strictly higher than the given threshold. 1.0 binary bool Determines if the counts are binary or if they can accumulate. False strip_accents str Remove accents and perform normalisation. Can be set to \"ascii\" or \"unicode\". None random_state int Random state for SVD algorithm. 42 For more elaborate explainers on these arguments, check out the scikit-learn documentation . Usage : > from whatlies.language import CountVectorLanguage > lang = CountVectorLanguage ( n_components = 2 , ngram_range = ( 1 , 2 ), analyzer = \"char\" ) > lang [[ 'pizza' , 'pizzas' , 'firehouse' , 'firehydrant' ]]","title":"whatlies.language.CountVectorLanguage"},{"location":"api/language/countvector_lang/#whatlies.language._countvector_lang.CountVectorLanguage.embset_similar","text":"Show source code in language/_countvector_lang.py 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 def embset_similar ( self , emb : Union [ str , Embedding ], n : int = 10 , lower = False , metric = \"cosine\" , ) -> EmbeddingSet : \"\"\" Retreive an [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] that are the most similar to the passed query. Note that we will only consider words that were passed in the `.fit_manual()` step. Arguments: emb: query to use n: the number of items you'd like to see returned metric: metric to use to calculate distance, must be scipy or sklearn compatible lower: only fetch lower case tokens Returns: An [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] containing the similar embeddings. \"\"\" embs = [ w [ 0 ] for w in self . score_similar ( emb = emb , n = n , lower = lower , metric = metric ) ] return EmbeddingSet ({ w . name : w for w in embs }) Retreive an EmbeddingSet that are the most similar to the passed query. Note that we will only consider words that were passed in the .fit_manual() step. Parameters Name Type Description Default emb Union[str, whatlies.embedding.Embedding] query to use required n int the number of items you'd like to see returned 10 metric metric to use to calculate distance, must be scipy or sklearn compatible 'cosine' lower only fetch lower case tokens False Returns Type Description EmbeddingSet An EmbeddingSet containing the similar embeddings.","title":"embset_similar()"},{"location":"api/language/countvector_lang/#whatlies.language._countvector_lang.CountVectorLanguage.fit_manual","text":"Show source code in language/_countvector_lang.py 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 def fit_manual ( self , query ): \"\"\" Fit the model manually. This way you can call `__getitem__` independantly of training. Arguments: query: list of strings **Usage** ```python > from whatlies.language import CountVectorLanguage > lang = CountVectorLanguage(n_components=2, ngram_range=(1, 2), analyzer=\"char\") > lang.fit_manual(['pizza', 'pizzas', 'firehouse', 'firehydrant']) > lang[['piza', 'pizza', 'pizzaz', 'fyrehouse', 'firehouse', 'fyrehidrant']] ``` \"\"\" if any ([ len ( q ) == 0 for q in query ]): raise ValueError ( \"You've passed an empty string to the language model which is not allowed.\" ) X = self . cv . fit_transform ( query ) self . svd . fit ( X ) self . fitted_manual = True self . corpus = query return self Fit the model manually. This way you can call __getitem__ independantly of training. Parameters Name Type Description Default query list of strings required Usage > from whatlies.language import CountVectorLanguage > lang = CountVectorLanguage ( n_components = 2 , ngram_range = ( 1 , 2 ), analyzer = \"char\" ) > lang . fit_manual ([ 'pizza' , 'pizzas' , 'firehouse' , 'firehydrant' ]) > lang [[ 'piza' , 'pizza' , 'pizzaz' , 'fyrehouse' , 'firehouse' , 'fyrehidrant' ]]","title":"fit_manual()"},{"location":"api/language/countvector_lang/#whatlies.language._countvector_lang.CountVectorLanguage.score_similar","text":"Show source code in language/_countvector_lang.py 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 def score_similar ( self , emb : Union [ str , Embedding ], n : int = 10 , metric = \"cosine\" , lower = False , ) -> List : \"\"\" Retreive a list of (Embedding, score) tuples that are the most similar to the passed query. Note that we will only consider words that were passed in the `.fit_manual()` step. Arguments: emb: query to use n: the number of items you'd like to see returned metric: metric to use to calculate distance, must be scipy or sklearn compatible lower: only fetch lower case tokens Returns: An list of ([Embedding][whatlies.embedding.Embedding], score) tuples. \"\"\" if isinstance ( emb , str ): emb = self [ emb ] queries = self . _prepare_queries ( lower = lower ) distances = self . _calculate_distances ( emb = emb , queries = queries , metric = metric ) by_similarity = sorted ( zip ( queries , distances ), key = lambda z : z [ 1 ]) if len ( self . corpus ) < n : raise ValueError ( f \"You're trying to retreive { n } items while the corpus only trained on { len ( self . corpus ) } .\" ) if len ( queries ) < n : warnings . warn ( f \"We could only find { len ( queries ) } feasible words. Consider changing `top_n` or `lower`\" , UserWarning , ) return [( self [ q ], float ( d )) for q , d in by_similarity [: n ]] Retreive a list of (Embedding, score) tuples that are the most similar to the passed query. Note that we will only consider words that were passed in the .fit_manual() step. Parameters Name Type Description Default emb Union[str, whatlies.embedding.Embedding] query to use required n int the number of items you'd like to see returned 10 metric metric to use to calculate distance, must be scipy or sklearn compatible 'cosine' lower only fetch lower case tokens False Returns Type Description List An list of ( Embedding , score) tuples.","title":"score_similar()"},{"location":"api/language/diet/","text":"whatlies.language.DIETLanguage \u00b6 ::: whatlies.language.DIETLanguage","title":"`whatlies.language.DIETLanguage`"},{"location":"api/language/diet/#whatlieslanguagedietlanguage","text":"::: whatlies.language.DIETLanguage","title":"whatlies.language.DIETLanguage"},{"location":"api/language/fasttext_lang/","text":"whatlies.language.FasttextLanguage \u00b6 This object is used to lazily fetch Embedding s or EmbeddingSet s from a fasttext language backend. This object is meant for retreival, not plotting. Important The vectors are not given by this library they must be downloaded upfront. You can find the download links here . Note: you'll want the bin file, not the text file. To train your own fasttext model see the guide here . This language backend might require you to manually install extra dependencies unless you installed via either; pip install whatlies[fasttext] pip install whatlies[all] Warning You could theoretically use fasttext to train your own models with this code; > import fasttext > model = fasttext.train_unsupervised('data.txt', model='cbow', dim=10) > model = fasttext.train_unsupervised('data.txt', model='skipgram', dim=20, epoch=20, lr=0.1, min_count=1) > lang = FasttextLanguage(model) > lang['python'] > model.save_model(\"result/data-skipgram-20.bin\") > lang = FasttextLanguage(\"result/data-skipgram-20.bin\") But you need to be aware that the fasttext library from facebook has gone stale. Last update on pypi was June 2019. Our preferred usecase for it is to use the pretrained vectors. Note that you can also import these via spaCy but this requires a packaging step. Parameters Name Type Description Default model name of the model to load, be sure that it's downloaded or trained beforehand required Usage : > from whatlies.language import FasttextLanguage > lang = FasttextLanguage ( \"cc.en.300.bin\" ) > lang [ 'python' ] > lang = FasttextLanguage ( \"cc.en.300.bin\" , size = 10 ) > lang [[ 'python' , 'snake' , 'dog' ]] __getitem__ ( self , query ) \u00b6 Show source code in language/_fasttext_lang.py 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 def __getitem__ ( self , query : Union [ str , List [ str ]]): \"\"\" Retreive a single embedding or a set of embeddings. Depending on the spaCy model the strings can support multiple tokens of text but they can also use the Bert DSL. See the Language Options documentation: https://rasahq.github.io/whatlies/tutorial/languages/#bert-style. Arguments: query: single string or list of strings **Usage** ```python > lang = FasttextLanguage(\"cc.en.300.bin\") > lang['python'] > lang[['python'], ['snake']] > lang[['nobody expects'], ['the spanish inquisition']] ``` \"\"\" if isinstance ( query , str ): self . _input_str_legal ( query ) vec = self . model . get_word_vector ( query ) return Embedding ( query , vec ) return EmbeddingSet ( * [ self [ tok ] for tok in query ]) Retreive a single embedding or a set of embeddings. Depending on the spaCy model the strings can support multiple tokens of text but they can also use the Bert DSL. See the Language Options documentation: https://rasahq.github.io/whatlies/tutorial/languages/#bert-style. Parameters Name Type Description Default query Union[str, List[str]] single string or list of strings required Usage > lang = FasttextLanguage ( \"cc.en.300.bin\" ) > lang [ 'python' ] > lang [[ 'python' ], [ 'snake' ]] > lang [[ 'nobody expects' ], [ 'the spanish inquisition' ]] embset_proximity ( self , emb , max_proximity = 0.1 , top_n = 20000 , lower = True , metric = 'cosine' ) \u00b6 Show source code in language/_fasttext_lang.py 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 def embset_proximity ( self , emb : Union [ str , Embedding ], max_proximity : float = 0.1 , top_n = 20_000 , lower = True , metric = \"cosine\" , ): \"\"\" Retreive an [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] or embeddings that are within a proximity. Arguments: emb: query to use max_proximity: the number of items you'd like to see returned top_n: likelihood limit that sets the subset of words to search metric: metric to use to calculate distance, must be scipy or sklearn compatible lower: only fetch lower case tokens Returns: An [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] containing the similar embeddings. \"\"\" if isinstance ( emb , str ): emb = self [ emb ] queries = self . _prepare_queries ( top_n , lower ) distances = self . _calculate_distances ( emb , queries , metric ) return EmbeddingSet ( { w : self [ w ] for w , d in zip ( queries , distances ) if d <= max_proximity } ) Retreive an EmbeddingSet or embeddings that are within a proximity. Parameters Name Type Description Default emb Union[str, whatlies.embedding.Embedding] query to use required max_proximity float the number of items you'd like to see returned 0.1 top_n likelihood limit that sets the subset of words to search 20000 metric metric to use to calculate distance, must be scipy or sklearn compatible 'cosine' lower only fetch lower case tokens True Returns Type Description `` An EmbeddingSet containing the similar embeddings. embset_similar ( self , emb , n = 10 , top_n = 20000 , lower = False , metric = 'cosine' ) \u00b6 Show source code in language/_fasttext_lang.py 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 def embset_similar ( self , emb : Union [ str , Embedding ], n : int = 10 , top_n = 20_000 , lower = False , metric = \"cosine\" , ): \"\"\" Retreive an [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] that are the most similar to the passed query. Arguments: emb: query to use n: the number of items you'd like to see returned top_n: likelihood limit that sets the subset of words to search metric: metric to use to calculate distance, must be scipy or sklearn compatible lower: only fetch lower case tokens, note that the official english model only has lower case tokens Important: This method is incredibly slow at the moment without a good `top_n` setting due to [this bug](https://github.com/facebookresearch/fastText/issues/1040). Returns: An [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] containing the similar embeddings. \"\"\" embs = [ w [ 0 ] for w in self . score_similar ( emb , n , top_n , lower , metric )] return EmbeddingSet ({ w . name : w for w in embs }) Retreive an EmbeddingSet that are the most similar to the passed query. Parameters Name Type Description Default emb Union[str, whatlies.embedding.Embedding] query to use required n int the number of items you'd like to see returned 10 top_n likelihood limit that sets the subset of words to search 20000 metric metric to use to calculate distance, must be scipy or sklearn compatible 'cosine' lower only fetch lower case tokens, note that the official english model only has lower case tokens False Important This method is incredibly slow at the moment without a good top_n setting due to this bug . Returns Type Description `` An EmbeddingSet containing the similar embeddings. score_similar ( self , emb , n = 10 , top_n = 20000 , lower = False , metric = 'cosine' ) \u00b6 Show source code in language/_fasttext_lang.py 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 def score_similar ( self , emb : Union [ str , Embedding ], n : int = 10 , top_n = 20_000 , lower = False , metric = \"cosine\" , ): \"\"\" Retreive a list of (Embedding, score) tuples that are the most similar to the passed query. Arguments: emb: query to use n: the number of items you'd like to see returned top_n: likelihood limit that sets the subset of words to search, to ignore set to `None` metric: metric to use to calculate distance, must be scipy or sklearn compatible lower: only fetch lower case tokens, note that the official english model only has lower case tokens Important: This method is incredibly slow at the moment without a good `top_n` setting due to [this bug](https://github.com/facebookresearch/fastText/issues/1040). Returns: An list of ([Embedding][whatlies.embedding.Embedding], score) tuples. \"\"\" if isinstance ( emb , str ): emb = self [ emb ] queries = self . _prepare_queries ( top_n , lower ) distances = self . _calculate_distances ( emb , queries , metric ) by_similarity = sorted ( zip ( queries , distances ), key = lambda z : z [ 1 ]) if len ( queries ) < n : warnings . warn ( f \"We could only find { len ( queries ) } feasible words. Consider changing `top_n` or `lower`\" , UserWarning , ) return [( self [ q ], float ( d )) for q , d in by_similarity [: n ]] Retreive a list of (Embedding, score) tuples that are the most similar to the passed query. Parameters Name Type Description Default emb Union[str, whatlies.embedding.Embedding] query to use required n int the number of items you'd like to see returned 10 top_n likelihood limit that sets the subset of words to search, to ignore set to None 20000 metric metric to use to calculate distance, must be scipy or sklearn compatible 'cosine' lower only fetch lower case tokens, note that the official english model only has lower case tokens False Important This method is incredibly slow at the moment without a good top_n setting due to this bug . Returns Type Description `` An list of ( Embedding , score) tuples.","title":"fasttext"},{"location":"api/language/fasttext_lang/#whatlieslanguagefasttextlanguage","text":"This object is used to lazily fetch Embedding s or EmbeddingSet s from a fasttext language backend. This object is meant for retreival, not plotting. Important The vectors are not given by this library they must be downloaded upfront. You can find the download links here . Note: you'll want the bin file, not the text file. To train your own fasttext model see the guide here . This language backend might require you to manually install extra dependencies unless you installed via either; pip install whatlies[fasttext] pip install whatlies[all] Warning You could theoretically use fasttext to train your own models with this code; > import fasttext > model = fasttext.train_unsupervised('data.txt', model='cbow', dim=10) > model = fasttext.train_unsupervised('data.txt', model='skipgram', dim=20, epoch=20, lr=0.1, min_count=1) > lang = FasttextLanguage(model) > lang['python'] > model.save_model(\"result/data-skipgram-20.bin\") > lang = FasttextLanguage(\"result/data-skipgram-20.bin\") But you need to be aware that the fasttext library from facebook has gone stale. Last update on pypi was June 2019. Our preferred usecase for it is to use the pretrained vectors. Note that you can also import these via spaCy but this requires a packaging step. Parameters Name Type Description Default model name of the model to load, be sure that it's downloaded or trained beforehand required Usage : > from whatlies.language import FasttextLanguage > lang = FasttextLanguage ( \"cc.en.300.bin\" ) > lang [ 'python' ] > lang = FasttextLanguage ( \"cc.en.300.bin\" , size = 10 ) > lang [[ 'python' , 'snake' , 'dog' ]]","title":"whatlies.language.FasttextLanguage"},{"location":"api/language/fasttext_lang/#whatlies.language._fasttext_lang.FasttextLanguage.embset_proximity","text":"Show source code in language/_fasttext_lang.py 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 def embset_proximity ( self , emb : Union [ str , Embedding ], max_proximity : float = 0.1 , top_n = 20_000 , lower = True , metric = \"cosine\" , ): \"\"\" Retreive an [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] or embeddings that are within a proximity. Arguments: emb: query to use max_proximity: the number of items you'd like to see returned top_n: likelihood limit that sets the subset of words to search metric: metric to use to calculate distance, must be scipy or sklearn compatible lower: only fetch lower case tokens Returns: An [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] containing the similar embeddings. \"\"\" if isinstance ( emb , str ): emb = self [ emb ] queries = self . _prepare_queries ( top_n , lower ) distances = self . _calculate_distances ( emb , queries , metric ) return EmbeddingSet ( { w : self [ w ] for w , d in zip ( queries , distances ) if d <= max_proximity } ) Retreive an EmbeddingSet or embeddings that are within a proximity. Parameters Name Type Description Default emb Union[str, whatlies.embedding.Embedding] query to use required max_proximity float the number of items you'd like to see returned 0.1 top_n likelihood limit that sets the subset of words to search 20000 metric metric to use to calculate distance, must be scipy or sklearn compatible 'cosine' lower only fetch lower case tokens True Returns Type Description `` An EmbeddingSet containing the similar embeddings.","title":"embset_proximity()"},{"location":"api/language/fasttext_lang/#whatlies.language._fasttext_lang.FasttextLanguage.embset_similar","text":"Show source code in language/_fasttext_lang.py 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 def embset_similar ( self , emb : Union [ str , Embedding ], n : int = 10 , top_n = 20_000 , lower = False , metric = \"cosine\" , ): \"\"\" Retreive an [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] that are the most similar to the passed query. Arguments: emb: query to use n: the number of items you'd like to see returned top_n: likelihood limit that sets the subset of words to search metric: metric to use to calculate distance, must be scipy or sklearn compatible lower: only fetch lower case tokens, note that the official english model only has lower case tokens Important: This method is incredibly slow at the moment without a good `top_n` setting due to [this bug](https://github.com/facebookresearch/fastText/issues/1040). Returns: An [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] containing the similar embeddings. \"\"\" embs = [ w [ 0 ] for w in self . score_similar ( emb , n , top_n , lower , metric )] return EmbeddingSet ({ w . name : w for w in embs }) Retreive an EmbeddingSet that are the most similar to the passed query. Parameters Name Type Description Default emb Union[str, whatlies.embedding.Embedding] query to use required n int the number of items you'd like to see returned 10 top_n likelihood limit that sets the subset of words to search 20000 metric metric to use to calculate distance, must be scipy or sklearn compatible 'cosine' lower only fetch lower case tokens, note that the official english model only has lower case tokens False Important This method is incredibly slow at the moment without a good top_n setting due to this bug . Returns Type Description `` An EmbeddingSet containing the similar embeddings.","title":"embset_similar()"},{"location":"api/language/fasttext_lang/#whatlies.language._fasttext_lang.FasttextLanguage.score_similar","text":"Show source code in language/_fasttext_lang.py 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 def score_similar ( self , emb : Union [ str , Embedding ], n : int = 10 , top_n = 20_000 , lower = False , metric = \"cosine\" , ): \"\"\" Retreive a list of (Embedding, score) tuples that are the most similar to the passed query. Arguments: emb: query to use n: the number of items you'd like to see returned top_n: likelihood limit that sets the subset of words to search, to ignore set to `None` metric: metric to use to calculate distance, must be scipy or sklearn compatible lower: only fetch lower case tokens, note that the official english model only has lower case tokens Important: This method is incredibly slow at the moment without a good `top_n` setting due to [this bug](https://github.com/facebookresearch/fastText/issues/1040). Returns: An list of ([Embedding][whatlies.embedding.Embedding], score) tuples. \"\"\" if isinstance ( emb , str ): emb = self [ emb ] queries = self . _prepare_queries ( top_n , lower ) distances = self . _calculate_distances ( emb , queries , metric ) by_similarity = sorted ( zip ( queries , distances ), key = lambda z : z [ 1 ]) if len ( queries ) < n : warnings . warn ( f \"We could only find { len ( queries ) } feasible words. Consider changing `top_n` or `lower`\" , UserWarning , ) return [( self [ q ], float ( d )) for q , d in by_similarity [: n ]] Retreive a list of (Embedding, score) tuples that are the most similar to the passed query. Parameters Name Type Description Default emb Union[str, whatlies.embedding.Embedding] query to use required n int the number of items you'd like to see returned 10 top_n likelihood limit that sets the subset of words to search, to ignore set to None 20000 metric metric to use to calculate distance, must be scipy or sklearn compatible 'cosine' lower only fetch lower case tokens, note that the official english model only has lower case tokens False Important This method is incredibly slow at the moment without a good top_n setting due to this bug . Returns Type Description `` An list of ( Embedding , score) tuples.","title":"score_similar()"},{"location":"api/language/floret_lang/","text":"whatlies.language.FloretLanguage \u00b6 This object is used to lazily fetch Embedding s or EmbeddingSet s from a floret language backend. Important The vectors are not given by this library they must be on disk upfront. To train your own floret vectors see the guide here . In short, you can train your model via; import floret model = floret . train_unsupervised ( \"data.txt\" ) model . save_model ( \"vectors.bin\" ) This language backend might require you to manually install extra dependencies unless you installed via either; pip install whatlies[floret] pip install whatlies[all] Parameters Name Type Description Default path path to the vectors on disk, be sure that it's on disk beforehand required Usage : > from whatlies.language import FloretLanguage > lang = FasttextLanguage ( \"cc.en.300.bin\" ) > lang [ 'python' ] > lang = FasttextLanguage ( \"cc.en.300.bin\" , size = 10 ) > lang [[ 'python' , 'snake' , 'dog' ]] __getitem__ ( self , query ) \u00b6 Show source code in language/_floret_lang.py 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 def __getitem__ ( self , query : Union [ str , List [ str ]]): \"\"\" Retreive a single embedding or a set of embeddings. Depending on the spaCy model the strings can support multiple tokens of text but they can also use the Bert DSL. See the Language Options documentation: https://rasahq.github.io/whatlies/tutorial/languages/#bert-style. Arguments: query: single string or list of strings **Usage** ```python > lang = FasttextLanguage(\"cc.en.300.bin\") > lang['python'] > lang[['python'], ['snake']] > lang[['nobody expects'], ['the spanish inquisition']] ``` \"\"\" if isinstance ( query , str ): vec = self . model . get_word_vector ( query ) return Embedding ( query , vec ) return EmbeddingSet ( * [ self [ tok ] for tok in query ]) Retreive a single embedding or a set of embeddings. Depending on the spaCy model the strings can support multiple tokens of text but they can also use the Bert DSL. See the Language Options documentation: https://rasahq.github.io/whatlies/tutorial/languages/#bert-style. Parameters Name Type Description Default query Union[str, List[str]] single string or list of strings required Usage > lang = FasttextLanguage ( \"cc.en.300.bin\" ) > lang [ 'python' ] > lang [[ 'python' ], [ 'snake' ]] > lang [[ 'nobody expects' ], [ 'the spanish inquisition' ]] embset_proximity ( self , emb , max_proximity = 0.1 , top_n = 20000 , lower = True , metric = 'cosine' ) \u00b6 Show source code in language/_floret_lang.py 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 def embset_proximity ( self , emb : Union [ str , Embedding ], max_proximity : float = 0.1 , top_n = 20_000 , lower = True , metric = \"cosine\" , ): \"\"\" Retreive an [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] or embeddings that are within a proximity. Arguments: emb: query to use max_proximity: the number of items you'd like to see returned top_n: likelihood limit that sets the subset of words to search metric: metric to use to calculate distance, must be scipy or sklearn compatible lower: only fetch lower case tokens Returns: An [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] containing the similar embeddings. \"\"\" if isinstance ( emb , str ): emb = self [ emb ] queries = self . _prepare_queries ( top_n , lower ) distances = self . _calculate_distances ( emb , queries , metric ) return EmbeddingSet ( { w : self [ w ] for w , d in zip ( queries , distances ) if d <= max_proximity } ) Retreive an EmbeddingSet or embeddings that are within a proximity. Parameters Name Type Description Default emb Union[str, whatlies.embedding.Embedding] query to use required max_proximity float the number of items you'd like to see returned 0.1 top_n likelihood limit that sets the subset of words to search 20000 metric metric to use to calculate distance, must be scipy or sklearn compatible 'cosine' lower only fetch lower case tokens True Returns Type Description `` An EmbeddingSet containing the similar embeddings. embset_similar ( self , emb , n = 10 , top_n = 20000 , lower = False , metric = 'cosine' ) \u00b6 Show source code in language/_floret_lang.py 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 def embset_similar ( self , emb : Union [ str , Embedding ], n : int = 10 , top_n = 20_000 , lower = False , metric = \"cosine\" , ): \"\"\" Retreive an [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] that are the most similar to the passed query. Arguments: emb: query to use n: the number of items you'd like to see returned top_n: likelihood limit that sets the subset of words to search metric: metric to use to calculate distance, must be scipy or sklearn compatible lower: only fetch lower case tokens, note that the official english model only has lower case tokens Important: This method is incredibly slow at the moment without a good `top_n` setting due to [this bug](https://github.com/facebookresearch/fastText/issues/1040). Returns: An [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] containing the similar embeddings. \"\"\" embs = [ w [ 0 ] for w in self . score_similar ( emb , n , top_n , lower , metric )] return EmbeddingSet ({ w . name : w for w in embs }) Retreive an EmbeddingSet that are the most similar to the passed query. Parameters Name Type Description Default emb Union[str, whatlies.embedding.Embedding] query to use required n int the number of items you'd like to see returned 10 top_n likelihood limit that sets the subset of words to search 20000 metric metric to use to calculate distance, must be scipy or sklearn compatible 'cosine' lower only fetch lower case tokens, note that the official english model only has lower case tokens False Important This method is incredibly slow at the moment without a good top_n setting due to this bug . Returns Type Description `` An EmbeddingSet containing the similar embeddings. score_similar ( self , emb , n = 10 , top_n = 20000 , lower = False , metric = 'cosine' ) \u00b6 Show source code in language/_floret_lang.py 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 def score_similar ( self , emb : Union [ str , Embedding ], n : int = 10 , top_n = 20_000 , lower = False , metric = \"cosine\" , ): \"\"\" Retreive a list of (Embedding, score) tuples that are the most similar to the passed query. Arguments: emb: query to use n: the number of items you'd like to see returned top_n: likelihood limit that sets the subset of words to search, to ignore set to `None` metric: metric to use to calculate distance, must be scipy or sklearn compatible lower: only fetch lower case tokens, note that the official english model only has lower case tokens Important: This method is incredibly slow at the moment without a good `top_n` setting due to [this bug](https://github.com/facebookresearch/fastText/issues/1040). Returns: An list of ([Embedding][whatlies.embedding.Embedding], score) tuples. \"\"\" if isinstance ( emb , str ): emb = self [ emb ] queries = self . _prepare_queries ( top_n , lower ) distances = self . _calculate_distances ( emb , queries , metric ) by_similarity = sorted ( zip ( queries , distances ), key = lambda z : z [ 1 ]) if len ( queries ) < n : warnings . warn ( f \"We could only find { len ( queries ) } feasible words. Consider changing `top_n` or `lower`\" , UserWarning , ) return [( self [ q ], float ( d )) for q , d in by_similarity [: n ]] Retreive a list of (Embedding, score) tuples that are the most similar to the passed query. Parameters Name Type Description Default emb Union[str, whatlies.embedding.Embedding] query to use required n int the number of items you'd like to see returned 10 top_n likelihood limit that sets the subset of words to search, to ignore set to None 20000 metric metric to use to calculate distance, must be scipy or sklearn compatible 'cosine' lower only fetch lower case tokens, note that the official english model only has lower case tokens False Important This method is incredibly slow at the moment without a good top_n setting due to this bug . Returns Type Description `` An list of ( Embedding , score) tuples.","title":"floret"},{"location":"api/language/floret_lang/#whatlieslanguagefloretlanguage","text":"This object is used to lazily fetch Embedding s or EmbeddingSet s from a floret language backend. Important The vectors are not given by this library they must be on disk upfront. To train your own floret vectors see the guide here . In short, you can train your model via; import floret model = floret . train_unsupervised ( \"data.txt\" ) model . save_model ( \"vectors.bin\" ) This language backend might require you to manually install extra dependencies unless you installed via either; pip install whatlies[floret] pip install whatlies[all] Parameters Name Type Description Default path path to the vectors on disk, be sure that it's on disk beforehand required Usage : > from whatlies.language import FloretLanguage > lang = FasttextLanguage ( \"cc.en.300.bin\" ) > lang [ 'python' ] > lang = FasttextLanguage ( \"cc.en.300.bin\" , size = 10 ) > lang [[ 'python' , 'snake' , 'dog' ]]","title":"whatlies.language.FloretLanguage"},{"location":"api/language/floret_lang/#whatlies.language._floret_lang.FloretLanguage.embset_proximity","text":"Show source code in language/_floret_lang.py 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 def embset_proximity ( self , emb : Union [ str , Embedding ], max_proximity : float = 0.1 , top_n = 20_000 , lower = True , metric = \"cosine\" , ): \"\"\" Retreive an [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] or embeddings that are within a proximity. Arguments: emb: query to use max_proximity: the number of items you'd like to see returned top_n: likelihood limit that sets the subset of words to search metric: metric to use to calculate distance, must be scipy or sklearn compatible lower: only fetch lower case tokens Returns: An [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] containing the similar embeddings. \"\"\" if isinstance ( emb , str ): emb = self [ emb ] queries = self . _prepare_queries ( top_n , lower ) distances = self . _calculate_distances ( emb , queries , metric ) return EmbeddingSet ( { w : self [ w ] for w , d in zip ( queries , distances ) if d <= max_proximity } ) Retreive an EmbeddingSet or embeddings that are within a proximity. Parameters Name Type Description Default emb Union[str, whatlies.embedding.Embedding] query to use required max_proximity float the number of items you'd like to see returned 0.1 top_n likelihood limit that sets the subset of words to search 20000 metric metric to use to calculate distance, must be scipy or sklearn compatible 'cosine' lower only fetch lower case tokens True Returns Type Description `` An EmbeddingSet containing the similar embeddings.","title":"embset_proximity()"},{"location":"api/language/floret_lang/#whatlies.language._floret_lang.FloretLanguage.embset_similar","text":"Show source code in language/_floret_lang.py 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 def embset_similar ( self , emb : Union [ str , Embedding ], n : int = 10 , top_n = 20_000 , lower = False , metric = \"cosine\" , ): \"\"\" Retreive an [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] that are the most similar to the passed query. Arguments: emb: query to use n: the number of items you'd like to see returned top_n: likelihood limit that sets the subset of words to search metric: metric to use to calculate distance, must be scipy or sklearn compatible lower: only fetch lower case tokens, note that the official english model only has lower case tokens Important: This method is incredibly slow at the moment without a good `top_n` setting due to [this bug](https://github.com/facebookresearch/fastText/issues/1040). Returns: An [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] containing the similar embeddings. \"\"\" embs = [ w [ 0 ] for w in self . score_similar ( emb , n , top_n , lower , metric )] return EmbeddingSet ({ w . name : w for w in embs }) Retreive an EmbeddingSet that are the most similar to the passed query. Parameters Name Type Description Default emb Union[str, whatlies.embedding.Embedding] query to use required n int the number of items you'd like to see returned 10 top_n likelihood limit that sets the subset of words to search 20000 metric metric to use to calculate distance, must be scipy or sklearn compatible 'cosine' lower only fetch lower case tokens, note that the official english model only has lower case tokens False Important This method is incredibly slow at the moment without a good top_n setting due to this bug . Returns Type Description `` An EmbeddingSet containing the similar embeddings.","title":"embset_similar()"},{"location":"api/language/floret_lang/#whatlies.language._floret_lang.FloretLanguage.score_similar","text":"Show source code in language/_floret_lang.py 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 def score_similar ( self , emb : Union [ str , Embedding ], n : int = 10 , top_n = 20_000 , lower = False , metric = \"cosine\" , ): \"\"\" Retreive a list of (Embedding, score) tuples that are the most similar to the passed query. Arguments: emb: query to use n: the number of items you'd like to see returned top_n: likelihood limit that sets the subset of words to search, to ignore set to `None` metric: metric to use to calculate distance, must be scipy or sklearn compatible lower: only fetch lower case tokens, note that the official english model only has lower case tokens Important: This method is incredibly slow at the moment without a good `top_n` setting due to [this bug](https://github.com/facebookresearch/fastText/issues/1040). Returns: An list of ([Embedding][whatlies.embedding.Embedding], score) tuples. \"\"\" if isinstance ( emb , str ): emb = self [ emb ] queries = self . _prepare_queries ( top_n , lower ) distances = self . _calculate_distances ( emb , queries , metric ) by_similarity = sorted ( zip ( queries , distances ), key = lambda z : z [ 1 ]) if len ( queries ) < n : warnings . warn ( f \"We could only find { len ( queries ) } feasible words. Consider changing `top_n` or `lower`\" , UserWarning , ) return [( self [ q ], float ( d )) for q , d in by_similarity [: n ]] Retreive a list of (Embedding, score) tuples that are the most similar to the passed query. Parameters Name Type Description Default emb Union[str, whatlies.embedding.Embedding] query to use required n int the number of items you'd like to see returned 10 top_n likelihood limit that sets the subset of words to search, to ignore set to None 20000 metric metric to use to calculate distance, must be scipy or sklearn compatible 'cosine' lower only fetch lower case tokens, note that the official english model only has lower case tokens False Important This method is incredibly slow at the moment without a good top_n setting due to this bug . Returns Type Description `` An list of ( Embedding , score) tuples.","title":"score_similar()"},{"location":"api/language/gensim_lang/","text":"whatlies.language.GensimLanguage \u00b6 This object is used to lazily fetch Embedding s or EmbeddingSet s from a keyed vector file. These files are generated by gensim . This object is meant for retreival, not plotting. Important The vectors are not given by this library they must be download/created upfront. A potential benefit of this is that you can train your own embeddings using gensim and visualise them using this library. Here's a snippet that you can use to train your own (very limited) word2vec embeddings. from gensim.test.utils import common_texts from gensim.models import Word2Vec model = Word2Vec(common_texts, size=10, window=5, min_count=1, workers=4) model.wv.save(\"wordvectors.kv\") You can also download pre-trained embeddings that are hosted by the gensim project. import gensim.downloader as api # To check what models are available api.info()['models'].keys() # To download the vectors wv = api.load('glove-twitter-25') # This is typically saved in `~/gensim/data` but you can also edit these # vectors and save them someplace else if you'd like. wv.save(\"glove-twitter-25.kv\") Note that if a word is not available in the keyed vectors file then we'll assume a zero vector. If you pass a sentence then we'll add together the embeddings vectors of the seperate words. Parameters Name Type Description Default keyedfile name of the model to load, be sure that it's downloaded or trained beforehand required Usage : > from whatlies.language import GensimLanguage > lang = GensimLanguage ( \"wordvectors.kv\" ) > lang [ 'computer' ] > lang = GensimLanguage ( \"wordvectors.kv\" ) > lang [[ 'computer' , 'human' , 'dog' ]] __getitem__ ( self , query ) \u00b6 Show source code in language/_gensim_lang.py 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 def __getitem__ ( self , query : Union [ str , List [ str ]]): \"\"\" Retreive a single embedding or a set of embeddings. Arguments: query: single string or list of strings **Usage** ```python > from whatlies.language import GensimLanguage > lang = GensimLanguage(\"wordvectors.kv\") > lang['computer'] > lang = GensimLanguage(\"wordvectors.kv\") > lang[['computer', 'human', 'dog']] ``` \"\"\" if isinstance ( query , str ): if \" \" in query : return Embedding ( query , np . sum ([ self [ q ] . vector for q in query . split ( \" \" )], axis = 0 ) ) try : vec = np . sum ([ self . kv [ q ] for q in query . split ( \" \" )], axis = 0 ) except KeyError : vec = np . zeros ( self . kv . vector_size ) return Embedding ( query , vec ) return EmbeddingSet ( * [ self [ tok ] for tok in query ]) Retreive a single embedding or a set of embeddings. Parameters Name Type Description Default query Union[str, List[str]] single string or list of strings required Usage > from whatlies.language import GensimLanguage > lang = GensimLanguage ( \"wordvectors.kv\" ) > lang [ 'computer' ] > lang = GensimLanguage ( \"wordvectors.kv\" ) > lang [[ 'computer' , 'human' , 'dog' ]] embset_similar ( self , emb , n = 10 , lower = False , metric = 'cosine' ) \u00b6 Show source code in language/_gensim_lang.py 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 def embset_similar ( self , emb : Union [ str , Embedding ], n : int = 10 , lower = False , metric = \"cosine\" , ) -> EmbeddingSet : \"\"\" Retreive an [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] that are the most similar to the passed query. Arguments: emb: query to use n: the number of items you'd like to see returned metric: metric to use to calculate distance, must be scipy or sklearn compatible lower: only fetch lower case tokens Returns: An [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] containing the similar embeddings. \"\"\" embs = [ w [ 0 ] for w in self . score_similar ( emb = emb , n = n , lower = lower , metric = metric ) ] return EmbeddingSet ({ w . name : w for w in embs }) Retreive an EmbeddingSet that are the most similar to the passed query. Parameters Name Type Description Default emb Union[str, whatlies.embedding.Embedding] query to use required n int the number of items you'd like to see returned 10 metric metric to use to calculate distance, must be scipy or sklearn compatible 'cosine' lower only fetch lower case tokens False Returns Type Description EmbeddingSet An EmbeddingSet containing the similar embeddings. score_similar ( self , emb , n = 10 , metric = 'cosine' , lower = False ) \u00b6 Show source code in language/_gensim_lang.py 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 def score_similar ( self , emb : Union [ str , Embedding ], n : int = 10 , metric = \"cosine\" , lower = False , ) -> List : \"\"\" Retreive a list of (Embedding, score) tuples that are the most similar to the passed query. Arguments: emb: query to use n: the number of items you'd like to see returned metric: metric to use to calculate distance, must be scipy or sklearn compatible lower: only fetch lower case tokens Returns: An list of ([Embedding][whatlies.embedding.Embedding], score) tuples. \"\"\" if isinstance ( emb , str ): emb = self [ emb ] queries = self . _prepare_queries ( lower = lower ) distances = self . _calculate_distances ( emb = emb , queries = queries , metric = metric ) by_similarity = sorted ( zip ( queries , distances ), key = lambda z : z [ 1 ]) if len ( queries ) < n : warnings . warn ( f \"We could only find { len ( queries ) } feasible words. Consider changing `top_n` or `lower`\" , UserWarning , ) return [( self [ q ], float ( d )) for q , d in by_similarity [: n ]] Retreive a list of (Embedding, score) tuples that are the most similar to the passed query. Parameters Name Type Description Default emb Union[str, whatlies.embedding.Embedding] query to use required n int the number of items you'd like to see returned 10 metric metric to use to calculate distance, must be scipy or sklearn compatible 'cosine' lower only fetch lower case tokens False Returns Type Description List An list of ( Embedding , score) tuples.","title":"Gensim"},{"location":"api/language/gensim_lang/#whatlieslanguagegensimlanguage","text":"This object is used to lazily fetch Embedding s or EmbeddingSet s from a keyed vector file. These files are generated by gensim . This object is meant for retreival, not plotting. Important The vectors are not given by this library they must be download/created upfront. A potential benefit of this is that you can train your own embeddings using gensim and visualise them using this library. Here's a snippet that you can use to train your own (very limited) word2vec embeddings. from gensim.test.utils import common_texts from gensim.models import Word2Vec model = Word2Vec(common_texts, size=10, window=5, min_count=1, workers=4) model.wv.save(\"wordvectors.kv\") You can also download pre-trained embeddings that are hosted by the gensim project. import gensim.downloader as api # To check what models are available api.info()['models'].keys() # To download the vectors wv = api.load('glove-twitter-25') # This is typically saved in `~/gensim/data` but you can also edit these # vectors and save them someplace else if you'd like. wv.save(\"glove-twitter-25.kv\") Note that if a word is not available in the keyed vectors file then we'll assume a zero vector. If you pass a sentence then we'll add together the embeddings vectors of the seperate words. Parameters Name Type Description Default keyedfile name of the model to load, be sure that it's downloaded or trained beforehand required Usage : > from whatlies.language import GensimLanguage > lang = GensimLanguage ( \"wordvectors.kv\" ) > lang [ 'computer' ] > lang = GensimLanguage ( \"wordvectors.kv\" ) > lang [[ 'computer' , 'human' , 'dog' ]]","title":"whatlies.language.GensimLanguage"},{"location":"api/language/gensim_lang/#whatlies.language._gensim_lang.GensimLanguage.embset_similar","text":"Show source code in language/_gensim_lang.py 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 def embset_similar ( self , emb : Union [ str , Embedding ], n : int = 10 , lower = False , metric = \"cosine\" , ) -> EmbeddingSet : \"\"\" Retreive an [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] that are the most similar to the passed query. Arguments: emb: query to use n: the number of items you'd like to see returned metric: metric to use to calculate distance, must be scipy or sklearn compatible lower: only fetch lower case tokens Returns: An [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] containing the similar embeddings. \"\"\" embs = [ w [ 0 ] for w in self . score_similar ( emb = emb , n = n , lower = lower , metric = metric ) ] return EmbeddingSet ({ w . name : w for w in embs }) Retreive an EmbeddingSet that are the most similar to the passed query. Parameters Name Type Description Default emb Union[str, whatlies.embedding.Embedding] query to use required n int the number of items you'd like to see returned 10 metric metric to use to calculate distance, must be scipy or sklearn compatible 'cosine' lower only fetch lower case tokens False Returns Type Description EmbeddingSet An EmbeddingSet containing the similar embeddings.","title":"embset_similar()"},{"location":"api/language/gensim_lang/#whatlies.language._gensim_lang.GensimLanguage.score_similar","text":"Show source code in language/_gensim_lang.py 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 def score_similar ( self , emb : Union [ str , Embedding ], n : int = 10 , metric = \"cosine\" , lower = False , ) -> List : \"\"\" Retreive a list of (Embedding, score) tuples that are the most similar to the passed query. Arguments: emb: query to use n: the number of items you'd like to see returned metric: metric to use to calculate distance, must be scipy or sklearn compatible lower: only fetch lower case tokens Returns: An list of ([Embedding][whatlies.embedding.Embedding], score) tuples. \"\"\" if isinstance ( emb , str ): emb = self [ emb ] queries = self . _prepare_queries ( lower = lower ) distances = self . _calculate_distances ( emb = emb , queries = queries , metric = metric ) by_similarity = sorted ( zip ( queries , distances ), key = lambda z : z [ 1 ]) if len ( queries ) < n : warnings . warn ( f \"We could only find { len ( queries ) } feasible words. Consider changing `top_n` or `lower`\" , UserWarning , ) return [( self [ q ], float ( d )) for q , d in by_similarity [: n ]] Retreive a list of (Embedding, score) tuples that are the most similar to the passed query. Parameters Name Type Description Default emb Union[str, whatlies.embedding.Embedding] query to use required n int the number of items you'd like to see returned 10 metric metric to use to calculate distance, must be scipy or sklearn compatible 'cosine' lower only fetch lower case tokens False Returns Type Description List An list of ( Embedding , score) tuples.","title":"score_similar()"},{"location":"api/language/labse/","text":"whatlies.language.LaBSELanguage \u00b6 Retreive a Language Agnostic Bert model from huggingface. The model is suggested to support 109 languages. You can see the language list in the apendix of the original paper found here . Important This object will automatically download a large file if it is not cached yet. This language model does not contain a vocabulary, so it cannot be used to retreive similar tokens. Use an EmbeddingSet instead. This language backend might require you to manually install extra dependencies unless you installed via either; pip install whatlies[transformers] pip install whatlies[all] Usage : from whatlies.language import LaBSELanguage lang = LaBSELanguage () texts = [ 'ik vind honden leuk' , 'i really like dogs' , 'me gusta los perros!' , 'let us talk about money' , 'laten we over geld praten' , 'hablemos de dinero' , 'los stroopwafels son impresionantes' , 'stroopwafels zijn heerlijk' , 'give me more stroopwafels' ] lang [ texts ] . plot_similarity ()","title":"LaBSE"},{"location":"api/language/labse/#whatlieslanguagelabselanguage","text":"Retreive a Language Agnostic Bert model from huggingface. The model is suggested to support 109 languages. You can see the language list in the apendix of the original paper found here . Important This object will automatically download a large file if it is not cached yet. This language model does not contain a vocabulary, so it cannot be used to retreive similar tokens. Use an EmbeddingSet instead. This language backend might require you to manually install extra dependencies unless you installed via either; pip install whatlies[transformers] pip install whatlies[all] Usage : from whatlies.language import LaBSELanguage lang = LaBSELanguage () texts = [ 'ik vind honden leuk' , 'i really like dogs' , 'me gusta los perros!' , 'let us talk about money' , 'laten we over geld praten' , 'hablemos de dinero' , 'los stroopwafels son impresionantes' , 'stroopwafels zijn heerlijk' , 'give me more stroopwafels' ] lang [ texts ] . plot_similarity ()","title":"whatlies.language.LaBSELanguage"},{"location":"api/language/s2v_lang/","text":"whatlies.language.Sense2VecLanguage \u00b6 This object is used to lazily fetch Embedding s or EmbeddingSet s from a sense2vec language backend. This object is meant for retreival, not plotting. Parameters Name Type Description Default sense2vec_path path to downloaded vectors required Usage : > lang = Sense2VecLanguage ( sense2vec_path = \"/path/to/reddit_vectors-1.1.0\" ) > lang [ 'bank|NOUN' ] > lang [ 'bank|VERB' ] Important The reddit vectors are not given by this library. You can find the download link here . Warning This tool is temporarily not supported because sense2vec isn't supported by spaCy v3 just yet. __getitem__ ( self , query ) \u00b6 Show source code in language/_sense2vec_lang.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 def __getitem__ ( self , query ): \"\"\" Retreive a single embedding or a set of embeddings. Arguments: query: single string or list of strings **Usage** ```python > lang = SpacyLanguage(\"en_core_web_md\") > lang['duck|NOUN'] > lang[['duck|NOUN'], ['duck|VERB']] ``` \"\"\" if isinstance ( query , str ): vec = self . s2v [ query ] return Embedding ( query , vec ) return EmbeddingSet ( * [ self [ tok ] for tok in query ]) Retreive a single embedding or a set of embeddings. Parameters Name Type Description Default query single string or list of strings required Usage > lang = SpacyLanguage ( \"en_core_web_md\" ) > lang [ 'duck|NOUN' ] > lang [[ 'duck|NOUN' ], [ 'duck|VERB' ]] embset_similar ( self , query , n = 10 ) \u00b6 Show source code in language/_sense2vec_lang.py 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 def embset_similar ( self , query , n = 10 ): \"\"\" Retreive an [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] that are the most simmilar to the passed query. Arguments: query: query to use n: the number of items you'd like to see returned Returns: An [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] containing the similar embeddings. \"\"\" return EmbeddingSet ( * [ self [ tok ] for tok , sim in self . s2v . most_similar ( query , n = n )], name = f \"Embset[s2v similar_ { n } : { query } ]\" , ) Retreive an EmbeddingSet that are the most simmilar to the passed query. Parameters Name Type Description Default query query to use required n the number of items you'd like to see returned 10 Returns Type Description `` An EmbeddingSet containing the similar embeddings. score_similar ( self , query , n = 10 ) \u00b6 Show source code in language/_sense2vec_lang.py 73 74 75 76 77 78 79 80 81 82 83 84 def score_similar ( self , query , n = 10 ): \"\"\" Retreive an EmbeddingSet that are the most simmilar to the passed query. Arguments: query: query to use n: the number of items you'd like to see returned Returns: An list of ([Embedding][whatlies.embedding.Embedding], score) tuples. \"\"\" return [( self [ tok ], sim ) for tok , sim in self . s2v . most_similar ( query , n = n )] Retreive an EmbeddingSet that are the most simmilar to the passed query. Parameters Name Type Description Default query query to use required n the number of items you'd like to see returned 10 Returns Type Description `` An list of ( Embedding , score) tuples.","title":"Sense2Vec"},{"location":"api/language/s2v_lang/#whatlieslanguagesense2veclanguage","text":"This object is used to lazily fetch Embedding s or EmbeddingSet s from a sense2vec language backend. This object is meant for retreival, not plotting. Parameters Name Type Description Default sense2vec_path path to downloaded vectors required Usage : > lang = Sense2VecLanguage ( sense2vec_path = \"/path/to/reddit_vectors-1.1.0\" ) > lang [ 'bank|NOUN' ] > lang [ 'bank|VERB' ] Important The reddit vectors are not given by this library. You can find the download link here . Warning This tool is temporarily not supported because sense2vec isn't supported by spaCy v3 just yet.","title":"whatlies.language.Sense2VecLanguage"},{"location":"api/language/s2v_lang/#whatlies.language._sense2vec_lang.Sense2VecLanguage.embset_similar","text":"Show source code in language/_sense2vec_lang.py 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 def embset_similar ( self , query , n = 10 ): \"\"\" Retreive an [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] that are the most simmilar to the passed query. Arguments: query: query to use n: the number of items you'd like to see returned Returns: An [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] containing the similar embeddings. \"\"\" return EmbeddingSet ( * [ self [ tok ] for tok , sim in self . s2v . most_similar ( query , n = n )], name = f \"Embset[s2v similar_ { n } : { query } ]\" , ) Retreive an EmbeddingSet that are the most simmilar to the passed query. Parameters Name Type Description Default query query to use required n the number of items you'd like to see returned 10 Returns Type Description `` An EmbeddingSet containing the similar embeddings.","title":"embset_similar()"},{"location":"api/language/s2v_lang/#whatlies.language._sense2vec_lang.Sense2VecLanguage.score_similar","text":"Show source code in language/_sense2vec_lang.py 73 74 75 76 77 78 79 80 81 82 83 84 def score_similar ( self , query , n = 10 ): \"\"\" Retreive an EmbeddingSet that are the most simmilar to the passed query. Arguments: query: query to use n: the number of items you'd like to see returned Returns: An list of ([Embedding][whatlies.embedding.Embedding], score) tuples. \"\"\" return [( self [ tok ], sim ) for tok , sim in self . s2v . most_similar ( query , n = n )] Retreive an EmbeddingSet that are the most simmilar to the passed query. Parameters Name Type Description Default query query to use required n the number of items you'd like to see returned 10 Returns Type Description `` An list of ( Embedding , score) tuples.","title":"score_similar()"},{"location":"api/language/sentence_tfm/","text":"whatlies.language.SentenceTFMLanguage \u00b6 This class provides the abitilty to load and use the encoding strategies found in the sentence transformers library. A full list of pretrained embeddings can be found here . Parameters Name Type Description Default name name of the model to load, be sure that it's downloaded or trained beforehand required Usage : > from whatlies.language import SentenceTFMLanguage > lang = SentenceTFMLanguage ( 'distilbert-base-nli-stsb-mean-tokens' ) > lang [ 'python is great' ] > lang [[ 'python is a language' , 'python is a snake' , 'dogs are cool animals' ]]","title":"Sent TFM"},{"location":"api/language/sentence_tfm/#whatlieslanguagesentencetfmlanguage","text":"This class provides the abitilty to load and use the encoding strategies found in the sentence transformers library. A full list of pretrained embeddings can be found here . Parameters Name Type Description Default name name of the model to load, be sure that it's downloaded or trained beforehand required Usage : > from whatlies.language import SentenceTFMLanguage > lang = SentenceTFMLanguage ( 'distilbert-base-nli-stsb-mean-tokens' ) > lang [ 'python is great' ] > lang [[ 'python is a language' , 'python is a snake' , 'dogs are cool animals' ]]","title":"whatlies.language.SentenceTFMLanguage"},{"location":"api/language/spacy_lang/","text":"whatlies.language.SpacyLanguage \u00b6 This object is used to lazily fetch Embedding s or EmbeddingSet s from a spaCy language backend. This object is meant for retreival, not plotting. Parameters Name Type Description Default nlp Union[str, spacy.language.Language] name of the model to load, be sure that it's downloaded beforehand required Important This language backend might require you to manually install extra dependencies unless you installed via either; pip install whatlies[spacy] pip install whatlies[all] Usage : > lang = SpacyLanguage ( \"en_core_web_md\" ) > lang [ 'python' ] > lang [[ 'python' , 'snake' , 'dog' ]] __getitem__ ( self , query ) \u00b6 Show source code in language/_spacy_lang.py 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 def __getitem__ ( self , query : Union [ str , List [ str ]] ) -> Union [ Embedding , EmbeddingSet ]: \"\"\" Retreive a single embedding or a set of embeddings. Depending on the spaCy model the strings can support multiple tokens of text but they can also use the Bert DSL. See the Language Options documentation: https://rasahq.github.io/whatlies/tutorial/languages/#bert-style. Arguments: query: single string or list of strings **Usage** ```python > lang = SpacyLanguage(\"en_core_web_md\") > lang['python'] > lang[['python', 'snake']] > lang[['nobody expects', 'the spanish inquisition']] > lang = SpacyLanguage(\"en_trf_robertabase_lg\") > lang['programming in [python]'] ``` \"\"\" if isinstance ( query , str ): return self . _get_embedding ( query ) return EmbeddingSet ( * [ self . _get_embedding ( q ) for q in query ]) Retreive a single embedding or a set of embeddings. Depending on the spaCy model the strings can support multiple tokens of text but they can also use the Bert DSL. See the Language Options documentation: https://rasahq.github.io/whatlies/tutorial/languages/#bert-style. Parameters Name Type Description Default query Union[str, List[str]] single string or list of strings required Usage > lang = SpacyLanguage ( \"en_core_web_md\" ) > lang [ 'python' ] > lang [[ 'python' , 'snake' ]] > lang [[ 'nobody expects' , 'the spanish inquisition' ]] > lang = SpacyLanguage ( \"en_trf_robertabase_lg\" ) > lang [ 'programming in [python]' ] embset_proximity ( self , emb , max_proximity = 0.1 , prob_limit =- 15 , lower = True , metric = 'cosine' ) \u00b6 Show source code in language/_spacy_lang.py 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 def embset_proximity ( self , emb : Union [ str , Embedding ], max_proximity : float = 0.1 , prob_limit =- 15 , lower = True , metric = \"cosine\" , ): \"\"\" Retreive an [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] or embeddings that are within a proximity. Arguments: emb: query to use max_proximity: the number of items you'd like to see returned prob_limit: likelihood limit that sets the subset of words to search metric: metric to use to calculate distance, must be scipy or sklearn compatible lower: only fetch lower case tokens Returns: An [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] containing the similar embeddings. \"\"\" if isinstance ( emb , str ): emb = self [ emb ] queries = self . _prepare_queries ( prob_limit , lower ) distances = self . _calculate_distances ( emb , queries , metric ) return EmbeddingSet ( { w : self [ w ] for w , d in zip ( queries , distances ) if d <= max_proximity } ) Retreive an EmbeddingSet or embeddings that are within a proximity. Parameters Name Type Description Default emb Union[str, whatlies.embedding.Embedding] query to use required max_proximity float the number of items you'd like to see returned 0.1 prob_limit likelihood limit that sets the subset of words to search -15 metric metric to use to calculate distance, must be scipy or sklearn compatible 'cosine' lower only fetch lower case tokens True Returns Type Description `` An EmbeddingSet containing the similar embeddings. embset_similar ( self , emb , n = 10 , prob_limit =- 15 , lower = True , metric = 'cosine' ) \u00b6 Show source code in language/_spacy_lang.py 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 def embset_similar ( self , emb : Union [ str , Embedding ], n : int = 10 , prob_limit =- 15 , lower = True , metric = \"cosine\" , ): \"\"\" Retreive an [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] that are the most simmilar to the passed query. Arguments: emb: query to use n: the number of items you'd like to see returned prob_limit: likelihood limit that sets the subset of words to search metric: metric to use to calculate distance, must be scipy or sklearn compatible lower: only fetch lower case tokens Returns: An [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] containing the similar embeddings. \"\"\" embs = [ w [ 0 ] for w in self . score_similar ( emb , n , prob_limit , lower , metric )] return EmbeddingSet ({ w . name : w for w in embs }) Retreive an EmbeddingSet that are the most simmilar to the passed query. Parameters Name Type Description Default emb Union[str, whatlies.embedding.Embedding] query to use required n int the number of items you'd like to see returned 10 prob_limit likelihood limit that sets the subset of words to search -15 metric metric to use to calculate distance, must be scipy or sklearn compatible 'cosine' lower only fetch lower case tokens True Returns Type Description `` An EmbeddingSet containing the similar embeddings. score_similar ( self , emb , n = 10 , prob_limit =- 15 , lower = True , metric = 'cosine' ) \u00b6 Show source code in language/_spacy_lang.py 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 def score_similar ( self , emb : Union [ str , Embedding ], n : int = 10 , prob_limit =- 15 , lower = True , metric = \"cosine\" , ): \"\"\" Retreive a list of (Embedding, score) tuples that are the most simmilar to the passed query. Arguments: emb: query to use n: the number of items you'd like to see returned prob_limit: likelihood limit that sets the subset of words to search, to ignore set to `None` metric: metric to use to calculate distance, must be scipy or sklearn compatible lower: only fetch lower case tokens Returns: An list of ([Embedding][whatlies.embedding.Embedding], score) tuples. \"\"\" if isinstance ( emb , str ): emb = self [ emb ] queries = self . _prepare_queries ( prob_limit , lower ) distances = self . _calculate_distances ( emb , queries , metric ) by_similarity = sorted ( zip ( queries , distances ), key = lambda z : z [ 1 ]) if len ( queries ) < n : warnings . warn ( f \"We could only find { len ( queries ) } feasible words. Consider changing `prob_limit` or `lower`\" , UserWarning , ) return [( self [ q . text ], float ( d )) for q , d in by_similarity [: n ]] Retreive a list of (Embedding, score) tuples that are the most simmilar to the passed query. Parameters Name Type Description Default emb Union[str, whatlies.embedding.Embedding] query to use required n int the number of items you'd like to see returned 10 prob_limit likelihood limit that sets the subset of words to search, to ignore set to None -15 metric metric to use to calculate distance, must be scipy or sklearn compatible 'cosine' lower only fetch lower case tokens True Returns Type Description `` An list of ( Embedding , score) tuples.","title":"spaCy"},{"location":"api/language/spacy_lang/#whatlieslanguagespacylanguage","text":"This object is used to lazily fetch Embedding s or EmbeddingSet s from a spaCy language backend. This object is meant for retreival, not plotting. Parameters Name Type Description Default nlp Union[str, spacy.language.Language] name of the model to load, be sure that it's downloaded beforehand required Important This language backend might require you to manually install extra dependencies unless you installed via either; pip install whatlies[spacy] pip install whatlies[all] Usage : > lang = SpacyLanguage ( \"en_core_web_md\" ) > lang [ 'python' ] > lang [[ 'python' , 'snake' , 'dog' ]]","title":"whatlies.language.SpacyLanguage"},{"location":"api/language/spacy_lang/#whatlies.language._spacy_lang.SpacyLanguage.embset_proximity","text":"Show source code in language/_spacy_lang.py 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 def embset_proximity ( self , emb : Union [ str , Embedding ], max_proximity : float = 0.1 , prob_limit =- 15 , lower = True , metric = \"cosine\" , ): \"\"\" Retreive an [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] or embeddings that are within a proximity. Arguments: emb: query to use max_proximity: the number of items you'd like to see returned prob_limit: likelihood limit that sets the subset of words to search metric: metric to use to calculate distance, must be scipy or sklearn compatible lower: only fetch lower case tokens Returns: An [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] containing the similar embeddings. \"\"\" if isinstance ( emb , str ): emb = self [ emb ] queries = self . _prepare_queries ( prob_limit , lower ) distances = self . _calculate_distances ( emb , queries , metric ) return EmbeddingSet ( { w : self [ w ] for w , d in zip ( queries , distances ) if d <= max_proximity } ) Retreive an EmbeddingSet or embeddings that are within a proximity. Parameters Name Type Description Default emb Union[str, whatlies.embedding.Embedding] query to use required max_proximity float the number of items you'd like to see returned 0.1 prob_limit likelihood limit that sets the subset of words to search -15 metric metric to use to calculate distance, must be scipy or sklearn compatible 'cosine' lower only fetch lower case tokens True Returns Type Description `` An EmbeddingSet containing the similar embeddings.","title":"embset_proximity()"},{"location":"api/language/spacy_lang/#whatlies.language._spacy_lang.SpacyLanguage.embset_similar","text":"Show source code in language/_spacy_lang.py 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 def embset_similar ( self , emb : Union [ str , Embedding ], n : int = 10 , prob_limit =- 15 , lower = True , metric = \"cosine\" , ): \"\"\" Retreive an [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] that are the most simmilar to the passed query. Arguments: emb: query to use n: the number of items you'd like to see returned prob_limit: likelihood limit that sets the subset of words to search metric: metric to use to calculate distance, must be scipy or sklearn compatible lower: only fetch lower case tokens Returns: An [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] containing the similar embeddings. \"\"\" embs = [ w [ 0 ] for w in self . score_similar ( emb , n , prob_limit , lower , metric )] return EmbeddingSet ({ w . name : w for w in embs }) Retreive an EmbeddingSet that are the most simmilar to the passed query. Parameters Name Type Description Default emb Union[str, whatlies.embedding.Embedding] query to use required n int the number of items you'd like to see returned 10 prob_limit likelihood limit that sets the subset of words to search -15 metric metric to use to calculate distance, must be scipy or sklearn compatible 'cosine' lower only fetch lower case tokens True Returns Type Description `` An EmbeddingSet containing the similar embeddings.","title":"embset_similar()"},{"location":"api/language/spacy_lang/#whatlies.language._spacy_lang.SpacyLanguage.score_similar","text":"Show source code in language/_spacy_lang.py 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 def score_similar ( self , emb : Union [ str , Embedding ], n : int = 10 , prob_limit =- 15 , lower = True , metric = \"cosine\" , ): \"\"\" Retreive a list of (Embedding, score) tuples that are the most simmilar to the passed query. Arguments: emb: query to use n: the number of items you'd like to see returned prob_limit: likelihood limit that sets the subset of words to search, to ignore set to `None` metric: metric to use to calculate distance, must be scipy or sklearn compatible lower: only fetch lower case tokens Returns: An list of ([Embedding][whatlies.embedding.Embedding], score) tuples. \"\"\" if isinstance ( emb , str ): emb = self [ emb ] queries = self . _prepare_queries ( prob_limit , lower ) distances = self . _calculate_distances ( emb , queries , metric ) by_similarity = sorted ( zip ( queries , distances ), key = lambda z : z [ 1 ]) if len ( queries ) < n : warnings . warn ( f \"We could only find { len ( queries ) } feasible words. Consider changing `prob_limit` or `lower`\" , UserWarning , ) return [( self [ q . text ], float ( d )) for q , d in by_similarity [: n ]] Retreive a list of (Embedding, score) tuples that are the most simmilar to the passed query. Parameters Name Type Description Default emb Union[str, whatlies.embedding.Embedding] query to use required n int the number of items you'd like to see returned 10 prob_limit likelihood limit that sets the subset of words to search, to ignore set to None -15 metric metric to use to calculate distance, must be scipy or sklearn compatible 'cosine' lower only fetch lower case tokens True Returns Type Description `` An list of ( Embedding , score) tuples.","title":"score_similar()"},{"location":"api/language/tfhub/","text":"whatlies.language.TFHubLanguage \u00b6 This class provides the abitilty to load and use text-embedding models of Tensorflow Hub to retrieve Embedding s or EmbeddingSet s from them. A list of supported models is available here ; however, note that only those models which operate directly on raw text (i.e. don't require any pre-processing such as tokenization) are supported for the moment (e.g. models such as BERT or ALBERT are not supported). Further, the TF-Hub compatible models from other repositories (i.e. other than tfhub.dev ) are also supported. Important This object will automatically download a large file if it is not cached yet. This language model does not contain a vocabulary, so it cannot be used to retreive similar tokens. Use an EmbeddingSet instead. This language backend might require you to manually install extra dependencies unless you installed via either; pip install whatlies[tfhub] pip install whatlies[all] Further, consider that this language model mainly supports TensorFlow 2.x models (i.e. TF2 SavedModel format); although, TensorFlow 1.x models might be supported to some extent as well (see hub.load documentation as well as model compatibility guide ). Parameters Name Type Description Default url str The url or local directory path of the model. required tags Optional[List[str]] A set of strings specifying the graph variant to use, if loading from a TF1 module. It is passed to hub.load function. None signature Optional[str] An optional signature of the model to use. None Usage : > from whatlies.language import TFHubLanguage > lang = TFHubLanguage ( \"https://tfhub.dev/google/nnlm-en-dim50/2\" ) > lang [ 'today is a gift' ] > lang = TFHubLanguage ( \"https://tfhub.dev/google/nnlm-en-dim50/2\" ) > lang [[ 'withdraw some money' , 'take out cash' , 'cash out funds' ]] __getitem__ ( self , query ) \u00b6 Show source code in language/_tfhub_lang.py 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 def __getitem__ ( self , query : Union [ str , List [ str ]] ) -> Union [ Embedding , EmbeddingSet ]: \"\"\" Retreive a single embedding or a set of embeddings. Arguments: query: single string or list of strings **Usage** ```python > from whatlies.language import TFHubLanguage > lang = TFHubLanguage(\"https://tfhub.dev/google/nnlm-en-dim50/2\") > lang['today is a gift'] > lang = TFHubLanguage(\"https://tfhub.dev/google/nnlm-en-dim50/2\") > lang[['withdraw some money', 'take out cash', 'cash out funds']] ``` \"\"\" if isinstance ( query , str ): return self . _get_embedding ( query ) return EmbeddingSet ( * [ self . _get_embedding ( q ) for q in query ]) Retreive a single embedding or a set of embeddings. Parameters Name Type Description Default query Union[str, List[str]] single string or list of strings required Usage > from whatlies.language import TFHubLanguage > lang = TFHubLanguage ( \"https://tfhub.dev/google/nnlm-en-dim50/2\" ) > lang [ 'today is a gift' ] > lang = TFHubLanguage ( \"https://tfhub.dev/google/nnlm-en-dim50/2\" ) > lang [[ 'withdraw some money' , 'take out cash' , 'cash out funds' ]]","title":"TFHub"},{"location":"api/language/tfhub/#whatlieslanguagetfhublanguage","text":"This class provides the abitilty to load and use text-embedding models of Tensorflow Hub to retrieve Embedding s or EmbeddingSet s from them. A list of supported models is available here ; however, note that only those models which operate directly on raw text (i.e. don't require any pre-processing such as tokenization) are supported for the moment (e.g. models such as BERT or ALBERT are not supported). Further, the TF-Hub compatible models from other repositories (i.e. other than tfhub.dev ) are also supported. Important This object will automatically download a large file if it is not cached yet. This language model does not contain a vocabulary, so it cannot be used to retreive similar tokens. Use an EmbeddingSet instead. This language backend might require you to manually install extra dependencies unless you installed via either; pip install whatlies[tfhub] pip install whatlies[all] Further, consider that this language model mainly supports TensorFlow 2.x models (i.e. TF2 SavedModel format); although, TensorFlow 1.x models might be supported to some extent as well (see hub.load documentation as well as model compatibility guide ). Parameters Name Type Description Default url str The url or local directory path of the model. required tags Optional[List[str]] A set of strings specifying the graph variant to use, if loading from a TF1 module. It is passed to hub.load function. None signature Optional[str] An optional signature of the model to use. None Usage : > from whatlies.language import TFHubLanguage > lang = TFHubLanguage ( \"https://tfhub.dev/google/nnlm-en-dim50/2\" ) > lang [ 'today is a gift' ] > lang = TFHubLanguage ( \"https://tfhub.dev/google/nnlm-en-dim50/2\" ) > lang [[ 'withdraw some money' , 'take out cash' , 'cash out funds' ]]","title":"whatlies.language.TFHubLanguage"},{"location":"api/language/tfidf_lang/","text":"whatlies.language.TFIDFVectorLanguage \u00b6 This object is used to lazily fetch Embedding s or EmbeddingSet s from a tf/idf language backend. This object is meant for retreival, not plotting. This model will first train a scikit-learn TfidfVectorizer after which it will perform dimensionality reduction to make the numeric representation a vector. The reduction occurs via TruncatedSVD , also from scikit-learn. Warning This method does not implement a word embedding in the traditional sense. The interpretation needs to be altered. The information that is captured here only relates to the words/characters that are used in the text. There is no notion of meaning that should be suggested. Also, in order to keep this system consistent with the rest of the api you train the system when you retreive vectors if you just use __getitem__ . If you want to seperate train/test you need to call fit_manual yourself or use it in a scikit-learn pipeline. Parameters Name Type Description Default n_components int Number of components that TruncatedSVD will reduce to. required lowercase bool If the tokens need to be lowercased beforehand. True analyzer str Which analyzer to use, can be \"word\", \"char\", \"char_wb\". 'char' ngram_range Tuple[int, int] The range that specifies how many ngrams to use. (1, 2) min_df Union[int, float] Ignore terms that have a document frequency strictly lower than the given threshold. 1 max_df Union[int, float] Ignore terms that have a document frequency strictly higher than the given threshold. 1.0 binary bool Determines if the counts are binary or if they can accumulate. False strip_accents str Remove accents and perform normalisation. Can be set to \"ascii\" or \"unicode\". None random_state int Random state for SVD algorithm. 42 For more elaborate explainers on these arguments, check out the scikit-learn documentation . Usage : from whatlies.language import TFIDFVectorLanguage lang = TFIDFVectorLanguage ( n_components = 2 , ngram_range = ( 1 , 2 ), analyzer = \"char\" ) lang [[ 'pizza' , 'pizzas' , 'firehouse' , 'firehydrant' ]] __getitem__ ( self , query ) \u00b6 Show source code in language/_tfidf_lang.py 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 def __getitem__ ( self , query : Union [ str , List [ str ]]): \"\"\" Retreive a set of embeddings. Arguments: query: list of strings **Usage** ```python from whatlies.language import CountVectorLanguage lang = CountVectorLanguage(n_components=2, ngram_range=(1, 2), analyzer=\"char\") lang[['pizza', 'pizzas', 'firehouse', 'firehydrant']] ``` \"\"\" orig_str = isinstance ( query , str ) if orig_str : query = [ query ] if any ([ len ( q ) == 0 for q in query ]): raise ValueError ( \"You've passed an empty string to the language model which is not allowed.\" ) if self . fitted_manual : X = self . cv . transform ( query ) X_vec = self . svd . transform ( X ) else : X = self . cv . fit_transform ( query ) X_vec = self . svd . fit_transform ( X ) if orig_str : return Embedding ( name = query [ 0 ], vector = X_vec [ 0 ]) return EmbeddingSet ( * [ Embedding ( name = n , vector = v ) for n , v in zip ( query , X_vec )] ) Retreive a set of embeddings. Parameters Name Type Description Default query Union[str, List[str]] list of strings required Usage from whatlies.language import CountVectorLanguage lang = CountVectorLanguage ( n_components = 2 , ngram_range = ( 1 , 2 ), analyzer = \"char\" ) lang [[ 'pizza' , 'pizzas' , 'firehouse' , 'firehydrant' ]] embset_similar ( self , emb , n = 10 , lower = False , metric = 'cosine' ) \u00b6 Show source code in language/_tfidf_lang.py 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 def embset_similar ( self , emb : Union [ str , Embedding ], n : int = 10 , lower = False , metric = \"cosine\" , ) -> EmbeddingSet : \"\"\" Retreive an [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] that are the most similar to the passed query. Note that we will only consider words that were passed in the `.fit_manual()` step. Arguments: emb: query to use n: the number of items you'd like to see returned metric: metric to use to calculate distance, must be scipy or sklearn compatible lower: only fetch lower case tokens Returns: An [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] containing the similar embeddings. \"\"\" embs = [ w [ 0 ] for w in self . score_similar ( emb = emb , n = n , lower = lower , metric = metric ) ] return EmbeddingSet ({ w . name : w for w in embs }) Retreive an EmbeddingSet that are the most similar to the passed query. Note that we will only consider words that were passed in the .fit_manual() step. Parameters Name Type Description Default emb Union[str, whatlies.embedding.Embedding] query to use required n int the number of items you'd like to see returned 10 metric metric to use to calculate distance, must be scipy or sklearn compatible 'cosine' lower only fetch lower case tokens False Returns Type Description EmbeddingSet An EmbeddingSet containing the similar embeddings. fit_manual ( self , query ) \u00b6 Show source code in language/_tfidf_lang.py 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 def fit_manual ( self , query ): \"\"\" Fit the model manually. This way you can call `__getitem__` independantly of training. Arguments: query: list of strings **Usage** ```python from whatlies.language import CountVectorLanguage lang = CountVectorLanguage(n_components=2, ngram_range=(1, 2), analyzer=\"char\") lang.fit_manual(['pizza', 'pizzas', 'firehouse', 'firehydrant']) lang[['piza', 'pizza', 'pizzaz', 'fyrehouse', 'firehouse', 'fyrehidrant']] ``` \"\"\" if any ([ len ( q ) == 0 for q in query ]): raise ValueError ( \"You've passed an empty string to the language model which is not allowed.\" ) X = self . cv . fit_transform ( query ) self . svd . fit ( X ) self . fitted_manual = True self . corpus = query return self Fit the model manually. This way you can call __getitem__ independantly of training. Parameters Name Type Description Default query list of strings required Usage from whatlies.language import CountVectorLanguage lang = CountVectorLanguage ( n_components = 2 , ngram_range = ( 1 , 2 ), analyzer = \"char\" ) lang . fit_manual ([ 'pizza' , 'pizzas' , 'firehouse' , 'firehydrant' ]) lang [[ 'piza' , 'pizza' , 'pizzaz' , 'fyrehouse' , 'firehouse' , 'fyrehidrant' ]] score_similar ( self , emb , n = 10 , metric = 'cosine' , lower = False ) \u00b6 Show source code in language/_tfidf_lang.py 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 def score_similar ( self , emb : Union [ str , Embedding ], n : int = 10 , metric = \"cosine\" , lower = False , ) -> List : \"\"\" Retreive a list of (Embedding, score) tuples that are the most similar to the passed query. Note that we will only consider words that were passed in the `.fit_manual()` step. Arguments: emb: query to use n: the number of items you'd like to see returned metric: metric to use to calculate distance, must be scipy or sklearn compatible lower: only fetch lower case tokens Returns: An list of ([Embedding][whatlies.embedding.Embedding], score) tuples. \"\"\" if isinstance ( emb , str ): emb = self [ emb ] queries = self . _prepare_queries ( lower = lower ) distances = self . _calculate_distances ( emb = emb , queries = queries , metric = metric ) by_similarity = sorted ( zip ( queries , distances ), key = lambda z : z [ 1 ]) if len ( self . corpus ) < n : raise ValueError ( f \"You're trying to retreive { n } items while the corpus only trained on { len ( self . corpus ) } .\" ) if len ( queries ) < n : warnings . warn ( f \"We could only find { len ( queries ) } feasible words. Consider changing `top_n` or `lower`\" , UserWarning , ) return [( self [ q ], float ( d )) for q , d in by_similarity [: n ]] Retreive a list of (Embedding, score) tuples that are the most similar to the passed query. Note that we will only consider words that were passed in the .fit_manual() step. Parameters Name Type Description Default emb Union[str, whatlies.embedding.Embedding] query to use required n int the number of items you'd like to see returned 10 metric metric to use to calculate distance, must be scipy or sklearn compatible 'cosine' lower only fetch lower case tokens False Returns Type Description List An list of ( Embedding , score) tuples.","title":"TFIDFVector"},{"location":"api/language/tfidf_lang/#whatlieslanguagetfidfvectorlanguage","text":"This object is used to lazily fetch Embedding s or EmbeddingSet s from a tf/idf language backend. This object is meant for retreival, not plotting. This model will first train a scikit-learn TfidfVectorizer after which it will perform dimensionality reduction to make the numeric representation a vector. The reduction occurs via TruncatedSVD , also from scikit-learn. Warning This method does not implement a word embedding in the traditional sense. The interpretation needs to be altered. The information that is captured here only relates to the words/characters that are used in the text. There is no notion of meaning that should be suggested. Also, in order to keep this system consistent with the rest of the api you train the system when you retreive vectors if you just use __getitem__ . If you want to seperate train/test you need to call fit_manual yourself or use it in a scikit-learn pipeline. Parameters Name Type Description Default n_components int Number of components that TruncatedSVD will reduce to. required lowercase bool If the tokens need to be lowercased beforehand. True analyzer str Which analyzer to use, can be \"word\", \"char\", \"char_wb\". 'char' ngram_range Tuple[int, int] The range that specifies how many ngrams to use. (1, 2) min_df Union[int, float] Ignore terms that have a document frequency strictly lower than the given threshold. 1 max_df Union[int, float] Ignore terms that have a document frequency strictly higher than the given threshold. 1.0 binary bool Determines if the counts are binary or if they can accumulate. False strip_accents str Remove accents and perform normalisation. Can be set to \"ascii\" or \"unicode\". None random_state int Random state for SVD algorithm. 42 For more elaborate explainers on these arguments, check out the scikit-learn documentation . Usage : from whatlies.language import TFIDFVectorLanguage lang = TFIDFVectorLanguage ( n_components = 2 , ngram_range = ( 1 , 2 ), analyzer = \"char\" ) lang [[ 'pizza' , 'pizzas' , 'firehouse' , 'firehydrant' ]]","title":"whatlies.language.TFIDFVectorLanguage"},{"location":"api/language/tfidf_lang/#whatlies.language._tfidf_lang.TFIDFVectorLanguage.embset_similar","text":"Show source code in language/_tfidf_lang.py 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 def embset_similar ( self , emb : Union [ str , Embedding ], n : int = 10 , lower = False , metric = \"cosine\" , ) -> EmbeddingSet : \"\"\" Retreive an [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] that are the most similar to the passed query. Note that we will only consider words that were passed in the `.fit_manual()` step. Arguments: emb: query to use n: the number of items you'd like to see returned metric: metric to use to calculate distance, must be scipy or sklearn compatible lower: only fetch lower case tokens Returns: An [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] containing the similar embeddings. \"\"\" embs = [ w [ 0 ] for w in self . score_similar ( emb = emb , n = n , lower = lower , metric = metric ) ] return EmbeddingSet ({ w . name : w for w in embs }) Retreive an EmbeddingSet that are the most similar to the passed query. Note that we will only consider words that were passed in the .fit_manual() step. Parameters Name Type Description Default emb Union[str, whatlies.embedding.Embedding] query to use required n int the number of items you'd like to see returned 10 metric metric to use to calculate distance, must be scipy or sklearn compatible 'cosine' lower only fetch lower case tokens False Returns Type Description EmbeddingSet An EmbeddingSet containing the similar embeddings.","title":"embset_similar()"},{"location":"api/language/tfidf_lang/#whatlies.language._tfidf_lang.TFIDFVectorLanguage.fit_manual","text":"Show source code in language/_tfidf_lang.py 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 def fit_manual ( self , query ): \"\"\" Fit the model manually. This way you can call `__getitem__` independantly of training. Arguments: query: list of strings **Usage** ```python from whatlies.language import CountVectorLanguage lang = CountVectorLanguage(n_components=2, ngram_range=(1, 2), analyzer=\"char\") lang.fit_manual(['pizza', 'pizzas', 'firehouse', 'firehydrant']) lang[['piza', 'pizza', 'pizzaz', 'fyrehouse', 'firehouse', 'fyrehidrant']] ``` \"\"\" if any ([ len ( q ) == 0 for q in query ]): raise ValueError ( \"You've passed an empty string to the language model which is not allowed.\" ) X = self . cv . fit_transform ( query ) self . svd . fit ( X ) self . fitted_manual = True self . corpus = query return self Fit the model manually. This way you can call __getitem__ independantly of training. Parameters Name Type Description Default query list of strings required Usage from whatlies.language import CountVectorLanguage lang = CountVectorLanguage ( n_components = 2 , ngram_range = ( 1 , 2 ), analyzer = \"char\" ) lang . fit_manual ([ 'pizza' , 'pizzas' , 'firehouse' , 'firehydrant' ]) lang [[ 'piza' , 'pizza' , 'pizzaz' , 'fyrehouse' , 'firehouse' , 'fyrehidrant' ]]","title":"fit_manual()"},{"location":"api/language/tfidf_lang/#whatlies.language._tfidf_lang.TFIDFVectorLanguage.score_similar","text":"Show source code in language/_tfidf_lang.py 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 def score_similar ( self , emb : Union [ str , Embedding ], n : int = 10 , metric = \"cosine\" , lower = False , ) -> List : \"\"\" Retreive a list of (Embedding, score) tuples that are the most similar to the passed query. Note that we will only consider words that were passed in the `.fit_manual()` step. Arguments: emb: query to use n: the number of items you'd like to see returned metric: metric to use to calculate distance, must be scipy or sklearn compatible lower: only fetch lower case tokens Returns: An list of ([Embedding][whatlies.embedding.Embedding], score) tuples. \"\"\" if isinstance ( emb , str ): emb = self [ emb ] queries = self . _prepare_queries ( lower = lower ) distances = self . _calculate_distances ( emb = emb , queries = queries , metric = metric ) by_similarity = sorted ( zip ( queries , distances ), key = lambda z : z [ 1 ]) if len ( self . corpus ) < n : raise ValueError ( f \"You're trying to retreive { n } items while the corpus only trained on { len ( self . corpus ) } .\" ) if len ( queries ) < n : warnings . warn ( f \"We could only find { len ( queries ) } feasible words. Consider changing `top_n` or `lower`\" , UserWarning , ) return [( self [ q ], float ( d )) for q , d in by_similarity [: n ]] Retreive a list of (Embedding, score) tuples that are the most similar to the passed query. Note that we will only consider words that were passed in the .fit_manual() step. Parameters Name Type Description Default emb Union[str, whatlies.embedding.Embedding] query to use required n int the number of items you'd like to see returned 10 metric metric to use to calculate distance, must be scipy or sklearn compatible 'cosine' lower only fetch lower case tokens False Returns Type Description List An list of ( Embedding , score) tuples.","title":"score_similar()"},{"location":"api/language/transformers/","text":"whatlies.language.HFTransformersLanguage \u00b6 This language class can be used to load Hugging Face Transformer models and use them to obtain representation of input string(s) as Embedding or EmbeddingSet . Important To use this language class, either of TensorFlow or PyTorch should be installed. This language model does not contain a vocabulary, so it cannot be used to retreive similar tokens. Use an EmbeddingSet instead. This language backend might require you to manually install extra dependencies unless you installed via either; pip install whatlies[transformers] pip install whatlies[all] Parameters Name Type Description Default model_name_or_path str A string which is the name or identifier of a model from Hugging Face model repository , or is the path to a local directory which contains a pre-trained transformer model files. required **kwargs Any Additional key-value pair argument(s) which are passed to transformers.pipeline function. {} Usage : > from whatlies.language import HFTransformersLanguage > lang = HFTransformersLanguage ( 'bert-base-cased' ) > lang [ 'today is a nice day' ] > lang = HFTransformersLanguage ( 'gpt2' ) > lang [[ 'day and night' , 'it is as clear as day' , 'today the sky is clear' ]] __getitem__ ( self , query ) \u00b6 Show source code in language/_hftransformers_lang.py 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 def __getitem__ ( self , query : Union [ str , List [ str ]]): \"\"\" Retreive a single embedding or a set of embeddings. Arguments: query: A single string or a list of strings Returns: An instance of [Embedding][whatlies.embedding.Embedding] (when `query` is a string) or [EmbeddingSet][whatlies.embeddingset.EmbeddingSet] (when `query` is a list of strings). The embedding vector is computed as the sum of hidden-state representaions of tokens (excluding special tokens, e.g. [CLS]). **Usage** ```python > from whatlies.language import HFTransformersLanguage > lang = HFTransformersLanguage('bert-base-cased') > lang['today is a nice day'] > lang = HFTransformersLanguage('gpt2') > lang[['day and night', 'it is as clear as day', 'today the sky is clear']] ``` \"\"\" if isinstance ( query , str ): return self . _get_embedding ( query ) return EmbeddingSet ( * [ self . _get_embedding ( q ) for q in query ]) Retreive a single embedding or a set of embeddings. Parameters Name Type Description Default query Union[str, List[str]] A single string or a list of strings required Returns Type Description `` An instance of Embedding (when query is a string) or EmbeddingSet (when query is a list of strings). The embedding vector is computed as the sum of hidden-state representaions of tokens (excluding special tokens, e.g. [CLS]). Usage > from whatlies.language import HFTransformersLanguage > lang = HFTransformersLanguage ( 'bert-base-cased' ) > lang [ 'today is a nice day' ] > lang = HFTransformersLanguage ( 'gpt2' ) > lang [[ 'day and night' , 'it is as clear as day' , 'today the sky is clear' ]]","title":"Huggingface"},{"location":"api/language/transformers/#whatlieslanguagehftransformerslanguage","text":"This language class can be used to load Hugging Face Transformer models and use them to obtain representation of input string(s) as Embedding or EmbeddingSet . Important To use this language class, either of TensorFlow or PyTorch should be installed. This language model does not contain a vocabulary, so it cannot be used to retreive similar tokens. Use an EmbeddingSet instead. This language backend might require you to manually install extra dependencies unless you installed via either; pip install whatlies[transformers] pip install whatlies[all] Parameters Name Type Description Default model_name_or_path str A string which is the name or identifier of a model from Hugging Face model repository , or is the path to a local directory which contains a pre-trained transformer model files. required **kwargs Any Additional key-value pair argument(s) which are passed to transformers.pipeline function. {} Usage : > from whatlies.language import HFTransformersLanguage > lang = HFTransformersLanguage ( 'bert-base-cased' ) > lang [ 'today is a nice day' ] > lang = HFTransformersLanguage ( 'gpt2' ) > lang [[ 'day and night' , 'it is as clear as day' , 'today the sky is clear' ]]","title":"whatlies.language.HFTransformersLanguage"},{"location":"api/language/universal_sentence/","text":"UniversalSentenceLanguage ( variant = 'base' , version = None ) \u00b6 Show source code in language/_sentence_encode_lang.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 def UniversalSentenceLanguage ( variant : str = \"base\" , version : Union [ int , None ] = None ): \"\"\" Retreive a [universal sentence encoder](https://tfhub.dev/google/collections/universal-sentence-encoder/1) model from tfhub. You can download specific versions for specific variants. The variants that we support are listed below. - `\"base\"`: the base variant (915MB) [link](https://tfhub.dev/google/universal-sentence-encoder/4) - `\"large\"`: the large variant (523MB) [link](https://tfhub.dev/google/universal-sentence-encoder-large/5) - `\"qa\"`: the variant based on question/answer (528MB) [link](https://tfhub.dev/google/universal-sentence-encoder-qa/3) - `\"multi\"`: the multi-language variant (245MB) [link](https://tfhub.dev/google/universal-sentence-encoder-multilingual/3) - `\"multi-large\"`: the large multi-language variant (303MB) [link](https://tfhub.dev/google/universal-sentence-encoder-multilingual-large/3) - `\"multi-qa\"`: the multi-language qa variant (310MB) [link](https://tfhub.dev/google/universal-sentence-encoder-multilingual-qa/3) TFHub reports that the multi-language models support Arabic, Chinese-simplified, Chinese-traditional, English, French, German, Italian, Japanese, Korean, Dutch, Polish, Portuguese, Spanish, Thai, Turkish and Russian. Important: This object will automatically download a large file if it is not cached yet. This language model does not contain a vocabulary, so it cannot be used to retreive similar tokens. Use an `EmbeddingSet` instead. This language backend might require you to manually install extra dependencies unless you installed via either; ``` pip install whatlies[tfhub] pip install whatlies[all] ``` Arguments: variant: select a specific variant version: select a specific version, if kept `None` we'll assume the most recent version \"\"\" urls = { \"base\" : \"https://tfhub.dev/google/universal-sentence-encoder/\" , \"large\" : \"https://tfhub.dev/google/universal-sentence-encoder-large/\" , \"qa\" : \"https://tfhub.dev/google/universal-sentence-encoder-qa/\" , \"multi\" : \"https://tfhub.dev/google/universal-sentence-encoder-multilingual/\" , \"multi-large\" : \"https://tfhub.dev/google/universal-sentence-encoder-multilingual-large/\" , \"multi-qa\" : \"https://tfhub.dev/google/universal-sentence-encoder-multilingual-qa/3\" , } versions = { \"base\" : 4 , \"large\" : 5 , \"qa\" : 3 , \"multi\" : 3 , \"multi-large\" : 3 , \"multi-qa\" : 3 , } version = versions [ variant ] if not version else version url = urls [ variant ] + str ( version ) return TFHubLanguage ( url = url ) Retreive a universal sentence encoder model from tfhub. You can download specific versions for specific variants. The variants that we support are listed below. \"base\" : the base variant (915MB) link \"large\" : the large variant (523MB) link \"qa\" : the variant based on question/answer (528MB) link \"multi\" : the multi-language variant (245MB) link \"multi-large\" : the large multi-language variant (303MB) link \"multi-qa\" : the multi-language qa variant (310MB) link TFHub reports that the multi-language models support Arabic, Chinese-simplified, Chinese-traditional, English, French, German, Italian, Japanese, Korean, Dutch, Polish, Portuguese, Spanish, Thai, Turkish and Russian. Important This object will automatically download a large file if it is not cached yet. This language model does not contain a vocabulary, so it cannot be used to retreive similar tokens. Use an EmbeddingSet instead. This language backend might require you to manually install extra dependencies unless you installed via either; pip install whatlies[tfhub] pip install whatlies[all] Parameters Name Type Description Default variant str select a specific variant 'base' version Optional[int] select a specific version, if kept None we'll assume the most recent version None","title":"USE"},{"location":"api/language/universal_sentence/#whatlies.language._sentence_encode_lang.UniversalSentenceLanguage","text":"Show source code in language/_sentence_encode_lang.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 def UniversalSentenceLanguage ( variant : str = \"base\" , version : Union [ int , None ] = None ): \"\"\" Retreive a [universal sentence encoder](https://tfhub.dev/google/collections/universal-sentence-encoder/1) model from tfhub. You can download specific versions for specific variants. The variants that we support are listed below. - `\"base\"`: the base variant (915MB) [link](https://tfhub.dev/google/universal-sentence-encoder/4) - `\"large\"`: the large variant (523MB) [link](https://tfhub.dev/google/universal-sentence-encoder-large/5) - `\"qa\"`: the variant based on question/answer (528MB) [link](https://tfhub.dev/google/universal-sentence-encoder-qa/3) - `\"multi\"`: the multi-language variant (245MB) [link](https://tfhub.dev/google/universal-sentence-encoder-multilingual/3) - `\"multi-large\"`: the large multi-language variant (303MB) [link](https://tfhub.dev/google/universal-sentence-encoder-multilingual-large/3) - `\"multi-qa\"`: the multi-language qa variant (310MB) [link](https://tfhub.dev/google/universal-sentence-encoder-multilingual-qa/3) TFHub reports that the multi-language models support Arabic, Chinese-simplified, Chinese-traditional, English, French, German, Italian, Japanese, Korean, Dutch, Polish, Portuguese, Spanish, Thai, Turkish and Russian. Important: This object will automatically download a large file if it is not cached yet. This language model does not contain a vocabulary, so it cannot be used to retreive similar tokens. Use an `EmbeddingSet` instead. This language backend might require you to manually install extra dependencies unless you installed via either; ``` pip install whatlies[tfhub] pip install whatlies[all] ``` Arguments: variant: select a specific variant version: select a specific version, if kept `None` we'll assume the most recent version \"\"\" urls = { \"base\" : \"https://tfhub.dev/google/universal-sentence-encoder/\" , \"large\" : \"https://tfhub.dev/google/universal-sentence-encoder-large/\" , \"qa\" : \"https://tfhub.dev/google/universal-sentence-encoder-qa/\" , \"multi\" : \"https://tfhub.dev/google/universal-sentence-encoder-multilingual/\" , \"multi-large\" : \"https://tfhub.dev/google/universal-sentence-encoder-multilingual-large/\" , \"multi-qa\" : \"https://tfhub.dev/google/universal-sentence-encoder-multilingual-qa/3\" , } versions = { \"base\" : 4 , \"large\" : 5 , \"qa\" : 3 , \"multi\" : 3 , \"multi-large\" : 3 , \"multi-qa\" : 3 , } version = versions [ variant ] if not version else version url = urls [ variant ] + str ( version ) return TFHubLanguage ( url = url ) Retreive a universal sentence encoder model from tfhub. You can download specific versions for specific variants. The variants that we support are listed below. \"base\" : the base variant (915MB) link \"large\" : the large variant (523MB) link \"qa\" : the variant based on question/answer (528MB) link \"multi\" : the multi-language variant (245MB) link \"multi-large\" : the large multi-language variant (303MB) link \"multi-qa\" : the multi-language qa variant (310MB) link TFHub reports that the multi-language models support Arabic, Chinese-simplified, Chinese-traditional, English, French, German, Italian, Japanese, Korean, Dutch, Polish, Portuguese, Spanish, Thai, Turkish and Russian. Important This object will automatically download a large file if it is not cached yet. This language model does not contain a vocabulary, so it cannot be used to retreive similar tokens. Use an EmbeddingSet instead. This language backend might require you to manually install extra dependencies unless you installed via either; pip install whatlies[tfhub] pip install whatlies[all] Parameters Name Type Description Default variant str select a specific variant 'base' version Optional[int] select a specific version, if kept None we'll assume the most recent version None","title":"UniversalSentenceLanguage()"},{"location":"api/transformers/addrandom/","text":"whatlies.transformers.AddRandom \u00b6 This transformer adds random embeddings to the embeddingset. Parameters Name Type Description Default n the number of random vectors to add 1 Usage: from whatlies.language import SpacyLanguage from whatlies.transformers import AddRandom words = [ \"prince\" , \"princess\" , \"nurse\" , \"doctor\" , \"banker\" , \"man\" , \"woman\" , \"cousin\" , \"neice\" , \"king\" , \"queen\" , \"dude\" , \"guy\" , \"gal\" , \"fire\" , \"dog\" , \"cat\" , \"mouse\" , \"red\" , \"blue\" , \"green\" , \"yellow\" , \"water\" , \"person\" , \"family\" , \"brother\" , \"sister\" ] lang = SpacyLanguage ( \"en_core_web_md\" ) emb = lang [ words ] emb . transform ( AddRandom ( 3 )) . plot_interactive_matrix ( 'rand_0' , 'rand_1' , 'rand_2' ) fit ( self , embset ) \u00b6 Show source code in transformers/_addrandom.py 38 39 40 def fit ( self , embset ): self . is_fitted = True return self Fit the transformer on the given EmbeddingSet instance (if neccessary). This method should set the is_fitted flag to True . Parameters Name Type Description Default embset an EmbeddingSet instance used for fitting the transformer. required transform ( self , embset ) \u00b6 Show source code in transformers/_addrandom.py 42 43 44 45 46 47 48 49 50 51 52 def transform ( self , embset ): names , X = embset . to_names_X () np . random . seed ( self . seed ) orig_dict = embset . embeddings . copy () new_dict = { f \"rand_ { k } \" : Embedding ( f \"rand_ { k } \" , np . random . normal ( 0 , self . sigma , X . shape [ 1 ]) ) for k in range ( self . n ) } return EmbeddingSet ({ ** orig_dict , ** new_dict }) Transform the given EmbeddingSet instance. Parameters Name Type Description Default embset an EmbeddingSet instance to be transformed. required","title":"Add Random"},{"location":"api/transformers/addrandom/#whatliestransformersaddrandom","text":"This transformer adds random embeddings to the embeddingset. Parameters Name Type Description Default n the number of random vectors to add 1 Usage: from whatlies.language import SpacyLanguage from whatlies.transformers import AddRandom words = [ \"prince\" , \"princess\" , \"nurse\" , \"doctor\" , \"banker\" , \"man\" , \"woman\" , \"cousin\" , \"neice\" , \"king\" , \"queen\" , \"dude\" , \"guy\" , \"gal\" , \"fire\" , \"dog\" , \"cat\" , \"mouse\" , \"red\" , \"blue\" , \"green\" , \"yellow\" , \"water\" , \"person\" , \"family\" , \"brother\" , \"sister\" ] lang = SpacyLanguage ( \"en_core_web_md\" ) emb = lang [ words ] emb . transform ( AddRandom ( 3 )) . plot_interactive_matrix ( 'rand_0' , 'rand_1' , 'rand_2' )","title":"whatlies.transformers.AddRandom"},{"location":"api/transformers/addrandom/#whatlies.transformers._addrandom.AddRandom.fit","text":"Show source code in transformers/_addrandom.py 38 39 40 def fit ( self , embset ): self . is_fitted = True return self Fit the transformer on the given EmbeddingSet instance (if neccessary). This method should set the is_fitted flag to True . Parameters Name Type Description Default embset an EmbeddingSet instance used for fitting the transformer. required","title":"fit()"},{"location":"api/transformers/addrandom/#whatlies.transformers._addrandom.AddRandom.transform","text":"Show source code in transformers/_addrandom.py 42 43 44 45 46 47 48 49 50 51 52 def transform ( self , embset ): names , X = embset . to_names_X () np . random . seed ( self . seed ) orig_dict = embset . embeddings . copy () new_dict = { f \"rand_ { k } \" : Embedding ( f \"rand_ { k } \" , np . random . normal ( 0 , self . sigma , X . shape [ 1 ]) ) for k in range ( self . n ) } return EmbeddingSet ({ ** orig_dict , ** new_dict }) Transform the given EmbeddingSet instance. Parameters Name Type Description Default embset an EmbeddingSet instance to be transformed. required","title":"transform()"},{"location":"api/transformers/noise/","text":"whatlies.transformers.Noise \u00b6 This transformer adds gaussian noise to an embeddingset. Parameters Name Type Description Default sigma the amount of gaussian noise to add 0.1 seed seed value for random number generator 42 Usage: from whatlies.language import SpacyLanguage from whatlies.transformers import Noise words = [ \"prince\" , \"princess\" , \"nurse\" , \"doctor\" , \"banker\" , \"man\" , \"woman\" , \"cousin\" , \"neice\" , \"king\" , \"queen\" , \"dude\" , \"guy\" , \"gal\" , \"fire\" , \"dog\" , \"cat\" , \"mouse\" , \"red\" , \"blue\" , \"green\" , \"yellow\" , \"water\" , \"person\" , \"family\" , \"brother\" , \"sister\" ] lang = SpacyLanguage ( \"en_core_web_md\" ) emb = lang [ words ] emb . transform ( Noise ( 3 ))","title":"Noise"},{"location":"api/transformers/noise/#whatliestransformersnoise","text":"This transformer adds gaussian noise to an embeddingset. Parameters Name Type Description Default sigma the amount of gaussian noise to add 0.1 seed seed value for random number generator 42 Usage: from whatlies.language import SpacyLanguage from whatlies.transformers import Noise words = [ \"prince\" , \"princess\" , \"nurse\" , \"doctor\" , \"banker\" , \"man\" , \"woman\" , \"cousin\" , \"neice\" , \"king\" , \"queen\" , \"dude\" , \"guy\" , \"gal\" , \"fire\" , \"dog\" , \"cat\" , \"mouse\" , \"red\" , \"blue\" , \"green\" , \"yellow\" , \"water\" , \"person\" , \"family\" , \"brother\" , \"sister\" ] lang = SpacyLanguage ( \"en_core_web_md\" ) emb = lang [ words ] emb . transform ( Noise ( 3 ))","title":"whatlies.transformers.Noise"},{"location":"api/transformers/pca/","text":"whatlies.transformers.Pca \u00b6 This transformer scales all the vectors in an EmbeddingSet by means of principal component analysis. We're using the implementation found in scikit-learn Parameters Name Type Description Default n_components the number of compoments to create/add required **kwargs keyword arguments passed to the PCA from scikit-learn {} Usage: from whatlies.language import SpacyLanguage from whatlies.transformers import Pca words = [ \"prince\" , \"princess\" , \"nurse\" , \"doctor\" , \"banker\" , \"man\" , \"woman\" , \"cousin\" , \"neice\" , \"king\" , \"queen\" , \"dude\" , \"guy\" , \"gal\" , \"fire\" , \"dog\" , \"cat\" , \"mouse\" , \"red\" , \"blue\" , \"green\" , \"yellow\" , \"water\" , \"person\" , \"family\" , \"brother\" , \"sister\" ] lang = SpacyLanguage ( \"en_core_web_md\" ) emb = lang [ words ] emb . transform ( Pca ( 3 )) . plot_interactive_matrix ( 0 , 1 , 2 )","title":"Pca"},{"location":"api/transformers/pca/#whatliestransformerspca","text":"This transformer scales all the vectors in an EmbeddingSet by means of principal component analysis. We're using the implementation found in scikit-learn Parameters Name Type Description Default n_components the number of compoments to create/add required **kwargs keyword arguments passed to the PCA from scikit-learn {} Usage: from whatlies.language import SpacyLanguage from whatlies.transformers import Pca words = [ \"prince\" , \"princess\" , \"nurse\" , \"doctor\" , \"banker\" , \"man\" , \"woman\" , \"cousin\" , \"neice\" , \"king\" , \"queen\" , \"dude\" , \"guy\" , \"gal\" , \"fire\" , \"dog\" , \"cat\" , \"mouse\" , \"red\" , \"blue\" , \"green\" , \"yellow\" , \"water\" , \"person\" , \"family\" , \"brother\" , \"sister\" ] lang = SpacyLanguage ( \"en_core_web_md\" ) emb = lang [ words ] emb . transform ( Pca ( 3 )) . plot_interactive_matrix ( 0 , 1 , 2 )","title":"whatlies.transformers.Pca"},{"location":"api/transformers/tsne/","text":"whatlies.transformers.Tsne \u00b6 This transformer transformers all vectors in an EmbeddingSet by means of tsne. This implementation uses scikit-learn . Important TSNE does not allow you to train a transformation and re-use it. It must retrain every time it sees data. You may also notice that it is relatively slow. This unfortunately is a fact of life. Parameters Name Type Description Default n_components the number of compoments to create/add 2 **kwargs keyword arguments passed to the Tsne implementation, includes things like perplexity link {} Usage: from whatlies.language import SpacyLanguage from whatlies.transformers import Tsne words = [ \"prince\" , \"princess\" , \"nurse\" , \"doctor\" , \"banker\" , \"man\" , \"woman\" , \"cousin\" , \"neice\" , \"king\" , \"queen\" , \"dude\" , \"guy\" , \"gal\" , \"fire\" , \"dog\" , \"cat\" , \"mouse\" , \"red\" , \"blue\" , \"green\" , \"yellow\" , \"water\" , \"person\" , \"family\" , \"brother\" , \"sister\" ] lang = SpacyLanguage ( \"en_core_web_md\" ) emb = lang [ words ] emb . transform ( Tsne ( 3 )) . plot_interactive_matrix ( 0 , 1 , 2 ) transform ( self , embset ) \u00b6 Show source code in transformers/_tsne.py 46 47 48 49 50 51 52 def transform ( self , embset ): names , X = embset . to_names_X () # We are re-writing the transform method here because TSNE cannot .fit().transform(). # Check the docs here: https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html#sklearn.manifold.TSNE new_vecs = self . tfm . fit_transform ( X ) new_dict = new_embedding_dict ( names , new_vecs , embset ) return EmbeddingSet ( new_dict , name = f \" { embset . name } .tsne( { self . n_components } )\" ) Transform the given EmbeddingSet instance. Parameters Name Type Description Default embset an EmbeddingSet instance to be transformed. required","title":"Tsne"},{"location":"api/transformers/tsne/#whatliestransformerstsne","text":"This transformer transformers all vectors in an EmbeddingSet by means of tsne. This implementation uses scikit-learn . Important TSNE does not allow you to train a transformation and re-use it. It must retrain every time it sees data. You may also notice that it is relatively slow. This unfortunately is a fact of life. Parameters Name Type Description Default n_components the number of compoments to create/add 2 **kwargs keyword arguments passed to the Tsne implementation, includes things like perplexity link {} Usage: from whatlies.language import SpacyLanguage from whatlies.transformers import Tsne words = [ \"prince\" , \"princess\" , \"nurse\" , \"doctor\" , \"banker\" , \"man\" , \"woman\" , \"cousin\" , \"neice\" , \"king\" , \"queen\" , \"dude\" , \"guy\" , \"gal\" , \"fire\" , \"dog\" , \"cat\" , \"mouse\" , \"red\" , \"blue\" , \"green\" , \"yellow\" , \"water\" , \"person\" , \"family\" , \"brother\" , \"sister\" ] lang = SpacyLanguage ( \"en_core_web_md\" ) emb = lang [ words ] emb . transform ( Tsne ( 3 )) . plot_interactive_matrix ( 0 , 1 , 2 )","title":"whatlies.transformers.Tsne"},{"location":"api/transformers/tsne/#whatlies.transformers._tsne.Tsne.transform","text":"Show source code in transformers/_tsne.py 46 47 48 49 50 51 52 def transform ( self , embset ): names , X = embset . to_names_X () # We are re-writing the transform method here because TSNE cannot .fit().transform(). # Check the docs here: https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html#sklearn.manifold.TSNE new_vecs = self . tfm . fit_transform ( X ) new_dict = new_embedding_dict ( names , new_vecs , embset ) return EmbeddingSet ( new_dict , name = f \" { embset . name } .tsne( { self . n_components } )\" ) Transform the given EmbeddingSet instance. Parameters Name Type Description Default embset an EmbeddingSet instance to be transformed. required","title":"transform()"},{"location":"api/transformers/umap/","text":"whatlies.transformers.Umap \u00b6 This transformer transformers all vectors in an EmbeddingSet by means of umap. We're using the implementation in umap-learn . Parameters Name Type Description Default n_components the number of compoments to create/add 2 **kwargs keyword arguments passed to the UMAP algorithm {} Usage: from whatlies.language import SpacyLanguage from whatlies.transformers import Umap words = [ \"prince\" , \"princess\" , \"nurse\" , \"doctor\" , \"banker\" , \"man\" , \"woman\" , \"cousin\" , \"neice\" , \"king\" , \"queen\" , \"dude\" , \"guy\" , \"gal\" , \"fire\" , \"dog\" , \"cat\" , \"mouse\" , \"red\" , \"blue\" , \"green\" , \"yellow\" , \"water\" , \"person\" , \"family\" , \"brother\" , \"sister\" ] lang = SpacyLanguage ( \"en_core_web_md\" ) emb = lang [ words ] emb . transform ( Umap ( 3 )) . plot_interactive_matrix ( 0 , 1 , 2 )","title":"Umap"},{"location":"api/transformers/umap/#whatliestransformersumap","text":"This transformer transformers all vectors in an EmbeddingSet by means of umap. We're using the implementation in umap-learn . Parameters Name Type Description Default n_components the number of compoments to create/add 2 **kwargs keyword arguments passed to the UMAP algorithm {} Usage: from whatlies.language import SpacyLanguage from whatlies.transformers import Umap words = [ \"prince\" , \"princess\" , \"nurse\" , \"doctor\" , \"banker\" , \"man\" , \"woman\" , \"cousin\" , \"neice\" , \"king\" , \"queen\" , \"dude\" , \"guy\" , \"gal\" , \"fire\" , \"dog\" , \"cat\" , \"mouse\" , \"red\" , \"blue\" , \"green\" , \"yellow\" , \"water\" , \"person\" , \"family\" , \"brother\" , \"sister\" ] lang = SpacyLanguage ( \"en_core_web_md\" ) emb = lang [ words ] emb . transform ( Umap ( 3 )) . plot_interactive_matrix ( 0 , 1 , 2 )","title":"whatlies.transformers.Umap"},{"location":"examples/arabic/","text":"One of the goals of this package is to make it simple to explore embeddings. This includes embeddings that are Non-English. In this guide we'll demonstrate how you might be able to use this library to run simple Arabic classification benchmark using scikit-learn and this library. This benchmark was part of discussion on github . If you want to follow along, make sure that this tool is installed. pip install whatlies If you'd like to also try out the heavy transformer model, you'll also want to install extra dependencies. pip install \"whatlies[transformers]\" Finding Embeddings \u00b6 For Arabic there are many sources of embeddings. In this benchmark we'll limit ourselves to BytePair embeddings and pretrained BERT . There are also plenty of other embeddings out there. There's gensim embeddings available via AraVec and there's also pretrained subword-embeddings in fasttext . There's also support for the Egyptian dialect but we'll ignore these for now. The Task \u00b6 The task we'll benchmark is sentiment analysis for arabic tweets . It's a simple classification problem with two classes; positive and negative. We'll assume the labels for the dataset are accurate but we should remind ourselves to check correctness later if we intend to use this dataset for a real life use-case. For more info on this topic watch here . Explore \u00b6 Before you run the big benchmark, it makes sense to explore the embeddings first. Let's start by loading them in. from whatlies.language import BytePairLanguage , HFTransformersLanguage lang_bp1 = BytePairLanguage ( \"ar\" , vs = 10000 , dim = 300 ) lang_bp2 = BytePairLanguage ( \"ar\" , vs = 200000 , dim = 300 ) # Feel free to remove `lang_hf` from the benchmark if you want want quick results. # These BERT-style embeddings are very compute heavy and can take a while to benchmark. lang_hf = HFTransformersLanguage ( \"asafaya/bert-base-arabic\" ) A popular method of visualising embeddings is to make a scatterplot of clusters. We'll look at a few text examples by embedding them and reducing their dimensionality via Umap. import pandas as pd from whatlies.transformers import Umap # Read in the dataframes from Kaggle df = pd . concat ([ pd . read_csv ( \"test_Arabic_tweets_negative_20190413.tsv\" , sep = \" \\t \" ), pd . read_csv ( \"test_Arabic_tweets_positive_20190413.tsv\" , sep = \" \\t \" ) ], axis = 0 ) . sample ( frac = 1 ) . reset_index ( drop = True ) df . columns = [ \"label\" , \"text\" ] # Next we clean the dataset df = ( df . loc [ lambda d : d [ 'text' ] . str . len () < 200 ] . drop_duplicates () . sample ( frac = 1 ) . reset_index ( drop = True )) # Sample a small list such that the interactive charts render swiftly. small_text_list = list ( set ( df [: 1000 ][ 'text' ])) def mk_plot ( lang , title = \"\" ): return ( lang [ small_text_list ] . transform ( Umap ( 2 )) . plot_interactive ( annot = False ) . properties ( title = title , width = 200 , height = 200 )) mk_plot ( lang_bp2 , \"bp_big\" ) | mk_plot ( lang_hf , \"huggingface\" ) The results of this code are viewable below. Note that these charts are fully interactive and they'll show the text containing the tweets when you hover over them. fetch('charts.json') .then(res => res.json()) .then((out) => { vegaEmbed('#vis1', out); }) .catch(err => { throw err }); You might recognize some clusters, which is nice, but it's no benchmark yet. Benchmark \u00b6 The code below will actually run the actual benchmark. What's important to note is that we'll train on 7829 examples and we'll test on a holdout set of 1000 examples. In the interest of time we won't do a k-fold validation. After all, the main goal is to show how you might quickly prototype a solution using whatlies. import numpy as np from sklearn.pipeline import Pipeline , FeatureUnion from sklearn.linear_model import LogisticRegression from sklearn.feature_extraction.text import CountVectorizer from sklearn.model_selection import train_test_split from memo import grid , memfile , time_taken # Split dataframe into a train/test set. X_train , X_test , y_train , y_test = train_test_split ( list ( df [ 'text' ]), df [ 'label' ], test_size = 1000 ) # Create a dictionary with all of our embedding settings. # Note that we've removed lang_hf in order to make it easy to run on a laptop. embedders = { 'nada' : 'drop' , 'bp1' : lang_bp1 , 'bp2' : lang_bp2 , # 'hf': lang_hf } @memfile ( \"arabic-sentences-benchmark.jsonl\" ) @time_taken () def run_experiment ( embedder = \"nada\" , train_size = 1000 , smooth = 1 , ngram = True ): # This featurizers step is a list that is used in a scikit-learn pipeline. featurizers = [ ( 'cv-base' , CountVectorizer ()), ( 'cv-ngram' , CountVectorizer ( ngram_range = ( 2 , 4 )) if ngram else 'drop' ), ( 'emb' , embedders [ embedder ]) ] # After featurization we apply a Logistic Regression pipe = Pipeline ([ ( \"feat\" , FeatureUnion ( featurizers )), ( \"mod\" , LogisticRegression ( C = smooth , max_iter = 500 )) ]) # The trained pipeline is used to make a prediction. y_pred = pipe . fit ( X_train [: train_size ], y_train [: train_size ]) . predict ( X_test ) # By returning a dictionary `memo` will be able to properly log this. return { \"valid_accuracy\" : float ( np . mean ( y_test == y_pred )), \"train\" : float ( np . mean ( y_train == pipe . predict ( X_train )))} # The grid will loop over all the options and generate a progress bar # that means it's easy to run from the command line in the background. for setting in grid ( embedder = [ 'nada' , 'bp1' , 'bp2' , 'hf' ], smooth = [ 1 , 0.1 ], ngram = [ True , False ], train_size = [ 100 , 250 , 500 , 1000 , 2000 , 3000 , 4000 , 5000 , 6000 , 7000 ]): run_experiment ( ** setting ) print ( setting ) There's a few things to observe. We're using memo to handle the logging. The memfile decorator grabs app the keyword arguments and logs it together with the dictionary output. We're first embedding the text numerically, after which we apply a logistic regression. A logistic regression won't give us an upper limit of what we might be able to achieve after fine-tuning but it should serve as a reasonable lower bound of what we could expect. Besides testing the effect of the embedding we'll also have a look at the effect of training set size ( train_size ), parameter smoothing ( smooth ) on the logistic regression and the effect of adding subword countvectors ( ngram ). The benchmark will take a while! We've turned off the huggingface model so that you can get quick results locally but we will show our results below. Results \u00b6 You can play with the full benchmark dataset by exploring the parallel coordinates chart in a seperate tab here but we'll focus on the most important observations below. Overview \u00b6 It seems that smoothing and adding countvectors for the subwords doesn't contribute the most substantial predictive power. They can be tuned to add that 1% extra boost but the most substaintial gain in validation accuracy is contributed by a large training set and by chosing the huggingface embeddings. Feel free to play around with the charts below, they are fully interactive. fetch('details-1.json') .then(res => res.json()) .then((out) => { vegaEmbed('#vis2', out); }) .catch(err => { throw err }); The huggingface embeddings do come with a downside however. fetch('details-2.json') .then(res => res.json()) .then((out) => { vegaEmbed('#vis3', out); }) .catch(err => { throw err }); The huggingface embeddings are much slower. Notice the log-scale on the y-axis and note how the there's a big upfront calculation cost even for only 100 training examples. These results ran on a modern server and it can take up to 8 minutes to train/process only 8000 datapoints. This pales in comparison to the other approaches. That said, we're rapid prototyping here so we should keep in mind that we might be able to the compute time down if we build directly on top of hugginface. We also see that the train performance keeps increasing while the test performance starts to flatten. If we were to increase the train set further we might need to keep beware of overfitting. Conclusion \u00b6 Maybe the huggingface results look promising, maybe the compute time is a concern for you or maybe you've got a use-case where you need even more predictive power. Either way, we hope you see the potential for this library when it comes to rapid prototyping. The goal here wasn't to be \"state of the art\". Rather; whatlies allows you to try out a whole bunch of embeddings relatively quickly so you don't need to worry about integrating with all sorts of embedding backends when you're just starting out. This benchmark alone should not be considered as \"enough\" evidence to put a model into production, but it might be enough evidence to continue iterating. If you're curious about extending this benchmark; feel absolutely free! You might want to try out the LaBSELanguage . It's a multi-language model that should support 100+ languages. We'd also love to hear if there's useful tricks we're missing out on. We're especially keen to hear if there's tools missing for Non-English language. If you're interested in running this notebook yourself, you can download it here .","title":"Arabic Benchmarks"},{"location":"examples/arabic/#finding-embeddings","text":"For Arabic there are many sources of embeddings. In this benchmark we'll limit ourselves to BytePair embeddings and pretrained BERT . There are also plenty of other embeddings out there. There's gensim embeddings available via AraVec and there's also pretrained subword-embeddings in fasttext . There's also support for the Egyptian dialect but we'll ignore these for now.","title":"Finding Embeddings"},{"location":"examples/arabic/#the-task","text":"The task we'll benchmark is sentiment analysis for arabic tweets . It's a simple classification problem with two classes; positive and negative. We'll assume the labels for the dataset are accurate but we should remind ourselves to check correctness later if we intend to use this dataset for a real life use-case. For more info on this topic watch here .","title":"The Task"},{"location":"examples/arabic/#explore","text":"Before you run the big benchmark, it makes sense to explore the embeddings first. Let's start by loading them in. from whatlies.language import BytePairLanguage , HFTransformersLanguage lang_bp1 = BytePairLanguage ( \"ar\" , vs = 10000 , dim = 300 ) lang_bp2 = BytePairLanguage ( \"ar\" , vs = 200000 , dim = 300 ) # Feel free to remove `lang_hf` from the benchmark if you want want quick results. # These BERT-style embeddings are very compute heavy and can take a while to benchmark. lang_hf = HFTransformersLanguage ( \"asafaya/bert-base-arabic\" ) A popular method of visualising embeddings is to make a scatterplot of clusters. We'll look at a few text examples by embedding them and reducing their dimensionality via Umap. import pandas as pd from whatlies.transformers import Umap # Read in the dataframes from Kaggle df = pd . concat ([ pd . read_csv ( \"test_Arabic_tweets_negative_20190413.tsv\" , sep = \" \\t \" ), pd . read_csv ( \"test_Arabic_tweets_positive_20190413.tsv\" , sep = \" \\t \" ) ], axis = 0 ) . sample ( frac = 1 ) . reset_index ( drop = True ) df . columns = [ \"label\" , \"text\" ] # Next we clean the dataset df = ( df . loc [ lambda d : d [ 'text' ] . str . len () < 200 ] . drop_duplicates () . sample ( frac = 1 ) . reset_index ( drop = True )) # Sample a small list such that the interactive charts render swiftly. small_text_list = list ( set ( df [: 1000 ][ 'text' ])) def mk_plot ( lang , title = \"\" ): return ( lang [ small_text_list ] . transform ( Umap ( 2 )) . plot_interactive ( annot = False ) . properties ( title = title , width = 200 , height = 200 )) mk_plot ( lang_bp2 , \"bp_big\" ) | mk_plot ( lang_hf , \"huggingface\" ) The results of this code are viewable below. Note that these charts are fully interactive and they'll show the text containing the tweets when you hover over them. fetch('charts.json') .then(res => res.json()) .then((out) => { vegaEmbed('#vis1', out); }) .catch(err => { throw err }); You might recognize some clusters, which is nice, but it's no benchmark yet.","title":"Explore"},{"location":"examples/arabic/#benchmark","text":"The code below will actually run the actual benchmark. What's important to note is that we'll train on 7829 examples and we'll test on a holdout set of 1000 examples. In the interest of time we won't do a k-fold validation. After all, the main goal is to show how you might quickly prototype a solution using whatlies. import numpy as np from sklearn.pipeline import Pipeline , FeatureUnion from sklearn.linear_model import LogisticRegression from sklearn.feature_extraction.text import CountVectorizer from sklearn.model_selection import train_test_split from memo import grid , memfile , time_taken # Split dataframe into a train/test set. X_train , X_test , y_train , y_test = train_test_split ( list ( df [ 'text' ]), df [ 'label' ], test_size = 1000 ) # Create a dictionary with all of our embedding settings. # Note that we've removed lang_hf in order to make it easy to run on a laptop. embedders = { 'nada' : 'drop' , 'bp1' : lang_bp1 , 'bp2' : lang_bp2 , # 'hf': lang_hf } @memfile ( \"arabic-sentences-benchmark.jsonl\" ) @time_taken () def run_experiment ( embedder = \"nada\" , train_size = 1000 , smooth = 1 , ngram = True ): # This featurizers step is a list that is used in a scikit-learn pipeline. featurizers = [ ( 'cv-base' , CountVectorizer ()), ( 'cv-ngram' , CountVectorizer ( ngram_range = ( 2 , 4 )) if ngram else 'drop' ), ( 'emb' , embedders [ embedder ]) ] # After featurization we apply a Logistic Regression pipe = Pipeline ([ ( \"feat\" , FeatureUnion ( featurizers )), ( \"mod\" , LogisticRegression ( C = smooth , max_iter = 500 )) ]) # The trained pipeline is used to make a prediction. y_pred = pipe . fit ( X_train [: train_size ], y_train [: train_size ]) . predict ( X_test ) # By returning a dictionary `memo` will be able to properly log this. return { \"valid_accuracy\" : float ( np . mean ( y_test == y_pred )), \"train\" : float ( np . mean ( y_train == pipe . predict ( X_train )))} # The grid will loop over all the options and generate a progress bar # that means it's easy to run from the command line in the background. for setting in grid ( embedder = [ 'nada' , 'bp1' , 'bp2' , 'hf' ], smooth = [ 1 , 0.1 ], ngram = [ True , False ], train_size = [ 100 , 250 , 500 , 1000 , 2000 , 3000 , 4000 , 5000 , 6000 , 7000 ]): run_experiment ( ** setting ) print ( setting ) There's a few things to observe. We're using memo to handle the logging. The memfile decorator grabs app the keyword arguments and logs it together with the dictionary output. We're first embedding the text numerically, after which we apply a logistic regression. A logistic regression won't give us an upper limit of what we might be able to achieve after fine-tuning but it should serve as a reasonable lower bound of what we could expect. Besides testing the effect of the embedding we'll also have a look at the effect of training set size ( train_size ), parameter smoothing ( smooth ) on the logistic regression and the effect of adding subword countvectors ( ngram ). The benchmark will take a while! We've turned off the huggingface model so that you can get quick results locally but we will show our results below.","title":"Benchmark"},{"location":"examples/arabic/#results","text":"You can play with the full benchmark dataset by exploring the parallel coordinates chart in a seperate tab here but we'll focus on the most important observations below.","title":"Results"},{"location":"examples/arabic/#overview","text":"It seems that smoothing and adding countvectors for the subwords doesn't contribute the most substantial predictive power. They can be tuned to add that 1% extra boost but the most substaintial gain in validation accuracy is contributed by a large training set and by chosing the huggingface embeddings. Feel free to play around with the charts below, they are fully interactive. fetch('details-1.json') .then(res => res.json()) .then((out) => { vegaEmbed('#vis2', out); }) .catch(err => { throw err }); The huggingface embeddings do come with a downside however. fetch('details-2.json') .then(res => res.json()) .then((out) => { vegaEmbed('#vis3', out); }) .catch(err => { throw err }); The huggingface embeddings are much slower. Notice the log-scale on the y-axis and note how the there's a big upfront calculation cost even for only 100 training examples. These results ran on a modern server and it can take up to 8 minutes to train/process only 8000 datapoints. This pales in comparison to the other approaches. That said, we're rapid prototyping here so we should keep in mind that we might be able to the compute time down if we build directly on top of hugginface. We also see that the train performance keeps increasing while the test performance starts to flatten. If we were to increase the train set further we might need to keep beware of overfitting.","title":"Overview"},{"location":"examples/arabic/#conclusion","text":"Maybe the huggingface results look promising, maybe the compute time is a concern for you or maybe you've got a use-case where you need even more predictive power. Either way, we hope you see the potential for this library when it comes to rapid prototyping. The goal here wasn't to be \"state of the art\". Rather; whatlies allows you to try out a whole bunch of embeddings relatively quickly so you don't need to worry about integrating with all sorts of embedding backends when you're just starting out. This benchmark alone should not be considered as \"enough\" evidence to put a model into production, but it might be enough evidence to continue iterating. If you're curious about extending this benchmark; feel absolutely free! You might want to try out the LaBSELanguage . It's a multi-language model that should support 100+ languages. We'd also love to hear if there's useful tricks we're missing out on. We're especially keen to hear if there's tools missing for Non-English language. If you're interested in running this notebook yourself, you can download it here .","title":"Conclusion"},{"location":"examples/lipstick-pig/","text":"In this example we'd like to discuss the effectiveness of debiasing techniques in word embeddings. This example is heavily inspired by the \"Lipstick on a Pig\" paper by Hila Gonen and Yoav Goldberg. Meaning \u00b6 In word embedding space you might wonder if a direction in embedding space might represent meaning. \\(v_{man}\\) represents the word embedding for \"man\" . \\(v_{woman}\\) represents the word embedding for \"woman\" . You could argue that the axis \\(v_{man} - v_{woman}\\) might represent \"gender\". One side of the \"gender\" axis would represent the male gender while the other end represents the female gender. This spectrum might allow you to guess if words are more \"male\" or more \"female\". It can also be used as a measurement for bias in language. Similarity \u00b6 There are some axes that we might come up with that should be similar to this gender axis, like: \\(v_{he} - v_{she}\\) \\(v_{king} - v_{queen}\\) \\(v_{brother} - v_{sister}\\) It would be highly unfortunate though if the following pairs of words would display similarity: \\(v_{nurse} - v_{physician}\\) \\(v_{nurse} - v_{surgeon}\\) \\(v_{nurse} - v_{doctor}\\) Being a nurse should not imply that you are a woman just like that being a surgeon should not imply that you are a man. It'd be a shame if we were using embeddings where such stereotypical bias is present. Unfortunately, it's likely in the embeddings. Historically, women have not gotten the same opportunities as men. This is bound to be reflected on websites like Wikipedia which are a common source of data to train word embeddings. So let's make a distance chart to confirm if this is the case. Word Pairs stereotype_pairs = [ ( 'sewing' , 'carpentry' ), ( 'nurse' , 'physician' ), ( 'nurse' , 'surgeon' ), ( 'nurse' , 'doctor' ), ] appropriate_pairs = [ ( 'woman' , 'man' ), ( 'she' , 'he' ), ( 'her' , 'him' ), ( 'girl' , 'boy' ) ] random_pairs = [ ( 'dog' , 'firehydrant' ), ( 'carpet' , 'leg' ), ( 'hot' , 'cold' ), ] all_pairs = [ stereotype_pairs , appropriate_pairs , random_pairs ] from whatlies import Embedding , EmbeddingSet from whatlies.language import FasttextLanguage lang_ft = FasttextLanguage ( \"cc.en.300.bin\" ) flatten = lambda l : [ item for sublist in l for item in sublist ] def calc_axis ( pair_list , language_model ): return [ language_model [ t1 ] - language_model [ t2 ] for ( t1 , t2 ) in pair_list ] def make_correlation_plot ( pairs , language_model , metric = \"cosine\" ): axes = [ calc_axis ( p , language_model ) for p in pairs ] emb_pairs = EmbeddingSet ( * flatten ( axes )) emb_pairs . plot_distance ( metric = metric ) make_correlation_plot ( pairs = all_pairs , language_model = lang_ft ) This code generates a similarity chart for fasttext embeddings, shown below. Notice, that we indeed see correlation. The \"gender\" direction seems to correlate with the \"doctor-nurse\" direction. We'd prefer if it simply were zero. Projections \u00b6 We observe bias that we do not want to have. So it's natural to ask: can we remove it? There is a popular technique that proposes to filter out the \"gender\"-direction. If we can quantify the gender direction then we might also be able to project all the vectors in our set away from it. The 2D plot below demonstrates this idea. Plot Code from whatlies import Embedding import matplotlib.pylab as plt man = Embedding ( \"man\" , [ 0.5 , 0.1 ]) woman = Embedding ( \"woman\" , [ 0.5 , 0.6 ]) king = Embedding ( \"king\" , [ 0.7 , 0.33 ]) queen = Embedding ( \"queen\" , [ 0.7 , 0.9 ]) man . plot ( kind = \"arrow\" , color = \"blue\" ) woman . plot ( kind = \"arrow\" , color = \"red\" ) king . plot ( kind = \"arrow\" , color = \"blue\" ) queen . plot ( kind = \"arrow\" , color = \"red\" ) ( queen - king ) . plot ( kind = \"arrow\" , color = \"pink\" , show_ops = True ) ( man | ( queen - king )) . plot ( kind = \"arrow\" , color = \"pink\" , show_ops = True ) plt . axis ( 'off' ); In this example you can see that if we project \\(v_{man}\\) away from the \\(v_{queen} - v_{king}\\) axis we get a new vector \\(v_{man} | (v_{queen} - v_{king})\\) . The 2D example also demonstrates that we might achieve: \\[v_{man} | (v_{queen} - v_{king}) \\approx v_{woman} | (v_{queen} - v_{king})\\] This suggests that we can use linear algebra to \"filter\" away the gender information as well as the gender bias. Post-Processing \u00b6 def make_debias_correlation_plot ( pairs , language_model , metric = 'cosine' ): # Calculate the embeddings just like before. axes = [ calc_axis ( p , language_model ) for p in pairs ] emb_pairs = EmbeddingSet ( * flatten ( axes )) # Calculate the \"gender\"-direction norm_emb = EmbeddingSet ( ( language_model [ 'man' ] - language_model [ 'woman' ]), ( language_model [ 'king' ] - language_model [ 'queen' ]), ( language_model [ 'father' ] - language_model [ 'mother' ]) ) . average () # Project all embeddings away from this axis. emb_pairs = emb_pairs | norm_emb # Plot the result. emb_pairs . plot_distance ( metric = metric ) make_debias_correlation_plot ( pairs = all_pairs , language_model = lang_ft ) We'll now display the \"before\" as well as \"after\" chart. It's not a perfect removal of the similarity. But we can confirm that at least visually, it seems \"less\". Across Languages \u00b6 One benefit of this library is that it is fairly easy to repeat this exercise for different language backends. Just replace the language_model with a different backend. Here's the results for three backends; a large English spaCy model, FastText and a large English BytePair model. Relative Distances \u00b6 The results look promising but we need to be very careful here. We're able to show that on one bias-metric we're performing better now. But we should not assume that this solves all issues related to gender in word embeddings. To demonstrate why, let's try and use a debiased space to predict gender using standard algorithms in scikit-learn. As a data source we'll take two gendered word-lists. You can download the same word-lists here and here . These wordlists are subsets of the wordlists used in the Learning Gender - Neutral Word Embeddings paper. The original, and larger, datasets can be found here . import pathlib from whatlies.transformers import Pca , Umap from whatlies.language import SpacyLanguage , FasttextLanguage male_word = pathlib . Path ( \"male-words.txt\" ) . read_text () . split ( \" \\n \" ) female_word = pathlib . Path ( \"female-words.txt\" ) . read_text () . split ( \" \\n \" ) lang = FasttextLanguage ( \"cc.en.300.bin\" ) e1 = lang [ male_word ] . add_property ( \"group\" , lambda d : \"male\" ) e2 = lang [ female_word ] . add_property ( \"group\" , lambda d : \"female\" ) emb_debias = e1 . merge ( e2 ) | ( lang [ 'man' ] - lang [ 'woman' ]) Next, we'll use the fasttext language backend as a scikit-learn featurizer. You can read more on this feature here . from sklearn.svm import SVC from sklearn.pipeline import Pipeline # There is overlap in the word-lists which we remove via `set`. words = list ( male_word ) + list ( female_word ) words = list ( set ( words )) labels = [ w in male_word for w in words ] # We use our language backend as a transformer in scikit-learn. pipe = Pipeline ([ ( \"embed\" , lang ), ( \"model\" , SVC ()) ]) This pipeline can now be used to make predictions. Currently we do not perform any debiasing, so let's have a look at how well we can predict gender now. Method I: Biased Embedding, Biased Model \u00b6 The code below runs the schematic drawn above. from sklearn.model_selection import train_test_split , GridSearchCV from sklearn.metrics import classification_report X_train , X_test , y_train , y_test = train_test_split ( words , labels , train_size = 200 , random_state = 42 ) y_pred = pipe . fit ( X_train , y_train ) . predict ( X_test ) print ( classification_report ( y_pred , y_test )) This gives us the following result: precision recall f1-score support False 0.87 0.92 0.90 93 True 0.94 0.89 0.91 116 accuracy 0.90 209 macro avg 0.90 0.91 0.90 209 weighted avg 0.91 0.90 0.90 209 It seems that the information that is in the embeddings now give us a 90% accuracy on our test set. Method II: UnBiased Embedding, Biased Model \u00b6 If we now apply debiasing on the vectors then one might expect the old model to no longer be able to predict the gender. X , y = emb_debias . to_X_y ( 'group' ) X_train , X_test , y_train , y_test = train_test_split ( X , y , train_size = 200 , random_state = 42 ) y_pred = pipe . steps [ 1 ][ 1 ] . predict ( X_test ) print ( classification_report ( y_pred , y_test == 'male' )) This gives the following result: precision recall f1-score support False 0.97 0.73 0.83 131 True 0.68 0.96 0.79 78 accuracy 0.81 209 macro avg 0.82 0.84 0.81 209 weighted avg 0.86 0.81 0.82 209 We're using the same model as before, but now we're giving it the debiased vectors to predict on. Despite being trained on a different dataset, we're still able to predict 81% of the cases accurately. This does not bode well for our debiasing technique. Method III: UnBiased Embedding, UnBiased Model \u00b6 We can also try to create a model that is both trained and applied on the unbiased vectors. y_pred = SVC () . fit ( X_train , y_train ) . predict ( X_test ) print ( classification_report ( y_pred , y_test )) precision recall f1-score support female 0.80 0.83 0.81 94 male 0.86 0.83 0.84 115 accuracy 0.83 209 macro avg 0.83 0.83 0.83 209 weighted avg 0.83 0.83 0.83 209 If we train a model on the debiased embeddings and also apply it to another debiased set we're able to get 83% of the cases right. We were hoping more around 50% here. Conclusion \u00b6 Based on just cosine distance it seems that we're able to remove the gender \"direction\" from our embeddings by using linear projections as a debiasing technique. However, if we use the debiased embeddings to predict gender it seems that we still have a reasonable amount of predictive power. This demonstrates that the debiasing technique has a limited effect and that there's plenty of reasons to remain careful and critical when applying word embeddings in practice.","title":"Debiasing Projections"},{"location":"examples/lipstick-pig/#meaning","text":"In word embedding space you might wonder if a direction in embedding space might represent meaning. \\(v_{man}\\) represents the word embedding for \"man\" . \\(v_{woman}\\) represents the word embedding for \"woman\" . You could argue that the axis \\(v_{man} - v_{woman}\\) might represent \"gender\". One side of the \"gender\" axis would represent the male gender while the other end represents the female gender. This spectrum might allow you to guess if words are more \"male\" or more \"female\". It can also be used as a measurement for bias in language.","title":"Meaning"},{"location":"examples/lipstick-pig/#similarity","text":"There are some axes that we might come up with that should be similar to this gender axis, like: \\(v_{he} - v_{she}\\) \\(v_{king} - v_{queen}\\) \\(v_{brother} - v_{sister}\\) It would be highly unfortunate though if the following pairs of words would display similarity: \\(v_{nurse} - v_{physician}\\) \\(v_{nurse} - v_{surgeon}\\) \\(v_{nurse} - v_{doctor}\\) Being a nurse should not imply that you are a woman just like that being a surgeon should not imply that you are a man. It'd be a shame if we were using embeddings where such stereotypical bias is present. Unfortunately, it's likely in the embeddings. Historically, women have not gotten the same opportunities as men. This is bound to be reflected on websites like Wikipedia which are a common source of data to train word embeddings. So let's make a distance chart to confirm if this is the case. Word Pairs stereotype_pairs = [ ( 'sewing' , 'carpentry' ), ( 'nurse' , 'physician' ), ( 'nurse' , 'surgeon' ), ( 'nurse' , 'doctor' ), ] appropriate_pairs = [ ( 'woman' , 'man' ), ( 'she' , 'he' ), ( 'her' , 'him' ), ( 'girl' , 'boy' ) ] random_pairs = [ ( 'dog' , 'firehydrant' ), ( 'carpet' , 'leg' ), ( 'hot' , 'cold' ), ] all_pairs = [ stereotype_pairs , appropriate_pairs , random_pairs ] from whatlies import Embedding , EmbeddingSet from whatlies.language import FasttextLanguage lang_ft = FasttextLanguage ( \"cc.en.300.bin\" ) flatten = lambda l : [ item for sublist in l for item in sublist ] def calc_axis ( pair_list , language_model ): return [ language_model [ t1 ] - language_model [ t2 ] for ( t1 , t2 ) in pair_list ] def make_correlation_plot ( pairs , language_model , metric = \"cosine\" ): axes = [ calc_axis ( p , language_model ) for p in pairs ] emb_pairs = EmbeddingSet ( * flatten ( axes )) emb_pairs . plot_distance ( metric = metric ) make_correlation_plot ( pairs = all_pairs , language_model = lang_ft ) This code generates a similarity chart for fasttext embeddings, shown below. Notice, that we indeed see correlation. The \"gender\" direction seems to correlate with the \"doctor-nurse\" direction. We'd prefer if it simply were zero.","title":"Similarity"},{"location":"examples/lipstick-pig/#projections","text":"We observe bias that we do not want to have. So it's natural to ask: can we remove it? There is a popular technique that proposes to filter out the \"gender\"-direction. If we can quantify the gender direction then we might also be able to project all the vectors in our set away from it. The 2D plot below demonstrates this idea. Plot Code from whatlies import Embedding import matplotlib.pylab as plt man = Embedding ( \"man\" , [ 0.5 , 0.1 ]) woman = Embedding ( \"woman\" , [ 0.5 , 0.6 ]) king = Embedding ( \"king\" , [ 0.7 , 0.33 ]) queen = Embedding ( \"queen\" , [ 0.7 , 0.9 ]) man . plot ( kind = \"arrow\" , color = \"blue\" ) woman . plot ( kind = \"arrow\" , color = \"red\" ) king . plot ( kind = \"arrow\" , color = \"blue\" ) queen . plot ( kind = \"arrow\" , color = \"red\" ) ( queen - king ) . plot ( kind = \"arrow\" , color = \"pink\" , show_ops = True ) ( man | ( queen - king )) . plot ( kind = \"arrow\" , color = \"pink\" , show_ops = True ) plt . axis ( 'off' ); In this example you can see that if we project \\(v_{man}\\) away from the \\(v_{queen} - v_{king}\\) axis we get a new vector \\(v_{man} | (v_{queen} - v_{king})\\) . The 2D example also demonstrates that we might achieve: \\[v_{man} | (v_{queen} - v_{king}) \\approx v_{woman} | (v_{queen} - v_{king})\\] This suggests that we can use linear algebra to \"filter\" away the gender information as well as the gender bias.","title":"Projections"},{"location":"examples/lipstick-pig/#post-processing","text":"def make_debias_correlation_plot ( pairs , language_model , metric = 'cosine' ): # Calculate the embeddings just like before. axes = [ calc_axis ( p , language_model ) for p in pairs ] emb_pairs = EmbeddingSet ( * flatten ( axes )) # Calculate the \"gender\"-direction norm_emb = EmbeddingSet ( ( language_model [ 'man' ] - language_model [ 'woman' ]), ( language_model [ 'king' ] - language_model [ 'queen' ]), ( language_model [ 'father' ] - language_model [ 'mother' ]) ) . average () # Project all embeddings away from this axis. emb_pairs = emb_pairs | norm_emb # Plot the result. emb_pairs . plot_distance ( metric = metric ) make_debias_correlation_plot ( pairs = all_pairs , language_model = lang_ft ) We'll now display the \"before\" as well as \"after\" chart. It's not a perfect removal of the similarity. But we can confirm that at least visually, it seems \"less\".","title":"Post-Processing"},{"location":"examples/lipstick-pig/#across-languages","text":"One benefit of this library is that it is fairly easy to repeat this exercise for different language backends. Just replace the language_model with a different backend. Here's the results for three backends; a large English spaCy model, FastText and a large English BytePair model.","title":"Across Languages"},{"location":"examples/lipstick-pig/#relative-distances","text":"The results look promising but we need to be very careful here. We're able to show that on one bias-metric we're performing better now. But we should not assume that this solves all issues related to gender in word embeddings. To demonstrate why, let's try and use a debiased space to predict gender using standard algorithms in scikit-learn. As a data source we'll take two gendered word-lists. You can download the same word-lists here and here . These wordlists are subsets of the wordlists used in the Learning Gender - Neutral Word Embeddings paper. The original, and larger, datasets can be found here . import pathlib from whatlies.transformers import Pca , Umap from whatlies.language import SpacyLanguage , FasttextLanguage male_word = pathlib . Path ( \"male-words.txt\" ) . read_text () . split ( \" \\n \" ) female_word = pathlib . Path ( \"female-words.txt\" ) . read_text () . split ( \" \\n \" ) lang = FasttextLanguage ( \"cc.en.300.bin\" ) e1 = lang [ male_word ] . add_property ( \"group\" , lambda d : \"male\" ) e2 = lang [ female_word ] . add_property ( \"group\" , lambda d : \"female\" ) emb_debias = e1 . merge ( e2 ) | ( lang [ 'man' ] - lang [ 'woman' ]) Next, we'll use the fasttext language backend as a scikit-learn featurizer. You can read more on this feature here . from sklearn.svm import SVC from sklearn.pipeline import Pipeline # There is overlap in the word-lists which we remove via `set`. words = list ( male_word ) + list ( female_word ) words = list ( set ( words )) labels = [ w in male_word for w in words ] # We use our language backend as a transformer in scikit-learn. pipe = Pipeline ([ ( \"embed\" , lang ), ( \"model\" , SVC ()) ]) This pipeline can now be used to make predictions. Currently we do not perform any debiasing, so let's have a look at how well we can predict gender now.","title":"Relative Distances"},{"location":"examples/lipstick-pig/#method-i-biased-embedding-biased-model","text":"The code below runs the schematic drawn above. from sklearn.model_selection import train_test_split , GridSearchCV from sklearn.metrics import classification_report X_train , X_test , y_train , y_test = train_test_split ( words , labels , train_size = 200 , random_state = 42 ) y_pred = pipe . fit ( X_train , y_train ) . predict ( X_test ) print ( classification_report ( y_pred , y_test )) This gives us the following result: precision recall f1-score support False 0.87 0.92 0.90 93 True 0.94 0.89 0.91 116 accuracy 0.90 209 macro avg 0.90 0.91 0.90 209 weighted avg 0.91 0.90 0.90 209 It seems that the information that is in the embeddings now give us a 90% accuracy on our test set.","title":"Method I: Biased Embedding, Biased Model"},{"location":"examples/lipstick-pig/#method-ii-unbiased-embedding-biased-model","text":"If we now apply debiasing on the vectors then one might expect the old model to no longer be able to predict the gender. X , y = emb_debias . to_X_y ( 'group' ) X_train , X_test , y_train , y_test = train_test_split ( X , y , train_size = 200 , random_state = 42 ) y_pred = pipe . steps [ 1 ][ 1 ] . predict ( X_test ) print ( classification_report ( y_pred , y_test == 'male' )) This gives the following result: precision recall f1-score support False 0.97 0.73 0.83 131 True 0.68 0.96 0.79 78 accuracy 0.81 209 macro avg 0.82 0.84 0.81 209 weighted avg 0.86 0.81 0.82 209 We're using the same model as before, but now we're giving it the debiased vectors to predict on. Despite being trained on a different dataset, we're still able to predict 81% of the cases accurately. This does not bode well for our debiasing technique.","title":"Method II: UnBiased Embedding, Biased Model"},{"location":"examples/lipstick-pig/#method-iii-unbiased-embedding-unbiased-model","text":"We can also try to create a model that is both trained and applied on the unbiased vectors. y_pred = SVC () . fit ( X_train , y_train ) . predict ( X_test ) print ( classification_report ( y_pred , y_test )) precision recall f1-score support female 0.80 0.83 0.81 94 male 0.86 0.83 0.84 115 accuracy 0.83 209 macro avg 0.83 0.83 0.83 209 weighted avg 0.83 0.83 0.83 209 If we train a model on the debiased embeddings and also apply it to another debiased set we're able to get 83% of the cases right. We were hoping more around 50% here.","title":"Method III: UnBiased Embedding, UnBiased Model"},{"location":"examples/lipstick-pig/#conclusion","text":"Based on just cosine distance it seems that we're able to remove the gender \"direction\" from our embeddings by using linear projections as a debiasing technique. However, if we use the debiased embeddings to predict gender it seems that we still have a reasonable amount of predictive power. This demonstrates that the debiasing technique has a limited effect and that there's plenty of reasons to remain careful and critical when applying word embeddings in practice.","title":"Conclusion"},{"location":"tutorial/scikit-learn/","text":"Scikit-Learn \u00b6 Many of the language-backends inside of this package can be used in scikit-learn pipelines. We've implemented a compatible .fit() and .transform() API which means that you could write scikit-learn pipelines like this: import numpy as np from whatlies.language import SpacyLanguage from sklearn.pipeline import Pipeline from sklearn.linear_model import LogisticRegression pipe = Pipeline ([ ( \"embed\" , SpacyLanguage ( \"en_core_web_md\" )), ( \"model\" , LogisticRegression ()) ]) X = [ \"i really like this post\" , \"thanks for that comment\" , \"i enjoy this friendly forum\" , \"this is a bad post\" , \"i dislike this article\" , \"this is not well written\" ] y = np . array ([ 1 , 1 , 1 , 0 , 0 , 0 ]) pipe . fit ( X , y ) This pipeline is using the embeddings from spaCy now and passing those to the logistic regression. pipe.predict_proba(X) # array([[0.37862409, 0.62137591], # [0.27858304, 0.72141696], # [0.21386529, 0.78613471], # [0.7155662 , 0.2844338 ], # [0.64924579, 0.35075421], # [0.76414156, 0.23585844]]) You could make a pipeline that generates both dense and sparse features by using a FeatureUnion . from sklearn.pipeline import FeatureUnion from sklearn.feature_extraction.text import CountVectorizer preprocess = FeatureUnion ([ ( \"dense\" , SpacyLanguage ( \"en_core_web_md\" )), ( \"sparse_word\" , CountVectorizer ()), ( \"sparse_subword\" , CountVectorizer ( analyzer = \"char\" , ngram_range = ( 2 , 4 ))) ]) Supported Models \u00b6 Every language backend that this library offers is compatible for use in a scikit-learn pipeline. This includes the following; whatlies.language.SpacyLanguage whatlies.language.FasttextLanguage whatlies.language.CountVectorLanguage whatlies.language.BytePairLanguage whatlies.language.GensimLanguage whatlies.language.HFTransformersLanguage whatlies.language.TFHubLanguage whatlies.language.UniversalSentenceLanguage whatlies.language.SentenceTFMLanguage whatlies.language.UniversalSentenceLanguage whatlies.language.LaBSELanguage Caveats \u00b6 There's a few caveats to be aware of though. In general these language backends cannot be directly pickled so that means that you won't be able to save a pipeline if there's a whatlies component in it. This also means that you cannot use a gridsearch. Where possible we try to test against scikit-learn's testing utilities but for now the use-case is limited for use in a Pipeline . You should assume that you cannot use GridSearchCV and that you cannot pickle to disk. If you see a way to properly support this in general, let us know on github by creating an issue .","title":"Scikit-Learn Compatibility"},{"location":"tutorial/scikit-learn/#scikit-learn","text":"Many of the language-backends inside of this package can be used in scikit-learn pipelines. We've implemented a compatible .fit() and .transform() API which means that you could write scikit-learn pipelines like this: import numpy as np from whatlies.language import SpacyLanguage from sklearn.pipeline import Pipeline from sklearn.linear_model import LogisticRegression pipe = Pipeline ([ ( \"embed\" , SpacyLanguage ( \"en_core_web_md\" )), ( \"model\" , LogisticRegression ()) ]) X = [ \"i really like this post\" , \"thanks for that comment\" , \"i enjoy this friendly forum\" , \"this is a bad post\" , \"i dislike this article\" , \"this is not well written\" ] y = np . array ([ 1 , 1 , 1 , 0 , 0 , 0 ]) pipe . fit ( X , y ) This pipeline is using the embeddings from spaCy now and passing those to the logistic regression. pipe.predict_proba(X) # array([[0.37862409, 0.62137591], # [0.27858304, 0.72141696], # [0.21386529, 0.78613471], # [0.7155662 , 0.2844338 ], # [0.64924579, 0.35075421], # [0.76414156, 0.23585844]]) You could make a pipeline that generates both dense and sparse features by using a FeatureUnion . from sklearn.pipeline import FeatureUnion from sklearn.feature_extraction.text import CountVectorizer preprocess = FeatureUnion ([ ( \"dense\" , SpacyLanguage ( \"en_core_web_md\" )), ( \"sparse_word\" , CountVectorizer ()), ( \"sparse_subword\" , CountVectorizer ( analyzer = \"char\" , ngram_range = ( 2 , 4 ))) ])","title":"Scikit-Learn"},{"location":"tutorial/scikit-learn/#supported-models","text":"Every language backend that this library offers is compatible for use in a scikit-learn pipeline. This includes the following; whatlies.language.SpacyLanguage whatlies.language.FasttextLanguage whatlies.language.CountVectorLanguage whatlies.language.BytePairLanguage whatlies.language.GensimLanguage whatlies.language.HFTransformersLanguage whatlies.language.TFHubLanguage whatlies.language.UniversalSentenceLanguage whatlies.language.SentenceTFMLanguage whatlies.language.UniversalSentenceLanguage whatlies.language.LaBSELanguage","title":"Supported Models"},{"location":"tutorial/scikit-learn/#caveats","text":"There's a few caveats to be aware of though. In general these language backends cannot be directly pickled so that means that you won't be able to save a pipeline if there's a whatlies component in it. This also means that you cannot use a gridsearch. Where possible we try to test against scikit-learn's testing utilities but for now the use-case is limited for use in a Pipeline . You should assume that you cannot use GridSearchCV and that you cannot pickle to disk. If you see a way to properly support this in general, let us know on github by creating an issue .","title":"Caveats"},{"location":"tutorial/embeddings/","text":"Imaginary Tokens \u00b6 Let's make a few word-embeddings. The basic object for this is an Embedding object. from whatlies import Embedding foo = Embedding ( \"foo\" , [ 0.5 , 0.1 ]) bar = Embedding ( \"bar\" , [ 0.1 , 0.2 ]) buz = Embedding ( \"buz\" , [ 0.3 , 0.3 ]) These are all embedding objects. It has a name and a vector. It also has a representation. foo # Emb[foo] We can also apply operations on it as if it was a vector. foo | ( bar - buz ) # Emb[(foo | (bar - buz))] This will also change the internal vector. foo . vector # array([ 0.50, 0.10] ( foo | ( bar - buz )) . vector # array([ 0.06, -0.12]) But why read when we can plot? The whole point of this package is to make it visual. for t in [ foo , bar , buz ]: t . plot ( kind = \"scatter\" ) . plot ( kind = \"text\" ); Meaning \u00b6 Let's come up with imaginary embeddings for man , woman , king and queen . We will plot them using the arrow plotting type. man = Embedding ( \"man\" , [ 0.5 , 0.1 ]) woman = Embedding ( \"woman\" , [ 0.5 , 0.6 ]) king = Embedding ( \"king\" , [ 0.7 , 0.33 ]) queen = Embedding ( \"queen\" , [ 0.7 , 0.9 ]) man . plot ( kind = \"arrow\" , color = \"blue\" ) woman . plot ( kind = \"arrow\" , color = \"red\" ) king . plot ( kind = \"arrow\" , color = \"blue\" ) queen . plot ( kind = \"arrow\" , color = \"red\" ) plt . axis ( 'off' ); King - Man + Woman \u00b6 We can confirm the classic approximation that everybody likes to mention. man . plot ( kind = \"arrow\" , color = \"blue\" ) woman . plot ( kind = \"arrow\" , color = \"red\" ) king . plot ( kind = \"arrow\" , color = \"blue\" ) queen . plot ( kind = \"arrow\" , color = \"red\" ) ( king - man + woman ) . plot ( kind = \"arrow\" , color = \"pink\" ) plt . axis ( 'off' ); King - Queen \u00b6 But maybe I am interested in the vector that spans between queen and king . I'll use the - operator here to indicate the connection between the two tokens. Notice the poetry there... man . plot ( kind = \"arrow\" , color = \"blue\" ) woman . plot ( kind = \"arrow\" , color = \"red\" ) king . plot ( kind = \"arrow\" , color = \"blue\" ) queen . plot ( kind = \"arrow\" , color = \"red\" ) ( queen - king ) . plot ( kind = \"arrow\" , color = \"pink\" , show_ops = True ) plt . axis ( 'off' ); Man | (Queen - King) \u00b6 But that space queen-king ... we can also filter all that information out of our words. Linear algebra would call this \"making it orthogonal\". The | operator makes sense here. man . plot ( kind = \"arrow\" , color = \"blue\" ) woman . plot ( kind = \"arrow\" , color = \"red\" ) king . plot ( kind = \"arrow\" , color = \"blue\" ) queen . plot ( kind = \"arrow\" , color = \"red\" ) ( queen - king ) . plot ( kind = \"arrow\" , color = \"pink\" , show_ops = True ) ( man | ( queen - king )) . plot ( kind = \"arrow\" , color = \"pink\" , show_ops = True ) plt . axis ( 'off' ); Embedding Mathmatics \u00b6 This is interesting. We have our original tokens and can filter away the (man-woman) axis. By doing this we get \"new\" embeddings with different properties. Numerically we can confirm in our example that this new space maps Emb(man) to be very similar to Emb(woman) . ( man | ( queen - king )) . vector # array([0.5, 0. ] ( woman | ( queen - king )) . vector # array([0.49999999, 1e-16. ] The same holds for Emb(queen) and Emb(man) . ( queen | ( man - woman )) . vector # array([0.7, 0. ] ( king | ( man - woman )) . vector # array([0.7, 0. ] More Operations \u00b6 Let's consider some other operations. For this we will make new embeddings. man = Embedding ( \"man\" , [ 0.5 , 0.15 ]) woman = Embedding ( \"woman\" , [ 0.35 , 0.2 ]) king = Embedding ( \"king\" , [ 0.2 , 0.2 ]) man . plot ( kind = \"arrow\" , color = \"blue\" ) woman . plot ( kind = \"arrow\" , color = \"red\" ) king . plot ( kind = \"arrow\" , color = \"green\" ) plt . xlim ( 0 , 0.5 ) plt . ylim ( 0 , 0.5 ) plt . axis ( 'off' ); Mapping Unto Tokens \u00b6 In the previous example we demonstrated how to map \"away\" from vectors. But we can also map \"unto\" vectors. For this we introduce the >> operator. man . plot ( kind = \"arrow\" , color = \"blue\" ) woman . plot ( kind = \"arrow\" , color = \"red\" ) ( woman >> man ) . plot ( kind = \"arrow\" , color = \"red\" ) ( woman >> king ) . plot ( kind = \"arrow\" , color = \"red\" ) king . plot ( kind = \"arrow\" , color = \"green\" ) plt . xlim ( 0 , 0.5 ) plt . ylim ( 0 , 0.5 ) plt . axis ( 'off' ); Measuring the Mapping \u00b6 Note that the woman vector in our embedding maps partially unto man and overshoots a bit on king . We can quantify this by measuring what percentage of the vector is covered. This factor can be retreived by using the > operator. woman > king # 1.3749 woman > man # 0.7522 Interesting \u00b6 This suggests that perhaps ... king and man can be used as axes for plotting? It would also work if the embeddings were in a very high dimensional plane. No matter how large the embedding, we could've said woman spans 1.375 of king and 0.752 of man . Given king as the x-axis and man as the y-axis, we can map the token of man to a 2d representation (1.375, 0.752) which is easy to plot. This is an interesting way of thinking about it. We can plot high dimensional vectors in 2d as long as we can plot it along two axes. An axis could be a vector of a token, or a token that has had operations on it. Note that this > mapping can also cause negative values. foo = Embedding ( \"foo\" , [ - 0.2 , - 0.2 ]) foo . plot ( kind = \"arrow\" , color = \"pink\" ) woman . plot ( kind = \"arrow\" , color = \"red\" ) king . plot ( kind = \"arrow\" , color = \"green\" ) ( foo >> woman ) . plot ( kind = \"arrow\" , color = \"red\" , show_ops = True ) plt . xlim ( - .3 , 0.4 ) plt . ylim ( - .3 , 0.4 ) plt . axis ( 'off' ); foo > woman # -0.6769 Plotting High Dimensions \u00b6 Let's confirm this idea by using some spaCy word-vectors. import spacy nlp = spacy . load ( 'en_core_web_md' ) words = [ \"cat\" , \"dog\" , \"fish\" , \"kitten\" , \"man\" , \"woman\" , \"king\" , \"queen\" , \"doctor\" , \"nurse\" ] tokens = { t . text : Embedding ( t . text , t . vector ) for t in nlp . pipe ( words )} x_axis = tokens [ 'man' ] y_axis = tokens [ 'woman' ] for name , t in tokens . items (): t . plot ( x_axis = x_axis , y_axis = y_axis ) . plot ( kind = \"text\" , x_axis = x_axis , y_axis = y_axis ) The interesting thing here is that we can also perform operations on these words before plotting them. royalty = tokens [ 'king' ] - tokens [ 'queen' ] gender = tokens [ 'man' ] - tokens [ 'woman' ] for n , t in tokens . items (): ( t . plot ( x_axis = royalty , y_axis = gender ) . plot ( kind = \"text\" , x_axis = royalty , y_axis = gender )) The idea seems to work. But maybe we can introduce cooler charts and easier ways to deal with collections of embeddings.","title":"What Are Embeddings"},{"location":"tutorial/embeddings/#imaginary-tokens","text":"Let's make a few word-embeddings. The basic object for this is an Embedding object. from whatlies import Embedding foo = Embedding ( \"foo\" , [ 0.5 , 0.1 ]) bar = Embedding ( \"bar\" , [ 0.1 , 0.2 ]) buz = Embedding ( \"buz\" , [ 0.3 , 0.3 ]) These are all embedding objects. It has a name and a vector. It also has a representation. foo # Emb[foo] We can also apply operations on it as if it was a vector. foo | ( bar - buz ) # Emb[(foo | (bar - buz))] This will also change the internal vector. foo . vector # array([ 0.50, 0.10] ( foo | ( bar - buz )) . vector # array([ 0.06, -0.12]) But why read when we can plot? The whole point of this package is to make it visual. for t in [ foo , bar , buz ]: t . plot ( kind = \"scatter\" ) . plot ( kind = \"text\" );","title":"Imaginary Tokens"},{"location":"tutorial/embeddings/#meaning","text":"Let's come up with imaginary embeddings for man , woman , king and queen . We will plot them using the arrow plotting type. man = Embedding ( \"man\" , [ 0.5 , 0.1 ]) woman = Embedding ( \"woman\" , [ 0.5 , 0.6 ]) king = Embedding ( \"king\" , [ 0.7 , 0.33 ]) queen = Embedding ( \"queen\" , [ 0.7 , 0.9 ]) man . plot ( kind = \"arrow\" , color = \"blue\" ) woman . plot ( kind = \"arrow\" , color = \"red\" ) king . plot ( kind = \"arrow\" , color = \"blue\" ) queen . plot ( kind = \"arrow\" , color = \"red\" ) plt . axis ( 'off' );","title":"Meaning"},{"location":"tutorial/embeddings/#king-man-woman","text":"We can confirm the classic approximation that everybody likes to mention. man . plot ( kind = \"arrow\" , color = \"blue\" ) woman . plot ( kind = \"arrow\" , color = \"red\" ) king . plot ( kind = \"arrow\" , color = \"blue\" ) queen . plot ( kind = \"arrow\" , color = \"red\" ) ( king - man + woman ) . plot ( kind = \"arrow\" , color = \"pink\" ) plt . axis ( 'off' );","title":"King - Man + Woman"},{"location":"tutorial/embeddings/#king-queen","text":"But maybe I am interested in the vector that spans between queen and king . I'll use the - operator here to indicate the connection between the two tokens. Notice the poetry there... man . plot ( kind = \"arrow\" , color = \"blue\" ) woman . plot ( kind = \"arrow\" , color = \"red\" ) king . plot ( kind = \"arrow\" , color = \"blue\" ) queen . plot ( kind = \"arrow\" , color = \"red\" ) ( queen - king ) . plot ( kind = \"arrow\" , color = \"pink\" , show_ops = True ) plt . axis ( 'off' );","title":"King - Queen"},{"location":"tutorial/embeddings/#man-queen-king","text":"But that space queen-king ... we can also filter all that information out of our words. Linear algebra would call this \"making it orthogonal\". The | operator makes sense here. man . plot ( kind = \"arrow\" , color = \"blue\" ) woman . plot ( kind = \"arrow\" , color = \"red\" ) king . plot ( kind = \"arrow\" , color = \"blue\" ) queen . plot ( kind = \"arrow\" , color = \"red\" ) ( queen - king ) . plot ( kind = \"arrow\" , color = \"pink\" , show_ops = True ) ( man | ( queen - king )) . plot ( kind = \"arrow\" , color = \"pink\" , show_ops = True ) plt . axis ( 'off' );","title":"Man | (Queen - King)"},{"location":"tutorial/embeddings/#embedding-mathmatics","text":"This is interesting. We have our original tokens and can filter away the (man-woman) axis. By doing this we get \"new\" embeddings with different properties. Numerically we can confirm in our example that this new space maps Emb(man) to be very similar to Emb(woman) . ( man | ( queen - king )) . vector # array([0.5, 0. ] ( woman | ( queen - king )) . vector # array([0.49999999, 1e-16. ] The same holds for Emb(queen) and Emb(man) . ( queen | ( man - woman )) . vector # array([0.7, 0. ] ( king | ( man - woman )) . vector # array([0.7, 0. ]","title":"Embedding Mathmatics"},{"location":"tutorial/embeddings/#more-operations","text":"Let's consider some other operations. For this we will make new embeddings. man = Embedding ( \"man\" , [ 0.5 , 0.15 ]) woman = Embedding ( \"woman\" , [ 0.35 , 0.2 ]) king = Embedding ( \"king\" , [ 0.2 , 0.2 ]) man . plot ( kind = \"arrow\" , color = \"blue\" ) woman . plot ( kind = \"arrow\" , color = \"red\" ) king . plot ( kind = \"arrow\" , color = \"green\" ) plt . xlim ( 0 , 0.5 ) plt . ylim ( 0 , 0.5 ) plt . axis ( 'off' );","title":"More Operations"},{"location":"tutorial/embeddings/#mapping-unto-tokens","text":"In the previous example we demonstrated how to map \"away\" from vectors. But we can also map \"unto\" vectors. For this we introduce the >> operator. man . plot ( kind = \"arrow\" , color = \"blue\" ) woman . plot ( kind = \"arrow\" , color = \"red\" ) ( woman >> man ) . plot ( kind = \"arrow\" , color = \"red\" ) ( woman >> king ) . plot ( kind = \"arrow\" , color = \"red\" ) king . plot ( kind = \"arrow\" , color = \"green\" ) plt . xlim ( 0 , 0.5 ) plt . ylim ( 0 , 0.5 ) plt . axis ( 'off' );","title":"Mapping Unto Tokens"},{"location":"tutorial/embeddings/#measuring-the-mapping","text":"Note that the woman vector in our embedding maps partially unto man and overshoots a bit on king . We can quantify this by measuring what percentage of the vector is covered. This factor can be retreived by using the > operator. woman > king # 1.3749 woman > man # 0.7522","title":"Measuring the Mapping"},{"location":"tutorial/embeddings/#interesting","text":"This suggests that perhaps ... king and man can be used as axes for plotting? It would also work if the embeddings were in a very high dimensional plane. No matter how large the embedding, we could've said woman spans 1.375 of king and 0.752 of man . Given king as the x-axis and man as the y-axis, we can map the token of man to a 2d representation (1.375, 0.752) which is easy to plot. This is an interesting way of thinking about it. We can plot high dimensional vectors in 2d as long as we can plot it along two axes. An axis could be a vector of a token, or a token that has had operations on it. Note that this > mapping can also cause negative values. foo = Embedding ( \"foo\" , [ - 0.2 , - 0.2 ]) foo . plot ( kind = \"arrow\" , color = \"pink\" ) woman . plot ( kind = \"arrow\" , color = \"red\" ) king . plot ( kind = \"arrow\" , color = \"green\" ) ( foo >> woman ) . plot ( kind = \"arrow\" , color = \"red\" , show_ops = True ) plt . xlim ( - .3 , 0.4 ) plt . ylim ( - .3 , 0.4 ) plt . axis ( 'off' ); foo > woman # -0.6769","title":"Interesting"},{"location":"tutorial/embeddings/#plotting-high-dimensions","text":"Let's confirm this idea by using some spaCy word-vectors. import spacy nlp = spacy . load ( 'en_core_web_md' ) words = [ \"cat\" , \"dog\" , \"fish\" , \"kitten\" , \"man\" , \"woman\" , \"king\" , \"queen\" , \"doctor\" , \"nurse\" ] tokens = { t . text : Embedding ( t . text , t . vector ) for t in nlp . pipe ( words )} x_axis = tokens [ 'man' ] y_axis = tokens [ 'woman' ] for name , t in tokens . items (): t . plot ( x_axis = x_axis , y_axis = y_axis ) . plot ( kind = \"text\" , x_axis = x_axis , y_axis = y_axis ) The interesting thing here is that we can also perform operations on these words before plotting them. royalty = tokens [ 'king' ] - tokens [ 'queen' ] gender = tokens [ 'man' ] - tokens [ 'woman' ] for n , t in tokens . items (): ( t . plot ( x_axis = royalty , y_axis = gender ) . plot ( kind = \"text\" , x_axis = royalty , y_axis = gender )) The idea seems to work. But maybe we can introduce cooler charts and easier ways to deal with collections of embeddings.","title":"Plotting High Dimensions"},{"location":"tutorial/embeddingsets/","text":"Sets of Embeddings \u00b6 The Embedding object merely has support for matplotlib, but the EmbeddingSet has support for interactive tools. It is also more convenient. You can create an Direct Creation \u00b6 You can create these objects directly. import spacy from whatlies.embedding import Embedding from whatlies.embeddingset import EmbeddingSet nlp = spacy . load ( \"en_core_web_md\" ) words = [ \"prince\" , \"princess\" , \"nurse\" , \"doctor\" , \"banker\" , \"man\" , \"woman\" , \"cousin\" , \"neice\" , \"king\" , \"queen\" , \"dude\" , \"guy\" , \"gal\" , \"fire\" , \"dog\" , \"cat\" , \"mouse\" , \"red\" , \"blue\" , \"green\" , \"yellow\" , \"water\" , \"person\" , \"family\" , \"brother\" , \"sister\" ] emb = EmbeddingSet ({ t . text : Embedding ( t . text , t . vector ) for t in nlp . pipe ( words )}) This can be especially useful if you're creating your own embeddings. Via Languages \u00b6 But odds are that you just want to grab a language model from elsewhere. We've added backends to our library and this can be a convenient method of getting sets of embeddings (typically more performant too). from whatlies.language import SpacyLanguage words = [ \"prince\" , \"princess\" , \"nurse\" , \"doctor\" , \"banker\" , \"man\" , \"woman\" , \"cousin\" , \"neice\" , \"king\" , \"queen\" , \"dude\" , \"guy\" , \"gal\" , \"fire\" , \"dog\" , \"cat\" , \"mouse\" , \"red\" , \"blue\" , \"green\" , \"yellow\" , \"water\" , \"person\" , \"family\" , \"brother\" , \"sister\" ] lang = SpacyLanguage ( \"en_core_web_md\" ) emb = lang [ words ] Plotting \u00b6 Either way, with an EmbeddingSet you can create meaningful interactive charts. emb . plot_interactive ( 'man' , 'woman' ) fetch('tut2-chart1.json') .then(res => res.json()) .then((out) => { vegaEmbed('#vis1', out); }) .catch(err => { throw err }); We can also retreive embeddings from the embeddingset. emb [ 'king' ] Remember the operations we did before? We can also do that on these sets! new_emb = emb | ( emb [ 'king' ] - emb [ 'queen' ]) new_emb . plot_interactive ( 'man' , 'woman' ) fetch('tut2-chart2.json') .then(res => res.json()) .then((out) => { vegaEmbed('#vis2', out); }) .catch(err => { throw err }); Combining Charts \u00b6 Often you'd like to compare the effect of a mapping. Since we make our interactive charts with altair we get a nice api to stack charts next to eachother. orig_chart = emb . plot_interactive ( 'man' , 'woman' ) new_chart = new_emb . plot_interactive ( 'man' , 'woman' ) orig_chart | new_chart fetch('tut2-chart3.json') .then(res => res.json()) .then((out) => { vegaEmbed('#vis3', out); }) .catch(err => { throw err }); You may have noticed that these charts appear in the documentation, fully interactively. This is another nice feature of Altair, the charts can be serialized in a json format and hosted on the web. More Transformation \u00b6 But there are more transformations that we might visualise. Let's demonstrate two here. from whatlies.transformers import Pca , Umap orig_chart = emb . plot_interactive ( 'man' , 'woman' ) pca_emb = emb . transform ( Pca ( 2 )) umap_emb = emb . transform ( Umap ( 2 )) The transform method is able to take a transformation, let's say pca(2) and this will change the embeddings in the set. It might also create new embeddings. In case of pca(2) it will also add two embeddings which represent the principal components. This is nice because that means that we can plot along those axes. plot_pca = pca_emb . plot_interactive () plot_umap = umap_emb . plot_interactive () plot_pca | plot_umap fetch('tut2-chart4.json') .then(res => res.json()) .then((out) => { vegaEmbed('#vis4', out); }) .catch(err => { throw err }); Adding Color to the Charts \u00b6 Sometimes it might be helpful to add color to the charts. In these situations we first need to add a property to the embeddings in the embeddingset. This property can then be picked up by a chart in order to make a subset stand out from the rest of the group. from whatlies.language import SpacyLanguage from whatlies.transformers import Pca words = [ \"prince\" , \"princess\" , \"nurse\" , \"doctor\" , \"banker\" , \"man\" , \"woman\" , \"cousin\" , \"neice\" , \"king\" , \"queen\" , \"dude\" , \"guy\" , \"gal\" , \"fire\" , \"dog\" , \"cat\" , \"mouse\" , \"red\" , \"blue\" , \"green\" , \"yellow\" , \"water\" , \"person\" , \"family\" , \"brother\" , \"sister\" ] colors = [ \"red\" , \"blue\" , \"green\" , \"yellow\" ] lang = SpacyLanguage ( \"en_core_web_md\" ) # Notice the `assign` method, this is where we assign the `is_color` property # to each embedding in the embeddingset based on the \"name\". embset = ( lang [ words ] . transform ( Pca ( 2 )) . assign ( is_color = lambda e : e . name in colors )) embset . plot_interactive ( color = \"is_color\" ) fetch('tut-chart-color.json') .then(res => res.json()) .then((out) => { vegaEmbed('#vis-color', out); }) .catch(err => { throw err }); Using an Interactive Brush \u00b6 We can also choose to use plot_hover instead of plot_interactive . The hover chart cannot zoom in/out but it does allow you to draw a box to make a subselection. This can be very useful when you're trying to get an overview of a cluster of embeddings. from whatlies.language import SpacyLanguage from whatlies.transformers import Pca words = [ \"prince\" , \"princess\" , \"nurse\" , \"doctor\" , \"banker\" , \"man\" , \"woman\" , \"cousin\" , \"neice\" , \"king\" , \"queen\" , \"dude\" , \"guy\" , \"gal\" , \"fire\" , \"dog\" , \"cat\" , \"mouse\" , \"red\" , \"blue\" , \"green\" , \"yellow\" , \"water\" , \"person\" , \"family\" , \"brother\" , \"sister\" ] colors = [ \"red\" , \"blue\" , \"green\" , \"yellow\" ] lang = SpacyLanguage ( \"en_core_web_md\" ) embset = ( lang [ words ] . transform ( Pca ( 2 )) . assign ( is_color = lambda e : e . name in colors )) embset . plot_brush ( n_show = 15 , color = \"is_color\" ) fetch('tut-chart-hover.json') .then(res => res.json()) .then((out) => { vegaEmbed('#vis-hover', out); }) .catch(err => { throw err }); Large Matrix Visualisations \u00b6 If you're up for it, you can draw large matrices of charts too. from whatlies.language import SpacyLanguage from whatlies.transformers import Pca words = [ \"prince\" , \"princess\" , \"nurse\" , \"doctor\" , \"banker\" , \"man\" , \"woman\" , \"cousin\" , \"neice\" , \"king\" , \"queen\" , \"dude\" , \"guy\" , \"gal\" , \"fire\" , \"dog\" , \"cat\" , \"mouse\" , \"red\" , \"blue\" , \"green\" , \"yellow\" , \"water\" , \"person\" , \"family\" , \"brother\" , \"sister\" ] lang = SpacyLanguage ( \"en_core_web_md\" ) lang [ words ] . transform ( Pca ( 2 )) . plot_interactive_matrix ( 0 , 1 , 2 ) fetch('tut2-chart6.json') .then(res => res.json()) .then((out) => { vegaEmbed('#vis6', out); }) .catch(err => { throw err }); Zoom in on that chart. Don't forget to click and drag. Can we interpret the components?","title":"Interactive Visualisation"},{"location":"tutorial/embeddingsets/#sets-of-embeddings","text":"The Embedding object merely has support for matplotlib, but the EmbeddingSet has support for interactive tools. It is also more convenient. You can create an","title":"Sets of Embeddings"},{"location":"tutorial/embeddingsets/#direct-creation","text":"You can create these objects directly. import spacy from whatlies.embedding import Embedding from whatlies.embeddingset import EmbeddingSet nlp = spacy . load ( \"en_core_web_md\" ) words = [ \"prince\" , \"princess\" , \"nurse\" , \"doctor\" , \"banker\" , \"man\" , \"woman\" , \"cousin\" , \"neice\" , \"king\" , \"queen\" , \"dude\" , \"guy\" , \"gal\" , \"fire\" , \"dog\" , \"cat\" , \"mouse\" , \"red\" , \"blue\" , \"green\" , \"yellow\" , \"water\" , \"person\" , \"family\" , \"brother\" , \"sister\" ] emb = EmbeddingSet ({ t . text : Embedding ( t . text , t . vector ) for t in nlp . pipe ( words )}) This can be especially useful if you're creating your own embeddings.","title":"Direct Creation"},{"location":"tutorial/embeddingsets/#via-languages","text":"But odds are that you just want to grab a language model from elsewhere. We've added backends to our library and this can be a convenient method of getting sets of embeddings (typically more performant too). from whatlies.language import SpacyLanguage words = [ \"prince\" , \"princess\" , \"nurse\" , \"doctor\" , \"banker\" , \"man\" , \"woman\" , \"cousin\" , \"neice\" , \"king\" , \"queen\" , \"dude\" , \"guy\" , \"gal\" , \"fire\" , \"dog\" , \"cat\" , \"mouse\" , \"red\" , \"blue\" , \"green\" , \"yellow\" , \"water\" , \"person\" , \"family\" , \"brother\" , \"sister\" ] lang = SpacyLanguage ( \"en_core_web_md\" ) emb = lang [ words ]","title":"Via Languages"},{"location":"tutorial/embeddingsets/#plotting","text":"Either way, with an EmbeddingSet you can create meaningful interactive charts. emb . plot_interactive ( 'man' , 'woman' ) fetch('tut2-chart1.json') .then(res => res.json()) .then((out) => { vegaEmbed('#vis1', out); }) .catch(err => { throw err }); We can also retreive embeddings from the embeddingset. emb [ 'king' ] Remember the operations we did before? We can also do that on these sets! new_emb = emb | ( emb [ 'king' ] - emb [ 'queen' ]) new_emb . plot_interactive ( 'man' , 'woman' ) fetch('tut2-chart2.json') .then(res => res.json()) .then((out) => { vegaEmbed('#vis2', out); }) .catch(err => { throw err });","title":"Plotting"},{"location":"tutorial/embeddingsets/#combining-charts","text":"Often you'd like to compare the effect of a mapping. Since we make our interactive charts with altair we get a nice api to stack charts next to eachother. orig_chart = emb . plot_interactive ( 'man' , 'woman' ) new_chart = new_emb . plot_interactive ( 'man' , 'woman' ) orig_chart | new_chart fetch('tut2-chart3.json') .then(res => res.json()) .then((out) => { vegaEmbed('#vis3', out); }) .catch(err => { throw err }); You may have noticed that these charts appear in the documentation, fully interactively. This is another nice feature of Altair, the charts can be serialized in a json format and hosted on the web.","title":"Combining Charts"},{"location":"tutorial/embeddingsets/#more-transformation","text":"But there are more transformations that we might visualise. Let's demonstrate two here. from whatlies.transformers import Pca , Umap orig_chart = emb . plot_interactive ( 'man' , 'woman' ) pca_emb = emb . transform ( Pca ( 2 )) umap_emb = emb . transform ( Umap ( 2 )) The transform method is able to take a transformation, let's say pca(2) and this will change the embeddings in the set. It might also create new embeddings. In case of pca(2) it will also add two embeddings which represent the principal components. This is nice because that means that we can plot along those axes. plot_pca = pca_emb . plot_interactive () plot_umap = umap_emb . plot_interactive () plot_pca | plot_umap fetch('tut2-chart4.json') .then(res => res.json()) .then((out) => { vegaEmbed('#vis4', out); }) .catch(err => { throw err });","title":"More Transformation"},{"location":"tutorial/embeddingsets/#adding-color-to-the-charts","text":"Sometimes it might be helpful to add color to the charts. In these situations we first need to add a property to the embeddings in the embeddingset. This property can then be picked up by a chart in order to make a subset stand out from the rest of the group. from whatlies.language import SpacyLanguage from whatlies.transformers import Pca words = [ \"prince\" , \"princess\" , \"nurse\" , \"doctor\" , \"banker\" , \"man\" , \"woman\" , \"cousin\" , \"neice\" , \"king\" , \"queen\" , \"dude\" , \"guy\" , \"gal\" , \"fire\" , \"dog\" , \"cat\" , \"mouse\" , \"red\" , \"blue\" , \"green\" , \"yellow\" , \"water\" , \"person\" , \"family\" , \"brother\" , \"sister\" ] colors = [ \"red\" , \"blue\" , \"green\" , \"yellow\" ] lang = SpacyLanguage ( \"en_core_web_md\" ) # Notice the `assign` method, this is where we assign the `is_color` property # to each embedding in the embeddingset based on the \"name\". embset = ( lang [ words ] . transform ( Pca ( 2 )) . assign ( is_color = lambda e : e . name in colors )) embset . plot_interactive ( color = \"is_color\" ) fetch('tut-chart-color.json') .then(res => res.json()) .then((out) => { vegaEmbed('#vis-color', out); }) .catch(err => { throw err });","title":"Adding Color to the Charts"},{"location":"tutorial/embeddingsets/#using-an-interactive-brush","text":"We can also choose to use plot_hover instead of plot_interactive . The hover chart cannot zoom in/out but it does allow you to draw a box to make a subselection. This can be very useful when you're trying to get an overview of a cluster of embeddings. from whatlies.language import SpacyLanguage from whatlies.transformers import Pca words = [ \"prince\" , \"princess\" , \"nurse\" , \"doctor\" , \"banker\" , \"man\" , \"woman\" , \"cousin\" , \"neice\" , \"king\" , \"queen\" , \"dude\" , \"guy\" , \"gal\" , \"fire\" , \"dog\" , \"cat\" , \"mouse\" , \"red\" , \"blue\" , \"green\" , \"yellow\" , \"water\" , \"person\" , \"family\" , \"brother\" , \"sister\" ] colors = [ \"red\" , \"blue\" , \"green\" , \"yellow\" ] lang = SpacyLanguage ( \"en_core_web_md\" ) embset = ( lang [ words ] . transform ( Pca ( 2 )) . assign ( is_color = lambda e : e . name in colors )) embset . plot_brush ( n_show = 15 , color = \"is_color\" ) fetch('tut-chart-hover.json') .then(res => res.json()) .then((out) => { vegaEmbed('#vis-hover', out); }) .catch(err => { throw err });","title":"Using an Interactive Brush"},{"location":"tutorial/embeddingsets/#large-matrix-visualisations","text":"If you're up for it, you can draw large matrices of charts too. from whatlies.language import SpacyLanguage from whatlies.transformers import Pca words = [ \"prince\" , \"princess\" , \"nurse\" , \"doctor\" , \"banker\" , \"man\" , \"woman\" , \"cousin\" , \"neice\" , \"king\" , \"queen\" , \"dude\" , \"guy\" , \"gal\" , \"fire\" , \"dog\" , \"cat\" , \"mouse\" , \"red\" , \"blue\" , \"green\" , \"yellow\" , \"water\" , \"person\" , \"family\" , \"brother\" , \"sister\" ] lang = SpacyLanguage ( \"en_core_web_md\" ) lang [ words ] . transform ( Pca ( 2 )) . plot_interactive_matrix ( 0 , 1 , 2 ) fetch('tut2-chart6.json') .then(res => res.json()) .then((out) => { vegaEmbed('#vis6', out); }) .catch(err => { throw err }); Zoom in on that chart. Don't forget to click and drag. Can we interpret the components?","title":"Large Matrix Visualisations"},{"location":"tutorial/quickstart/","text":"Installation \u00b6 Whatlies is a library that tries to make it easy for you to play around with text embeddings. To quickly get started with it, you can install via pip; python -m pip install whatlies Fetching Embeddings \u00b6 To fetch embeddings from a language backend you will first need to import a backend. We'll use byte pair embeddings now. from whatlies.language import BytePairLanguage bp_lang = BytePairLanguage ( \"en\" ) You can use this language backend to fetch an embedding of a single word. emb_king = bp_lang [ \"king\" ] emb_king . vector This is a single embedding, but you can also fetch many embeddings at the same time. words = [ 'man' , 'woman' , 'king' , 'queen' , 'brother' , 'sister' , 'cat' , 'dog' , 'lion' , 'puppy' , 'male student' , 'female student' , 'university' , 'school' , 'kitten' ] embset = bp_lang [ words ] This embset resembles an embeddingset which is a collection of embeddings. Where a dataframe is a container for tabular data, this embeddingset is a container for a set of embeddings. This container has many utility methods implemented. You can for example turn it into a numpy array. embset . to_X () Visualisation \u00b6 But you can also use it for visualisation. embset . plot_similarity ( metric = \"cosine\" ) There are many visualisations implemented. In particular we offer interactive altair charts. These can be useful to show how points are clustered. Before plotting this, it might be common to apply a dimensionality reduction technique first. # To use the Umap feature you will need to install an extra dependency. # pip install whatlies[umap] from whatlies.transformers import Pca , Umap p1 = embset . transform ( Pca ( 2 )) . plot_interactive ( title = \"pca\" ) p2 = embset . transform ( Umap ( 2 )) . plot_interactive ( title = \"umap\" ) p1 | p2 fetch('chart.json') .then(res => res.json()) .then((out) => { vegaEmbed('#vis', out); }) .catch(err => { throw err }); These charts are fully interactive. You can zoom and drag with your cursor. Compatibility \u00b6 The goal of this project is to have a single API for many different backends. We offer support for: spaCy huggingface , tensorflow hub gensim bytepair fasttext sentence transformers Note that you're not just limited to word embeddings, our API deals with text in general and can also fetch embeddings for contextualized language models. We also do our best to be compatible with popular tools like scikit-learn. Since we standardise the output of our language backends we can give a familiar API to scikit-learn pipelines too. import numpy as np from whatlies.language import BytePairLanguage from sklearn.pipeline import Pipeline from sklearn.linear_model import LogisticRegression pipe = Pipeline ([ ( \"embed\" , BytePairLanguage ( \"en\" )), ( \"model\" , LogisticRegression ()) ]) X = [ \"i really like this post\" , \"thanks for that comment\" , \"i enjoy this friendly forum\" , \"this is a bad post\" , \"i dislike this article\" , \"this is not well written\" ] y = np . array ([ 1 , 1 , 1 , 0 , 0 , 0 ]) pipe . fit ( X , y ) We hope this tools makes it easier for you to explore word-embeddings.","title":"Quickstart"},{"location":"tutorial/quickstart/#installation","text":"Whatlies is a library that tries to make it easy for you to play around with text embeddings. To quickly get started with it, you can install via pip; python -m pip install whatlies","title":"Installation"},{"location":"tutorial/quickstart/#fetching-embeddings","text":"To fetch embeddings from a language backend you will first need to import a backend. We'll use byte pair embeddings now. from whatlies.language import BytePairLanguage bp_lang = BytePairLanguage ( \"en\" ) You can use this language backend to fetch an embedding of a single word. emb_king = bp_lang [ \"king\" ] emb_king . vector This is a single embedding, but you can also fetch many embeddings at the same time. words = [ 'man' , 'woman' , 'king' , 'queen' , 'brother' , 'sister' , 'cat' , 'dog' , 'lion' , 'puppy' , 'male student' , 'female student' , 'university' , 'school' , 'kitten' ] embset = bp_lang [ words ] This embset resembles an embeddingset which is a collection of embeddings. Where a dataframe is a container for tabular data, this embeddingset is a container for a set of embeddings. This container has many utility methods implemented. You can for example turn it into a numpy array. embset . to_X ()","title":"Fetching Embeddings"},{"location":"tutorial/quickstart/#visualisation","text":"But you can also use it for visualisation. embset . plot_similarity ( metric = \"cosine\" ) There are many visualisations implemented. In particular we offer interactive altair charts. These can be useful to show how points are clustered. Before plotting this, it might be common to apply a dimensionality reduction technique first. # To use the Umap feature you will need to install an extra dependency. # pip install whatlies[umap] from whatlies.transformers import Pca , Umap p1 = embset . transform ( Pca ( 2 )) . plot_interactive ( title = \"pca\" ) p2 = embset . transform ( Umap ( 2 )) . plot_interactive ( title = \"umap\" ) p1 | p2 fetch('chart.json') .then(res => res.json()) .then((out) => { vegaEmbed('#vis', out); }) .catch(err => { throw err }); These charts are fully interactive. You can zoom and drag with your cursor.","title":"Visualisation"},{"location":"tutorial/quickstart/#compatibility","text":"The goal of this project is to have a single API for many different backends. We offer support for: spaCy huggingface , tensorflow hub gensim bytepair fasttext sentence transformers Note that you're not just limited to word embeddings, our API deals with text in general and can also fetch embeddings for contextualized language models. We also do our best to be compatible with popular tools like scikit-learn. Since we standardise the output of our language backends we can give a familiar API to scikit-learn pipelines too. import numpy as np from whatlies.language import BytePairLanguage from sklearn.pipeline import Pipeline from sklearn.linear_model import LogisticRegression pipe = Pipeline ([ ( \"embed\" , BytePairLanguage ( \"en\" )), ( \"model\" , LogisticRegression ()) ]) X = [ \"i really like this post\" , \"thanks for that comment\" , \"i enjoy this friendly forum\" , \"this is a bad post\" , \"i dislike this article\" , \"this is not well written\" ] y = np . array ([ 1 , 1 , 1 , 0 , 0 , 0 ]) pipe . fit ( X , y ) We hope this tools makes it easier for you to explore word-embeddings.","title":"Compatibility"},{"location":"tutorial/transformations/","text":"State and Colors \u00b6 A goal of this package is to be able to compare the effect of transformations. That is why some of our transformations carry state. Umap is one such example. from whatlies.language.language import SpacyLanguage lang = SpacyLanguage ( 'en_core_web_sm' ) words1 = [ \"dog\" , \"cat\" , \"mouse\" , \"deer\" , \"elephant\" , \"zebra\" , \"fish\" , \"rabbit\" , \"rat\" , \"tomato\" , \"banana\" , \"coffee\" , \"tea\" , \"apple\" , \"union\" ] words2 = [ \"run\" , \"swim\" , \"dance\" , \"sit\" , \"eat\" , \"hear\" , \"look\" , \"run\" , \"stand\" ] umap = Umap ( 2 ) emb1 = lang [ words1 ] . transform ( umap ) . add_property ( 'set' , lambda d : 'set-one' ) emb2 = lang [ words2 ] . transform ( umap ) . add_property ( 'set' , lambda d : 'set-two' ) both = emb1 . merge ( emb2 ) In this code the transformer is trained on emb1 and applied on both emb1 and emb2 . We use the .add_property helper to indicate from which set the embeddings came. This way we can use it as a color in an interactive plot. both . plot_interactive ( color = 'set' ) fetch('colors.json') .then(res => res.json()) .then((out) => { vegaEmbed('#vis1', out); }) .catch(err => { throw err }); Visualising Differences \u00b6 Let's create two embeddings. from whatlies.language.language import SpacyLanguage lang = SpacyLanguage ( \"en_core_web_md\" ) words = [ \"prince\" , \"princess\" , \"nurse\" , \"doctor\" , \"banker\" , \"man\" , \"woman\" , \"cousin\" , \"neice\" , \"king\" , \"queen\" , \"dude\" , \"guy\" , \"gal\" , \"fire\" , \"dog\" , \"cat\" , \"mouse\" , \"red\" , \"blue\" , \"green\" , \"yellow\" , \"water\" , \"person\" , \"family\" , \"brother\" , \"sister\" , \"happy prince\" , \"sad prince\" ] emb1 = lang [ words ] emb2 = lang [ words ] | ( lang [ \"king\" ] - lang [ \"queen\" ]) The two embeddings should be similar but we can show that they are different. p1 = emb1 . plot_interactive ( \"man\" , \"woman\" ) p2 = emb2 . plot_interactive ( \"man\" , \"woman\" ) p1 | p2 fetch('two-groups-one.json') .then(res => res.json()) .then((out) => { vegaEmbed('#vis2', out); }) .catch(err => { throw err }); In this case, both plots will plot their embeddings with regards to their own embedding for man and woman . But we can also explicitly tell them to compare against the original vectors from emb1 . p1 = emb1 . plot_interactive ( emb1 [ \"man\" ], emb1 [ \"woman\" ]) p2 = emb2 . plot_interactive ( emb1 [ \"man\" ], emb1 [ \"woman\" ]) p1 | p2 fetch('two-groups-two.json') .then(res => res.json()) .then((out) => { vegaEmbed('#vis3', out); }) .catch(err => { throw err }); It's subtle but it is important to recognize. Movement \u00b6 If you want to highlight the movement that occurs because of a transformation then you might prefer to show a movement plot. emb1 . plot_movement ( emb2 , \"man\" , \"woman\" ) . properties ( width = 600 , height = 450 ) fetch('movement.json') .then(res => res.json()) .then((out) => { vegaEmbed('#vis4', out); }) .catch(err => { throw err });","title":"Index"},{"location":"tutorial/transformations/#state-and-colors","text":"A goal of this package is to be able to compare the effect of transformations. That is why some of our transformations carry state. Umap is one such example. from whatlies.language.language import SpacyLanguage lang = SpacyLanguage ( 'en_core_web_sm' ) words1 = [ \"dog\" , \"cat\" , \"mouse\" , \"deer\" , \"elephant\" , \"zebra\" , \"fish\" , \"rabbit\" , \"rat\" , \"tomato\" , \"banana\" , \"coffee\" , \"tea\" , \"apple\" , \"union\" ] words2 = [ \"run\" , \"swim\" , \"dance\" , \"sit\" , \"eat\" , \"hear\" , \"look\" , \"run\" , \"stand\" ] umap = Umap ( 2 ) emb1 = lang [ words1 ] . transform ( umap ) . add_property ( 'set' , lambda d : 'set-one' ) emb2 = lang [ words2 ] . transform ( umap ) . add_property ( 'set' , lambda d : 'set-two' ) both = emb1 . merge ( emb2 ) In this code the transformer is trained on emb1 and applied on both emb1 and emb2 . We use the .add_property helper to indicate from which set the embeddings came. This way we can use it as a color in an interactive plot. both . plot_interactive ( color = 'set' ) fetch('colors.json') .then(res => res.json()) .then((out) => { vegaEmbed('#vis1', out); }) .catch(err => { throw err });","title":"State and Colors"},{"location":"tutorial/transformations/#visualising-differences","text":"Let's create two embeddings. from whatlies.language.language import SpacyLanguage lang = SpacyLanguage ( \"en_core_web_md\" ) words = [ \"prince\" , \"princess\" , \"nurse\" , \"doctor\" , \"banker\" , \"man\" , \"woman\" , \"cousin\" , \"neice\" , \"king\" , \"queen\" , \"dude\" , \"guy\" , \"gal\" , \"fire\" , \"dog\" , \"cat\" , \"mouse\" , \"red\" , \"blue\" , \"green\" , \"yellow\" , \"water\" , \"person\" , \"family\" , \"brother\" , \"sister\" , \"happy prince\" , \"sad prince\" ] emb1 = lang [ words ] emb2 = lang [ words ] | ( lang [ \"king\" ] - lang [ \"queen\" ]) The two embeddings should be similar but we can show that they are different. p1 = emb1 . plot_interactive ( \"man\" , \"woman\" ) p2 = emb2 . plot_interactive ( \"man\" , \"woman\" ) p1 | p2 fetch('two-groups-one.json') .then(res => res.json()) .then((out) => { vegaEmbed('#vis2', out); }) .catch(err => { throw err }); In this case, both plots will plot their embeddings with regards to their own embedding for man and woman . But we can also explicitly tell them to compare against the original vectors from emb1 . p1 = emb1 . plot_interactive ( emb1 [ \"man\" ], emb1 [ \"woman\" ]) p2 = emb2 . plot_interactive ( emb1 [ \"man\" ], emb1 [ \"woman\" ]) p1 | p2 fetch('two-groups-two.json') .then(res => res.json()) .then((out) => { vegaEmbed('#vis3', out); }) .catch(err => { throw err }); It's subtle but it is important to recognize.","title":"Visualising Differences"},{"location":"tutorial/transformations/#movement","text":"If you want to highlight the movement that occurs because of a transformation then you might prefer to show a movement plot. emb1 . plot_movement ( emb2 , \"man\" , \"woman\" ) . properties ( width = 600 , height = 450 ) fetch('movement.json') .then(res => res.json()) .then((out) => { vegaEmbed('#vis4', out); }) .catch(err => { throw err });","title":"Movement"}]}